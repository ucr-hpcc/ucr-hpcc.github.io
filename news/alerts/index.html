<!doctype html><html lang=en class=no-js><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=generator content="Hugo 0.83.1"><meta name=ROBOTS content="INDEX, FOLLOW"><link rel="shortcut icon" href=../../favicons/favicon.ico><link rel=apple-touch-icon href=../../favicons/apple-touch-icon-180x180.png sizes=180x180><link rel=icon type=image/png href=../../favicons/favicon-16x16.png sizes=16x16><link rel=icon type=image/png href=../../favicons/favicon-32x32.png sizes=32x32><link rel=icon type=image/png href=../../favicons/android-36x36.png sizes=36x36><link rel=icon type=image/png href=../../favicons/android-48x48.png sizes=48x48><link rel=icon type=image/png href=../../favicons/android-72x72.png sizes=72x72><link rel=icon type=image/png href=../../favicons/android-96x96.png sizes=96x96><link rel=icon type=image/png href=../../favicons/android-144x144.png sizes=144x144><link rel=icon type=image/png href=../../favicons/android-192x192.png sizes=192x192><title>User alerts for HPCC's computing resources | HPCC</title><meta property="og:title" content="User alerts for HPCC's computing resources"><meta property="og:description" content="Unscheduled exceptions and downtimes 14-May-2025
  1:00 AM: There was a power outage affecting several buildings on campus, including our server room in SOMED1 at approximately 1AM. Until the larger 500kW UPS will be installed in late summer, not all of our computing nodes are battery backed up. However, our core infrastructure is UPS backed up, including storage, administration nodes, etc. Our newer &ldquo;epyc&rdquo; and modern GPU nodes (gpu[06-09]) are also battery backed up and should not have lost any jobs."><meta property="og:type" content="article"><meta property="og:url" content="https://hpcc.ucr.edu/news/alerts/"><meta property="article:section" content="News"><meta property="article:modified_time" content="2025-05-14T09:47:44-07:00"><meta property="og:site_name" content="HPCC"><meta itemprop=name content="User alerts for HPCC's computing resources"><meta itemprop=description content="Unscheduled exceptions and downtimes 14-May-2025
  1:00 AM: There was a power outage affecting several buildings on campus, including our server room in SOMED1 at approximately 1AM. Until the larger 500kW UPS will be installed in late summer, not all of our computing nodes are battery backed up. However, our core infrastructure is UPS backed up, including storage, administration nodes, etc. Our newer &ldquo;epyc&rdquo; and modern GPU nodes (gpu[06-09]) are also battery backed up and should not have lost any jobs."><meta itemprop=dateModified content="2025-05-14T09:47:44-07:00"><meta itemprop=wordCount content="4226"><meta itemprop=keywords content><meta name=twitter:card content="summary"><meta name=twitter:title content="User alerts for HPCC's computing resources"><meta name=twitter:description content="Unscheduled exceptions and downtimes 14-May-2025
  1:00 AM: There was a power outage affecting several buildings on campus, including our server room in SOMED1 at approximately 1AM. Until the larger 500kW UPS will be installed in late summer, not all of our computing nodes are battery backed up. However, our core infrastructure is UPS backed up, including storage, administration nodes, etc. Our newer &ldquo;epyc&rdquo; and modern GPU nodes (gpu[06-09]) are also battery backed up and should not have lost any jobs."><link rel=preload href=../../scss/main.min.398442d9d5d4e5f14be4a4e8e9b450b7e55d46418388cfab16ab414422fad5fa.css as=style><link href=../../scss/main.min.398442d9d5d4e5f14be4a4e8e9b450b7e55d46418388cfab16ab414422fad5fa.css rel=stylesheet integrity><script src=https://code.jquery.com/jquery-3.5.1.min.js integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0=" crossorigin=anonymous></script><script src=https://unpkg.com/lunr@2.3.8/lunr.min.js integrity=sha384-vRQ9bDyE0Wnu+lMfm57BlYLO0/XauFuKpVsZPs7KEDwYKktWi5+Kz3MP8++DFlRY crossorigin=anonymous></script><link rel=stylesheet href=../../css/prism.css><script src=../../js/driver0.9.8.min.js></script><link rel=stylesheet href=../../css/driver0.9.8.min.css><link rel=stylesheet href=../../css/site.css><script src=../../js/site.js></script></head><body class="td-page line-numbers"><header><nav class="js-navbar-scroll navbar navbar-expand navbar-dark flex-column flex-md-row td-navbar"><a class=navbar-brand href=../../><span class=navbar-logo><svg height="16" id="Layer_1" style="enable-background:new 0 0 16 16" viewBox="0 0 16 16" width="16" sodipodi:docname="126572_home_house_icon.svg" inkscape:export-filename="/home/jhayes/126572_home_house_icon.png" inkscape:export-xdpi="96" inkscape:export-ydpi="96" inkscape:version="1.1 (c4e8f9ed74, 2021-05-24)" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape" xmlns:sodipodi="http://sodipodi.sourceforge.net/DTD/sodipodi-0.dtd" xmlns="http://www.w3.org/2000/svg" xmlns:svg="http://www.w3.org/2000/svg"><defs id="defs7"/><sodipodi:namedview id="namedview5" pagecolor="#ffffff" bordercolor="#666666" borderopacity="1" inkscape:pageshadow="2" inkscape:pageopacity="0" inkscape:pagecheckerboard="0" showgrid="false" inkscape:zoom="49.0625" inkscape:cx="3.5464968" inkscape:cy="7.9694268" inkscape:window-width="3440" inkscape:window-height="1391" inkscape:window-x="0" inkscape:window-y="24" inkscape:window-maximized="1" inkscape:current-layer="Layer_1"/><path style="display:inline;fill:#fff;fill-opacity:1;stroke-width:.0203822" d="m2.8659775 15.977961c-.3936273-.051-.7525483-.389889-.8358103-.789155-.015862-.07606-.022524-1.009027-.022524-3.15414V8.988535H1.4412249C.82810729 8.988535.73920188 8.977312.53578892 8.8742347.39201211 8.8013774.21162731 8.6243834.13540635 8.4813782.01718088 8.2595644-.01861304 7.986667.04066723 7.7590746c.03869392-.148556.08395752-.2367427.20614877-.4016385C.36550378 7.1972684 1.9544118 5.5981695 5.0547771 2.5186377L7.3783439.21068447 7.5602468.12062886 7.7421497.03057325h.2563137c.246189.0.2617641.0025817.3942926.06535805.075888.03594693.1905382.10669563.254777.15721933.064239.0505237.6212568.59095827 1.2378173 1.20096567l1.1210187 1.1091045.01139-.4000817c.01-.3510397.01694-.413483.05667-.5094082.08521-.2057341.220395-.3661097.415434-.4928535.209421-.1360904.280363-.1452781 1.065556-.1380008l.692994.00642.159642.076227c.292895.1398524.506525.4124209.562217.7173255.01479.080962.02209.7172189.02209 1.9247442v1.8038031l.83131.8370395c.970032.9767169 1.061962 1.0876674 1.135666 1.3706362.134824.5176265-.189416 1.0615314-.71432 1.1982515-.09368.024401-.243383.031209-.686238.031209h-.566418v3.0331994c0 2.090488-.0069 3.070788-.02209 3.15414-.07531.412343-.432865.748345-.854561.803061-.08611.01117-.794528.02008-1.62526.02044L10.017834 16 10.017431 13.498089C10.01718 11.939735 10.009334 10.942371 9.9966286 10.853503 9.9447492 10.490634 9.6889196 10.179327 9.3299104 10.042205 9.2140078 9.9979369 9.1996048 9.9974522 8 9.9974522s-1.2140078 4851e-7-1.3299104.044753c-.3590092.137122-.6148388.448429-.6667182.811298-.012705.08887-.02055 1.086232-.020802 2.644586L5.9821656 16 4.489172 15.997416c-.8211465-.0014-1.551584-.01018-1.6231945-.01945z" id="path58637" sodipodi:insensitive="true"/></svg></span><span class=font-weight-bold>HPCC</span></a>
<row class=header-switch><div class="custom-control custom-switch"><input type=checkbox class=custom-control-input id=sidebarSwitch checked>
<label class=custom-control-label for=sidebarSwitch>sidebar</label></div><div class="custom-control custom-switch"><input type=checkbox class=custom-control-input id=tocSwitch checked>
<label class=custom-control-label for=tocSwitch>toc</label></div></row><div class="td-navbar-nav-scroll ml-md-auto" id=main_navbar><ul class="navbar-nav mt-2 mt-lg-0"><li class="nav-item mr-4 mb-2 mb-lg-0" style=position:relative><i class="fas fa-info-circle btn-primary"></i><a class="nav-link dropdown-toggle" href=../../about/ id=navbarDropdown role=button>
About</a><div class=dropdown-menu aria-labelledby=navbarDropdownMenuLink><a class=dropdown-item href=../../about/overview/ style=padding-left:1rem;font-weight:calc(3/3*500)>Overview</a>
<a class=dropdown-item href=../../about/overview/introduction/ style=padding-left:2rem;font-weight:calc(3/4*500)>Introduction</a>
<a class=dropdown-item href=../../about/overview/access/ style=padding-left:2rem;font-weight:calc(3/4*500)>Access</a>
<a class=dropdown-item href=../../about/overview/activity/ style=padding-left:2rem;font-weight:calc(3/4*500)>Activity Report</a>
<a class=dropdown-item href=../../about/overview/rates/ style=padding-left:2rem;font-weight:calc(3/4*500)>Rates</a>
<a class=dropdown-item href=../../about/overview/contact/ style=padding-left:2rem;font-weight:calc(3/4*500)>Contact</a>
<a class=dropdown-item href=../../about/overview/acknowledgement/ style=padding-left:2rem;font-weight:calc(3/4*500)>Acknowledgement</a>
<a class=dropdown-item href=../../about/hardware/ style=padding-left:1rem;font-weight:calc(3/3*500)>Hardware</a>
<a class=dropdown-item href=../../about/hardware/overview style=padding-left:2rem;font-weight:calc(3/4*500)>Overview</a>
<a class=dropdown-item href=../../about/hardware/details style=padding-left:2rem;font-weight:calc(3/4*500)>Details</a>
<a class=dropdown-item href=../../about/software/ style=padding-left:1rem;font-weight:calc(3/3*500)>Software</a>
<a class=dropdown-item href=../../about/software/conda_packages/ style=padding-left:2rem;font-weight:calc(3/4*500)>Conda Packages</a>
<a class=dropdown-item href=../../about/software/installs/ style=padding-left:2rem;font-weight:calc(3/4*500)>Installs</a>
<a class=dropdown-item href=../../about/software/modules/ style=padding-left:2rem;font-weight:calc(3/4*500)>Modules</a>
<a class=dropdown-item href=../../about/software/system/ style=padding-left:2rem;font-weight:calc(3/4*500)>System</a>
<a class=dropdown-item href=../../about/software/commercial style=padding-left:2rem;font-weight:calc(3/4*500)>Commercial</a></div></li><li class="nav-item mr-4 mb-2 mb-lg-0" style=position:relative><i class="fas fa-newspaper btn-primary"></i><a class="nav-link dropdown-toggle" href=../../news id=navbarDropdown role=button>
News</a><div class=dropdown-menu aria-labelledby=navbarDropdownMenuLink><a class=dropdown-item href=../../news/announce/ style=padding-left:1rem;font-weight:calc(3/3*500)>Announcements</a>
<a class=dropdown-item href=../../news/alerts/ style=padding-left:1rem;font-weight:calc(3/3*500)>Alerts</a>
<a class=dropdown-item href=../../changes/ style=padding-left:1rem;font-weight:calc(3/3*500)>Upgrades</a></div></li><li class="nav-item mr-4 mb-2 mb-lg-0" style=position:relative><i class="fas fa-users btn-primary"></i><a class="nav-link dropdown-toggle" href=../../events id=navbarDropdown role=button>
Events</a><div class=dropdown-menu aria-labelledby=navbarDropdownMenuLink><a class=dropdown-item href=../../events/small/ style=padding-left:1rem;font-weight:calc(3/3*500)>Workshops</a>
<a class=dropdown-item href=../../events/related/ style=padding-left:1rem;font-weight:calc(3/3*500)>Related Events</a></div></li><li class="nav-item mr-4 mb-2 mb-lg-0" style=position:relative><i class="fas fa-book btn-primary"></i><a class="nav-link dropdown-toggle" href=../../manuals id=navbarDropdown role=button>
Manuals</a><div class=dropdown-menu aria-labelledby=navbarDropdownMenuLink><a class=dropdown-item href=../../manuals/access style=padding-left:1rem;font-weight:calc(3/3*500)>Access</a>
<a class=dropdown-item href=../../about/facility/access/ style=padding-left:2rem;font-weight:calc(3/4*500)>How to Get Account?</a>
<a class=dropdown-item href=../../manuals/access/login style=padding-left:2rem;font-weight:calc(3/4*500)>Login</a>
<a class=dropdown-item href=../../manuals/linux_basics/ style=padding-left:1rem;font-weight:calc(3/3*500)>Linux Basics</a>
<a class=dropdown-item href=../../manuals/linux_basics/cmdline_basics style=padding-left:2rem;font-weight:calc(3/4*500)>Command Line Basics</a>
<a class=dropdown-item href=../../manuals/linux_basics/filesystems style=padding-left:2rem;font-weight:calc(3/4*500)>File Systems and Transfers</a>
<a class=dropdown-item href=../../manuals/linux_basics/permissions style=padding-left:2rem;font-weight:calc(3/4*500)>Permissions and Ownership</a>
<a class=dropdown-item href=../../manuals/linux_basics/finding_things style=padding-left:2rem;font-weight:calc(3/4*500)>Finding Things</a>
<a class=dropdown-item href=../../manuals/linux_basics/text style=padding-left:2rem;font-weight:calc(3/4*500)>Text Editors</a>
<a class=dropdown-item href=../../manuals/linux_basics/streams style=padding-left:2rem;font-weight:calc(3/4*500)>Streams</a>
<a class=dropdown-item href=../../manuals/linux_basics/pipes style=padding-left:2rem;font-weight:calc(3/4*500)>Piping</a>
<a class=dropdown-item href=../../manuals/linux_basics/variables style=padding-left:2rem;font-weight:calc(3/4*500)>Variables</a>
<a class=dropdown-item href=../../manuals/linux_basics/scripting style=padding-left:2rem;font-weight:calc(3/4*500)>Scripting</a>
<a class=dropdown-item href=../../manuals/linux_basics/processes style=padding-left:2rem;font-weight:calc(3/4*500)>Processe Management</a>
<a class=dropdown-item href=../../manuals/linux_basics/shell style=padding-left:2rem;font-weight:calc(3/4*500)>Shell Bootcamp</a>
<a class=dropdown-item href=../../manuals/hpc_cluster/ style=padding-left:1rem;font-weight:calc(3/3*500)>HPC Cluster</a>
<a class=dropdown-item href=../../manuals/hpc_cluster/intro style=padding-left:2rem;font-weight:calc(3/4*500)>Introduction</a>
<a class=dropdown-item href=../../manuals/hpc_cluster/start style=padding-left:2rem;font-weight:calc(3/4*500)>Getting Started</a>
<a class=dropdown-item href=../../manuals/hpc_cluster/jobs style=padding-left:2rem;font-weight:calc(3/4*500)>Managing Jobs</a>
<a class=dropdown-item href=../../manuals/hpc_cluster/queue style=padding-left:2rem;font-weight:calc(3/4*500)>Queue Policies</a>
<a class=dropdown-item href=../../manuals/hpc_cluster/package_manage style=padding-left:2rem;font-weight:calc(3/4*500)>Package Management</a>
<a class=dropdown-item href=../../manuals/hpc_cluster/selected_software style=padding-left:2rem;font-weight:calc(3/4*500)>Selected Software</a>
<a class=dropdown-item href=../../manuals/hpc_cluster/storage style=padding-left:2rem;font-weight:calc(3/4*500)>Data Storage</a>
<a class=dropdown-item href=../../manuals/hpc_cluster/sharing style=padding-left:2rem;font-weight:calc(3/4*500)>Sharing Data</a>
<a class=dropdown-item href=../../manuals/hpc_cluster/security style=padding-left:2rem;font-weight:calc(3/4*500)>Data Security</a>
<a class=dropdown-item href=../../manuals/hpc_cluster/users style=padding-left:2rem;font-weight:calc(3/4*500)>Communicating</a>
<a class=dropdown-item href=../../manuals/hpc_cluster/terminalide style=padding-left:2rem;font-weight:calc(3/4*500)>Terminal IDEs</a>
<a class=dropdown-item href=../../manuals/hpc_cluster/parallelr style=padding-left:2rem;font-weight:calc(3/4*500)>Parallel Evaluations in R</a>
<a class=dropdown-item href=../../manuals/hpc_cluster/sshkeys style=padding-left:2rem;font-weight:calc(3/4*500)>SSH Keys</a>
<a class=dropdown-item href=../../manuals/ext_cloud/ style=padding-left:1rem;font-weight:calc(3/3*500)>Cloud/External</a>
<a class=dropdown-item href=../../manuals/hpc_cluster/visual style=padding-left:2rem;font-weight:calc(3/4*500)>Visualization</a>
<a class=dropdown-item href=../../manuals/ext_cloud/aws/ style=padding-left:2rem;font-weight:calc(3/4*500)>AWS</a></div></li><li class="nav-item mr-4 mb-2 mb-lg-0" style=position:relative><i class="fas fa-share-alt btn-primary"></i><a class="nav-link dropdown-toggle" href=../../links/ id=navbarDropdown role=button>
Links</a><div class=dropdown-menu aria-labelledby=navbarDropdownMenuLink><a class=dropdown-item href=https://research.ucr.edu style=padding-left:1rem;font-weight:calc(3/3*500)>RED-UCR</a>
<a class=dropdown-item href=http://datascience.ucr.edu style=padding-left:1rem;font-weight:calc(3/3*500)>Data Science Center</a>
<a class=dropdown-item href=http://bigdata.ucr.edu style=padding-left:1rem;font-weight:calc(3/3*500)>FIELDS Program</a>
<a class=dropdown-item href=http://genomics.ucr.edu style=padding-left:1rem;font-weight:calc(3/3*500)>Institute of Integrative Genome Biology</a></div></li></ul></div><div class="navbar-nav d-none d-lg-block"><input type=search class="form-control td-search-input" placeholder="&#xf002; Search this site…" aria-label="Search this site…" autocomplete=off data-offline-search-index-json-src=../../offline-search-index.e93f6d5233e3f9f03770ce12ce1a9075.json data-offline-search-base-href=../../ data-offline-search-max-results=10></div></nav></header><div class="container-fluid td-outer"><div class=td-main><div class="row flex-xl-nowrap"><div class="col-12 col-md-3 col-xl-2 td-sidebar d-print-none"><div id=td-sidebar-menu class=td-sidebar__inner><form class="td-sidebar__search d-flex align-items-center d-lg-none"><input type=search class="form-control td-search-input" placeholder="&#xf002; Search this site…" aria-label="Search this site…" autocomplete=off data-offline-search-index-json-src=../../offline-search-index.e93f6d5233e3f9f03770ce12ce1a9075.json data-offline-search-base-href=../../ data-offline-search-max-results=10>
<button class="btn btn-link td-sidebar__toggle d-md-none p-0 ml-3 fas fa-bars" type=button data-toggle=collapse data-target=#td-section-nav aria-controls=td-docs-nav aria-expanded=false aria-label="Toggle section navigation"></button></form><nav class="collapse td-sidebar-nav" id=td-section-nav><ul class="td-sidebar-nav__section pr-md-3"><li class=td-sidebar-nav__section-title><a href=../../news/ class="align-left pl-0 pr-2 active td-sidebar-link td-sidebar-link__section">News</a></li><ul><li class="collapse show" id=news><a class="td-sidebar-link td-sidebar-link__page" id=m-newsannounce href=../../news/announce/>Announcements</a>
<a class="td-sidebar-link td-sidebar-link__page active" id=m-newsalerts href=../../news/alerts/>Alerts</a></li></ul></ul></nav></div></div><div class="d-none d-xl-block col-xl-2 td-toc d-print-none"><div class="td-page-meta ml-2 pb-1 pt-2 mb-0"><a href=https://raw.githubusercontent.com/ucr-hpcc/ucr-hpcc.github.io/master/content/en/News/alerts.md target=_blank class=source-link><i class="fa fa-code fa-fw"></i> View source code</a>
<a href=https://github.com/ucr-hpcc/ucr-hpcc.github.io/edit/master/content/en/News/alerts.md target=_blank><i class="fa fa-edit fa-fw"></i> Edit this page</a>
<a href="https://github.com/ucr-hpcc/ucr-hpcc.github.io/new/master/content/en/News/alerts.md?filename=change-me.md&value=---%0Atitle%3A+%22Long+Page+Title%22%0AlinkTitle%3A+%22Short+Nav+Title%22%0Aweight%3A+100%0Adescription%3A+%3E-%0A+++++Page+description+for+heading+and+indexes.%0A---%0A%0A%23%23+Heading%0A%0AEdit+this+template+to+create+your+new+page.%0A%0A%2A+Give+it+a+good+name%2C+ending+in+%60.md%60+-+e.g.+%60getting-started.md%60%0A%2A+Edit+the+%22front+matter%22+section+at+the+top+of+the+page+%28weight+controls+how+its+ordered+amongst+other+pages+in+the+same+directory%3B+lowest+number+first%29.%0A%2A+Add+a+good+commit+message+at+the+bottom+of+the+page+%28%3C80+characters%3B+use+the+extended+description+field+for+more+detail%29.%0A%2A+Create+a+new+branch+so+you+can+preview+your+new+file+and+request+a+review+via+Pull+Request.%0A" target=_blank><i class="fa fa-edit fa-fw"></i> Create child page</a>
<a href="https://github.com/ucr-hpcc/ucr-hpcc.github.io/issues/new?title=User%20alerts%20for%20HPCC&amp;#39;s%20computing%20resources" target=_blank><i class="fab fa-github fa-fw"></i> Create documentation issue</a></div><nav id=TableOfContents><ul><li><a href=#unscheduled-exceptions-and-downtimes>Unscheduled exceptions and downtimes</a></li><li><a href=#scheduled-exceptions-and-downtimes>Scheduled exceptions and downtimes</a></li><li><a href=#past-exceptions>Past exceptions</a></li><li><a href=#unannounced-exceptions>Unannounced exceptions</a></li><li><a href=#standard-operating-procedures>Standard Operating Procedures</a><ul><li><a href=#sop-for-unscheduled-outages>SOP for unscheduled outages</a></li><li><a href=#sop-for-scheduled-shutdowns>SOP for scheduled shutdowns</a></li></ul></li><li><a href=#twitter-feed>Twitter feed</a></li><li><a href=#team-collaborations-with-slack>Team collaborations with Slack</a></li></ul></nav></div><main class="col-12 col-md-9 col-xl-8 pl-md-5" role=main><nav aria-label=breadcrumb class="d-none d-md-block d-print-none"><ol class="breadcrumb spb-1"><li class=breadcrumb-item><a href=https://hpcc.ucr.edu/news/>News</a></li><li class="breadcrumb-item active" aria-current=page><a href=https://hpcc.ucr.edu/news/alerts/>Alerts</a></li></ol></nav><div class=td-content><h1>User alerts for HPCC's computing resources</h1><p class=reading-time><i class="fa fa-clock" aria-hidden=true></i> 20 minute read</p><h2 id=unscheduled-exceptions-and-downtimes>Unscheduled exceptions and downtimes</h2><p><strong>14-May-2025</strong></p><ul><li><p><strong>1:00 AM</strong>: There was a power outage affecting several buildings on campus, including our server room in SOMED1 at approximately 1AM. Until the larger 500kW UPS will be installed in late
summer, not all of our computing nodes are battery backed up. However, our core infrastructure is UPS backed up, including storage, administration nodes, etc.
Our newer &ldquo;epyc&rdquo; and modern GPU nodes (<code>gpu[06-09]</code>) are also battery backed up and should not have lost any jobs. Nodes in the &ldquo;intel&rdquo;, &ldquo;batch&rdquo;, and &ldquo;highmem&rdquo; partitions, and
nodes gpu[01-05], are not on battery backup at the moment, so any jobs running on them at the time of the outage were lost and will need to be resubmitted. We&rsquo;re sorry for this disruption,
and we&rsquo;re working on bringing the remaining nodes back online.</p></li><li><p><strong>9:00 AM</strong>: Most or all machines and services should be operational again. Jobs that were impacted should be requeued automatically.</p></li></ul><h2 id=scheduled-exceptions-and-downtimes>Scheduled exceptions and downtimes</h2><p>None observed at this time.</p><h2 id=past-exceptions>Past exceptions</h2><p><strong>17-Mar-2025</strong></p><ul><li>8:00 AM: We are still working on the getting the internal network configured in the new server room.
We are optimistic to finalize the configuration later today. Sorry again for these delays.</li><li>4:30 PM: <strong>Headnodes are available for user login again!!!</strong> We&rsquo;re now working on bringing up some compute nodes.
It&rsquo;s unlikely that all compute resources will come online by tonight.</li><li>8:00 PM: A significant portion of compute and GPU nodes have been brought online. The remaining nodes will
follow when we&rsquo;ve properly assessed power/network load balancing and server room cooling capacity. OnDemand
should be functional. Other secondary services like JupyterHub and license servers will come online tomorrow.</li></ul><p><strong>15-Mar-2025</strong></p><p>The cluster shutdown is unfortunately running behind schedule, due to unexpected delays in the moving process.
We apologize for the extended shutdown, and the negative impact this will have on your research.</p><ul><li>1:30 AM: Core HPCC infrastructure booted successfully.</li><li>10:00 AM: Sysadmins are on site to continue work on restoring cluster functionality.</li><li>9:00 PM: Sysadmins have concluded their efforts for the weekend. Network difficulties hindered our progress.</li></ul><p><strong>10-Mar-2025 to 14-Mar-2025</strong></p><p><strong><em>Move into New Server Room:</em></strong> The HPCC&rsquo;s long-awaited move to a much larger and newly renovated server room
in the SOM-ED1 building is finally happening! This major improvement will
unfortunately require about 5 days of downtime to move all HPCC systems to the
new server room, and redeploy and test them. We apologize for the disruption
this will cause to your research. This is a rare and unique event (once every
20+ years) that requires extensive organization and coordination between
multiple parties and is a major logistical undertaking.</p><p><strong><em>Benefits:</em></strong> The relocation of the HPCC will result in considerably more rack space, electrical and cooling capacity, and more dependable system operation, allowing us to accommodate substantially more CPU/GPU computing resources and data storage now and in the future. These enhancements will translate to significant benefits for all users.</p><p><strong><em>Schedule:</em></strong> Starting the morning of March 10th, users' access will be stopped, and we will begin shutting down all running jobs and services (see below). Any computing jobs that will still be running during the shutdown will need to be terminated by us. Users can restart or re-submit them after the move. Most services will be available again in the evening of March 14th.</p><p><strong><em>Services Impacted:</em></strong> All services hosted by the HPCC will be down during this
period. This includes, but is not limited to cluster access (SSH, Slurm, data
access), including HTTP access to user data; as well as services provided by OnDemand; RStudio/Posit, MATLAB, Jupyter, etc. Please note, email sent to <a href=mailto:support@hpcc.ucr.edu>support@hpcc.ucr.edu</a> will also not work (only direct email). During the move,
HPCC’s Slack workspace (<a href=https://ucr-hpcc.slack.com>https://ucr-hpcc.slack.com</a>) will be the main means of
group communication with users.</p><p><strong><em>Notes:</em></strong> As we get closer to the shutdown you might see your jobs get queued with the reason &ldquo;ReqNodeNotAvail, Reserved for maintenance&rdquo;. This will happen when the runtime of your job will overlap with the shutdown. If your job can tolerate a shorter runtime, please adjust it to not overlap. If the job requires a longer runtime, then it will need to wait until after our servers are back online.</p><p><strong>23-Feb-2025: AC Issues</strong></p><ul><li>8:00AM: The condensers on the roof of the Genomics building both lost power, which caused both CRAC units in our server room to stop cooling. Facilities is on site and are working to bring the AC units back online.</li><li>9:00AM: Facilities was able to bring one of the condensers back online, and are working on bringing one of the AC units back to alleviate some of the heat. To prevent our power infrastructure from overheating, we&rsquo;ve had to kill running jobs on the cluster. We apologize for this inconvenience.</li><li>12:00PM: Emergency repairs on the AC system are complete, and load-shedded servers have been brought back online. Thank you for your patience.</li></ul><p><strong>02-Nov-2024: AC Issues</strong></p><ul><li>11:00AM: One of our 2 redundent AC units has began blowing hot air into the server room. While facilities diagnoses and resolves the issue we have paused the Slurm queue. We are sorry for the interruption.</li><li>12:00PM: Update: to avoid overheating the power in the HPCC had to be shut down. This affects all HPCC services hosted from the Genomics Server room, including SSH login, Slurm jobs, and e-mail ticket system. We&rsquo;re working on bringing services up safely and quickly.</li><li>4:00PM: Currently, the cluster is still down because of a problem with one of the AC units. Physical plant and the systems administrator are working hard to bring everything back online again. As of now, it is hard to tell how long it will take until things are back to normal again. Please be patient and check the alert page here as well as your Slack messages.</li><li>5:00AM (Nov 3): Headnodes are available for login. The /rhome and /bigdata storage systems are fully functional, so you may retrieve your files and do some light work. A limited number of compute and GPU nodes may come online depending on sysadmin discretion and cooling constraints. AC repairs will begin on Monday at the earliest, and these repairs must be completed before we can bring the cluster back fully online. Again, we apologize for the disruption.</li><li>12:00PM (Nov 3): A small number of nodes from each partition have been made available. We will slowly release more nodes as we monitor the temperature and work on the AC unit continues.</li><li>12:00PM (Nov 4): A temporary fix is in place as facilities awaits parts for a complete fix. We are slowly increasing the number of available compute nodes, though with limited job runtimes. Currently any job with a runtime that will end before Nov 10 at Midnight will run. As temperatures in the room hold, we will increase the time limit and number of nodes in operation.</li><li>11:00AM (Nov 7): Facilities is in the process of obtaining the equipment necessary for the repairs. We&rsquo;ve been told by them that the repair will not require shutting down the AC units, so <strong>we can resume operations as normal. We have removed the existing reservation, so long-running jobs may begin. We will continue to release the remaining offline nodes throughout the day as well.</strong></li></ul><p><strong>24-Jun-2024: AC Work Completed</strong>
The AC repairs have been completed by facilities. Over the next few hours we will slowly begin to bring nodes back online.</p><p><strong>22-Jun-2024: Update on AC Unit Problems</strong></p><p>UCR Facilities was able to return the affected AC unit to working order. However, the unit is still in suboptimal condition, and will have to be powered off for full maintenance on Monday (Jun 24). Until then, HPCC will have to limit the capacity of the Slurm cluster. This compromise will allow maintenance to be safely performed without resorting to a full emergency cancellation of all jobs.</p><p><strong>21-Jun-2024: Network Outage and HVAC Problem</strong></p><p>Starting 5:15 PM the network connection to the HPCC server room is down. This might be related to a larger network problem on campus, see <a href=https://techalerts.ucr.edu/>here</a>. Update 7:30 PM: the network connection is working again, but as it turns out there also is a problem with one of the HVAC units in the server room. It is still unclear how the two problems are connected. Currently, facilities is working on the HVAC problem. To avoid overheating, the Slurm queue has been paused by the sys admins. Current jobs will continue to run, while pending jobs will need to wait until the situation improves.</p><p><strong>CANCELLED: June 14th Shutdown Ahead of Genomics Electrical Maintenance</strong></p><p>Starting on Friday, June 14th at 8am and extending to Saturday, June 15th at 11pm UCR HPCC will be powering down the cluster ahead of a scheduled electrical shutdown of the UCR Genomics Building. To make the most of this downtime, HPCC will offline the cluster on the 14th in order to install routine software updates and perform other minor maintenance tasks. During the shutdown, most of our online services hosted from the UCR Genomics Building will be unavailable including, but is not limited to: SSH, Slurm, Rstudio, JupyterHub, OnDemand, and web file access. Please save your work on any of these services before the maintenance window. E-mail support may be temporarily interrupted, but should otherwise remain online. We recommend checking our <a href=https://ucr-hpcc.slack.com/>Slack channel</a> for any minor status updates during the shutdown.</p><p>If you submit a Slurm or OnDemand job that extends into the maintenance window, you will receive an error containing &ldquo;ReqNodeNotAvail&rdquo;. Your job will queue, but will not start, until maintenance is over. If you want your job to start sooner, cancel the job request, and resubmit your job with a shorter &ndash;time duration, such that your job will finish before the maintenance window begins.</p><p>We apologize for the disruption to your research and teaching workflows. Thanks for your understanding.</p><p><strong>7-May-2024: AC Unit Repairs, Slurm Paused</strong></p><p>After investigating a High Temperature warning on one of our AC units, campus Facilities determined a component in one of the AC units appears to be faulty and needs to be replaced. They are planning on receiving the replacement part the morning of the 8th, but in order to do the replacement both AC units will need to be taken offline. We&rsquo;ve began putting nodes into a &ldquo;draining&rdquo; state in an attempt to bring the load on the cluster as low as possible to best manage heat in the server room. Existing jobs will continue to run, but newly scheduled jobs will be put in the queue until we can confirm that the repair has been completed and the AC units are online again. We know this is disruptive to your research and teaching, and we apologize for this development.</p><p><strong>Update 1:</strong> Facilities was unable to complete the job in their time allotted for today, and will continue work tomorrow the morning (May 9th).</p><p><strong>Update 2:</strong> While performing the repair, facilities identified a leak which needs to be patched before refilling the refrigerant, otherwise any refrigerant would immediately escape. The ETA for this repair, as given to us by facilities, is May 10th.</p><p><strong>Update 3:</strong> Facilities has contacted us to say that they are finishing up the repairs and that we can begin to start loading the servers again. As such we&rsquo;ve released ~50% of the nodes to begin running jobs again.</p><p><strong>Update 4:</strong> Facilities has completed the repairs, and all nodes have been released.</p><p><strong>08-Mar-2024 AC Unit Refrigerant Leak, Queue Paused</strong></p><p>The UCR HPCC Slurm queue has been paused due to another AC malfunction in the server room. Earlier on Thursday (March 7th) afternoon, UCR Facilities was called in to check an AC alarm and determined that one of our AC units had low refrigerant. While attempting to recharge the AC refrigerant a leak occurred, forcing personnel to evacuate the room for their own safety. They intend to start the repair first thing on Friday (March 8th) morning. But because that AC unit is still not running at full capacity, cooling is limited. As such, the Slurm queue has been paused as a precaution to keep the room within a safe temperature. We understand that this will be disruptive to your work, and we apologize for this development.</p><p>Update 1: After speaking with the person performing the repair, they said that the repair could take all day. We will make further announcements as we receive updates.</p><p>Update 2: The repair has been completed and the Slurm queue has been released.</p><p><strong>22-Feb-2023: Upgrades of OS and GPFS</strong></p><p>Starting Thursday, February 22nd at 8:00am and lasting until Friday, February 23rd at 8:00pm, the cluster will be unavailable due
to an upgrade of the Operating System as well as our central storage system. Any submitted jobs that overlap with the shutdown time will
be queued with the reason &ldquo;ReqNodeNotAvail&rdquo; and will need to be requeued with a shorter time or wait until the maintenance is over
at which point they will automatically start.</p><p>During the shutdown services will be unavailable including, but not limited to: SSH, RStudio, JupyterHub, OnDemand, and web-based file access.
If you need additional information or help, you can reach us at our <a href=https://ucr-hpcc.slack.com/>Slack</a> or outside of the shutdown through
email (<a href=mailto:support@hpcc.ucr.edu>support@hpcc.ucr.edu</a>).</p><p><strong>Update</strong>: The shutdown had finished and the Slurm queue has been reopened.</p><p><strong>19-Jan-2024: AC Unit Repair Followup</strong></p><p>Due to followup maintenance required following the January 17th repairs, the
slurm queue will once again need to be haulted in order to manage heat output
in the server room. Maintenance should only take a few hours, after which the
queue will be released again.</p><p><strong>Update</strong>: The repair has been completed. Nodes will be resumed and queue opened.</p><p><strong>17-Jan-2024: AC Unit repair</strong></p><p>The AC units in the HPCC server room are experiencing issues. To keep the
operating temperature within safe limits, the job queue has been halted. New
and pending jobs will stay queued, while currently running jobs will be allowed
to finish. We apologize for the inconvenience.</p><p><strong>Update</strong>: The maintenance has completed, but a followup repair will be required in
order to return to 100% operation of the AC units.</p><p><strong>14 Aug, 2023: Login and GPFS storage issues</strong></p><ul><li>The login service on the primary headnode failed on Monday afternoon. A new headnode with updated software is now
online, so users may access their files and do some light work. The GPFS cluster file system also exhibited some
performance stalls. The Slurm scheduler has been paused until this issue subsides.</li></ul><p><strong>4-Aug-2023: Upgrades of OS, GPFS, Slurm, Head Nodes and Power Distribution</strong></p><ul><li><p>HPCC staff will be performing maintenance work on Aug 4th to Aug 5th that will require a shut down of the HPCC cluster.
Objectives for this event include the following upgrades: GPFS, Slurm, OS upgrade to Rocky Linux 8.8,
electrical reconfiguration of PDUs, and various other maintenance tasks.</p></li><li><p>Update Aug 6th: Due to underestimated workload on planned maintenance tasks, services are still offline.
Basic functionality should be restored by the end of the day.</p></li><li><p>Update Aug 7th: Maintenance is still ongoing. Currently, we are redeploying computer nodes and other services.
User login is restricted until the central data storage system has been re-mounted.</p></li><li><p>Update Aug 8th: SSH login has been restored. Users can access their data and perform light work on the head nodes
until access to a larger number of compute nodes has been restored. In addition, web-based file sharing and JupyterHub are available again.
RStudio Server will be restored next.</p></li></ul><ul><li><p>Update Aug 9th: Slurm has been redployed and is operational. A larger number of computer nodes have been redeployed and are available
to users again.</p></li><li><p>Final update Aug 11th: All remaining services are available again. The maintenance is complete. This includes remaining CPU and GPU nodes, etc.
We apologize for the extended time it took us to bring all services online again. Thank you for your understanding.</p></li></ul><p><strong>17-18 Jul, 2023: Slurm halted</strong></p><ul><li>Facilities had to perform maintenance on the AC units due to the formation of condensation water by the AC units,
possibly caused by hot summer days. During the maintenance Slurm jobs had to be halted to avoid overheating.
Running jobs will be allowed to continue, provided that the server room does not get too hot.</li></ul><p><strong>10-Apr-2023: Bigdata back</strong></p><ul><li>5:30 PM - Bigdata is back. Thank for for your patience.</li></ul><p><strong>10-Apr-2023: Bigdata down</strong></p><ul><li>4:30 PM - Bigdata is currently down. Please be patient&mldr;</li></ul><p><strong>22-Dec-2022: Network outage</strong></p><ul><li>9:00 AM - Due to a network outage the cluster was inaccessible for several hours.</li></ul><p><strong>1-Nov-2022: Network router repair</strong></p><ul><li>9:00 PM - ITS had to repair a router in the Genomics Building. Around 4:00 AM
in the morning (Nov 2nd) network access to the Genomics Building became available
again. During the affected time window the cluster was not accessible (<em>e.g.</em> via ssh).
Processes running on the cluster were not affected.</li></ul><p><strong>25-Jun-2021: Bigdata storage repaired</strong></p><ul><li>5:00 PM - Server running our bigdata storage have been recovered, and all functions of bigdata directory is now back to normal.</li></ul><p><strong>25-Jun-2021: Bigdata storage failed</strong></p><ul><li>3:30 PM - Server running our bigdata storage crashed, and bigdata directory went down with it.</li></ul><p><strong>12-Jan-2020: AC unit repaired</strong></p><ul><li>5:00 PM - AC repairs have been completed. The reservation has been removed, and new Slurm jobs are now no longer suspended.</li></ul><p><strong>11-Jan-2020: AC unit failed</strong></p><ul><li>3:00 PM - One of our AC units is under emergency repairs. A Slurm reservation was put in place to suspend new jobs from running.</li></ul><p><strong>5-6 May, 2023: Maintenance and Electrical Power Upgrades in Server Room</strong></p><ul><li><p>UCR Facilities Services will be upgrading our electrical capacity in the Genomics server room. To take advantage
of the unavoidable system downtime, the HPCC will perform general maintenance and various upgrades on the cluster. The
shutdown is expected to last 2 days. It has been scheduled during a weekend to minimize disruptions for users.</p></li><li><p>Update: since Sat/Sun night, most systems are back online again. If users notice any missing functionality, please
let us know at <a href=mailto:support@hpcc.ucr.edu>support@hpcc.ucr.edu</a>, or at our Slack channel (<a href=https://ucr-hpcc.slack.com>https://ucr-hpcc.slack.com</a>). Thanks you for your
patience and understanding.</p></li></ul><p><strong>28-Oct-2020: Cluster jobs failed due to storage suspension</strong></p><ul><li>3:00 PM - During a routine extension of the bigdata filesystem, there were some complications and disk i/o had to be suspended.</li><li>5:30 PM - We have repaired the issue, and everything should be functioning as usual. However, this means that all computing jobs running during timeframe were stopped and will need to be restarted.</li></ul><p><strong>19-Aug-2020: Cluster inaccessible due to power outage in Genomics Bdg</strong></p><ul><li>11:30 PM - All systems were restored by Jordan Hayes and are opterational again.</li><li>10:30 PM - HPC systems admin Jordan Hayes is trying to restart the network, storage and cluster again.</li><li>10:00 PM - Facilities was able to bring up the power and cooling again.</li><li>8:30 PM - Facilities is investigating and trying to reactivate power and cooling.</li></ul><p><strong>10-Aug-2020: Cluster inaccessible due to power outage in Genomics Bdg</strong></p><p>At 5:10 PM: Facilities has restored power and cooling systems in the server room. HPC systems admin Jordan Hayes is restarting the cluster and storage systems.
At 10:10 PM: All HPCC services were restored (computing cluster, storage systems, web services).</p><p><strong>22-Mar-2020: Cluster inaccessible due to campus-wide network outage</strong></p><p>Due to a campus-wide network outage at UCR, many HPCC services were not accessible between 8:00 AM and 1:00 PM.
Currently, most HPCC services are accessible again. Note, running jobs on the cluster should not have been affected by this disruption.
Updates about the current situations can be found <a href=https://techalerts.ucr.edu/>here</a>.</p><p><strong>13-Mar-2020: Routine maintenance shutdown</strong></p><p>We have scheduled an HPCC Cluster Maintenance Shutdown for Friday, March 13th.
This will require a shutdown of ALL services: Slurm (queuing system), hosted websites,
virtual environments/machines, storage systems, backup systems, and network services.
We ask that you please make sure that you do not have any jobs running in the queue,
and that you completely logout of the cluster (pigeon, pelican, parrot) before the shutdown.</p><p><strong>08-Jan-2020: Storage outage</strong></p><p>We had some issues with our storage systems this evening that may have caused
disruptions in your work. These issues should be resolved. We&rsquo;re continuing to
monitor the situation to ensure everything is operational, and we apologize for
any inconveniences this may have caused. Please let us know at
<a href=mailto:support@hpcc.ucr.edu>support@hpcc.ucr.edu</a> if you require any assistance regarding job status and
recovery.</p><p><strong>21-Nov-2019: Routine filesystem maintenance and diagnostics</strong></p><p>We have scheduled an HPCC Cluster Maintenance Shutdown for this Thursday, November 21st.
This will require a shutdown of ALL services: Slurm (queuing system), hosted websites, virtual environments/machines, storage systems, backup systems, and network services.
We ask that you please make sure that you do not have any jobs running in the queue, and that you completely logout of the cluster (pigeon, pelican, parrot) before the shutdown.</p><p><strong>23-Aug-2019: Routine maintenance shutdown</strong></p><p>We have scheduled an HPCC Cluster Maintenance Shutdown for Friday, Aug 23, 2019.
This will require a shutdown of ALL services: Slurm (queuing system), hosted
websites, virtual environments/machines, storage systems, backup systems, and
network services. We ask that you please make sure that you do not have any
jobs running in the queue, and that you completely logout of the cluster
(pigeon, pelican, globus) before the shutdown. Status: completed.
For user changes related to this maintenance please see <a href=../../changes>here</a>.</p><p><strong>01-Mar-2019: Routine Maintenance Shutdown</strong></p><p>We have scheduled an HPCC Cluster Maintenance Shutdown for Friday, March 1st.
This will require a shutdown of ALL services: Slurm (queuing system), hosted
websites, virtual environments/machines, storage systems, backup systems, and
network services. We ask that you please make sure that you do not have any
jobs running in the queue, and that you completely logout of the cluster
(pigeon, pelican, globus) before the shutdown. Status: successfully completed.</p><p><strong>1:00 PM, 20-Dec-18: Outage due to AC failure</strong></p><p>All systems were down for 3 hours due to a failure of the AC units in our server
room. Electricians and AC technicians have repaired the units.</p><p><strong>2:30 PM, 11-Jul-18: Storage Issues</strong></p><p>For the past several weeks we have been observing slower storage access.
In some cases the /bigdata storage was inaccessible for several minutes and caused some jobs to terminate prematurely.
We have identified the issue and have taken steps to ensure that this problem does not reoccur.</p><p><strong>6:00 PM, 02-Jul-18: Storage Issues</strong></p><p>Storage issues on the afternoon of July 2, 2018 caused disruptions in some cluster services. The issues should be resolved, but we&rsquo;re continuing to monitor the situation for any other developments.</p><p><strong>12:00 AM, 31-Jan-18: routine maintenance shutdown</strong></p><p>For routine maintenance and upgrades we have scheduled an HPCC (Biocluster)
shutdown for 12:00AM, Jan-31-2018 to 12:00AM, Feb-01-2018. (complete)</p><p><strong>12:00 AM, 05-Dec-17: NFS & SMB issues</strong></p><p>NFS and SMB services have been suspended temporarily.
This will cause many of our web services to not function properly.
These include, but not limited to:</p><ul><li><a href=https://rstudio.bioinfo.ucr.edu>https://rstudio.bioinfo.ucr.edu</a> & <a href=https://rstudio2.bioinfo.ucr.edu>https://rstudio2.bioinfo.ucr.edu</a></li><li><a href=https://galaxy.bioinfo.ucr.edu>https://galaxy.bioinfo.ucr.edu</a></li><li><a href=https://dashboard.bioinfo.ucr.edu>https://dashboard.bioinfo.ucr.edu</a></li><li><a href=https://biocluster.ucr.edu/~username>https://biocluster.ucr.edu/~username</a> (.html directories)</li><li>mysql://bioclusterdb.int.bioinfo.ucr.edu (databases)</li></ul><p>Note, this issue was resolved soon after it occurred.</p><p><strong>11:00 AM, 13-Aug-17: Cooling problem</strong></p><p>Since Sat morning one of the HVAC units is not working properly. To avoid overheating,
we have shut down most of the idle nodes (1:30PM, Sun). As soon as the HVAC unit
is repaired we will power these nodes back on. Note, this issue was resolved on 17-Aug-17.
UCR facility services has repaired the broken HVAC unit and serviced the second one.</p><p><strong>12:00 AM, 16-Jun-17 to 17-Jun-17: maintenance shutdown</strong></p><p>To sustain future growth, the power load in the HPCC server room needs to be
optimized. For this we have scheduled an HPCC (Biocluster) shutdown in four
weeks from now which will start at noon on June 16th and last until noon June 17th. This
will require a shutdown of ALL services: Slurm (queuing system), hosted
websites, virtual environments/machines, storage access, backup systems and
network services. We ask that you please make sure that you do not have any
jobs running in the queue, and that you completely logout of the cluster
(pigeon, owl, penguin, pelican, globus) before the shutdown.</p><p><strong>10:02 AM, 13-Apr-17: UPS failure</strong></p><p>Our UPS unit went down some time last night causing a power failure on all systems. Jordan is bypassing the UPS to bring things back up in the next
few hours. Nationwide Power will come in asap to repair the UPS. Note, this failure has not resulted in any overheating problems since the AC units
are running on a different power cricuit.</p><p><strong>11:22 AM, 13-Apr-17: Cluster back up running</strong></p><p>Situation is resolved for now and things are working. We are currently discussing the situation with our electricians to avoid future instances.</p><h2 id=unannounced-exceptions>Unannounced exceptions</h2><p>None currently observed.</p><h2 id=standard-operating-procedures>Standard Operating Procedures</h2><h3 id=sop-for-unscheduled-outages>SOP for unscheduled outages</h3><p>When unforeseen issues arise they are categorized by severity:</p><ol start=0><li>Green - Normal operation, no current issues</li><li>Yellow - Minor issue[s], likely not observed by users (ie. jobs are not affected)</li><li>Orange - Medium issue[s], likely observed by users but not fatal (ie. jobs may perform slower than usual)</li><li>Red - Critical issue[s], major service or entire cluster is not functioning as expected (ie. jobs have terminated prematurely)</li></ol><p>Email notifications are only sent to users if there is a Red critical issue.</p><h3 id=sop-for-scheduled-shutdowns>SOP for scheduled shutdowns</h3><p>The following outlines the timeline for advance email notifications on scheduled shutdowns of the HPCC cluster and other exceptions:</p><ol><li>Four weeks advance notice</li><li>Followed by weekly reminders</li><li>Final reminder the day before the outage</li></ol><h2 id=twitter-feed>Twitter feed</h2><p>For additional news and information, please consult the <a href=https://twitter.com/UCR_HPCC>HPCC Twitter
site</a>. Also see the Tweets window at the bottom
of this and other pages of the HPCC website.</p><h2 id=team-collaborations-with-slack>Team collaborations with Slack</h2><p>Sign up and use Slack Team Collaboration app here: <a href=https://ucr-hpcc.slack.com>ucr-hpcc.slack</a></p><div class="text-muted mt-5 pt-3 border-top">Last modified May 14, 2025: <a href=https://github.com/ucr-hpcc/ucr-hpcc.github.io/commit/b59b0790aaaae5352e81efde1f678377c53322c6>power outage notification (b59b0790a)</a></div></div></main></div></div><footer class="bg-dark py-5 row d-print-none"><div class="container-fluid mx-sm-5"><div class=row><div class="col-6 col-sm-4 text-xs-center order-sm-2"><ul class="list-inline mb-0"><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title="Email Support" aria-label="Email Support"><a class=text-white target=_blank rel="noopener noreferrer" href=mailto:support@hpcc.ucr.edu><i class="fa fa-envelope"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Twitter aria-label=Twitter><a class=text-white target=_blank rel="noopener noreferrer" href=https://twitter.com/UCR_HPCC><i class="fab fa-twitter"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title="Stack Overflow" aria-label="Stack Overflow"><a class=text-white target=_blank rel="noopener noreferrer" href=https://ucr-hpcc.slack.com/><i class="fab fa-slack"></i></a></li></ul></div><div class="col-6 col-sm-4 text-right text-xs-center order-sm-3"><ul class="list-inline mb-0"><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title aria-label><a class=text-white target=_blank rel="noopener noreferrer" href><i></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title aria-label><a class=text-white target=_blank rel="noopener noreferrer" href><i></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title aria-label><a class=text-white target=_blank rel="noopener noreferrer" href><i></i></a></li></ul></div><div class="col-12 col-sm-4 text-center py-2 order-sm-2"><small class=text-white>&copy; 2025 The Docsy Authors All Rights Reserved</small>
<small class=ml-1><a href=https://policies.google.com/privacy target=_blank>Privacy Policy</a></small></div></div></div></footer></div><script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.3/umd/popper.min.js integrity=sha384-ZMP7rVo3mIykV+2+9J3UJ46jBk0WLaUAdn689aCwoqbBJiSnjAK/l8WvCWPIPm49 crossorigin=anonymous></script><script src=https://stackpath.bootstrapcdn.com/bootstrap/4.1.3/js/bootstrap.min.js integrity=sha384-ChfqqxuZUCnJSK3+MXmPNIyE6ZbWh2IMqE241rYiqJxyMiZ6OW/JmZQ5stwEULTy crossorigin=anonymous></script><script>(function(){var c,a,b,d=document.getElementsByTagName('code');for(c=0;c<d.length;){if(b=d[c],b.parentNode.tagName!=='PRE'&&b.childElementCount===0)if(a=b.textContent,/^\$[^$]/.test(a)&&/[^$]\$$/.test(a)&&(a=a.replace(/^\$/,'\\(').replace(/\$$/,'\\)'),b.textContent=a),/^\\\((.|\s)+\\\)$/.test(a)||/^\\\[(.|\s)+\\\]$/.test(a)||/^\$(.|\s)+\$$/.test(a)||/^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(a)){b.outerHTML=b.innerHTML;continue}c++}})()</script><script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?version=4.8.0&features=es6"></script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><script type=text/x-mathjax-config>
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
        processEscapes: true,
        processEnvironments: true
    },
    // Center justify equations in code and markdown cells. Elsewhere
    // we use CSS to left justify single line equations in code cells.
    displayAlign: 'center',
    "HTML-CSS": {
        styles: {'.MathJax_Display': {"margin": 0}},
        linebreaks: { automatic: true }
    }
});
</script><script src=../../js/main.min.63db3e552bd0543be17ce4a79a18b744e9cb72256bec42b540e3ab0cd43722d0.js integrity="sha256-Y9s+VSvQVDvhfOSnmhi3ROnLciVr7EK1QOOrDNQ3ItA=" crossorigin=anonymous></script><script src=../../js/prism.js></script></body></html>