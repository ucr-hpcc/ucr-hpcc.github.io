[{"body":"","excerpt":"","ref":"/manuals/access/","title":"Access"},{"body":"AlphaFold3 Loading the module You can load AlphaFold3 using the following commands:\nmodule load alphafold/3 singularity shell $ALPHAFOLD_SING  You can also run AlphaFold3 with a gpu. If you wish to use a GPU, log into an A100 gpu node and then use the following commands:\nmodule load alphafold/3 singularity shell --nv $ALPHAFOLD_SING  Using AlphaFold databases A handful of databases are available at $ALPHAFOLD_DB (available after loading the alphafold/3 module).\nAn example command is as follows:\nmodule load alphafold/3 singularity shell --nv $ALPHAFOLD_SING # Commands from here on are run inside of the Alphafold container python3 /app/alphafold/run_alphafold.py \\ --model_dir=$ALPHAFOLD_DB/model \\ --db_dir=$ALPHAFOLD_DB \\ --json_path=fold_input.json \\ --output_dir=my_output_folder/  More information on using Alphafold3 can be found in the Alphafold3 GitHub repo, including input documentation and output documentation.\nProcessing Large Datasets Sometimes the dataset cannot fit within the memory of a single GPU. In this case you’ll need to use Unified Memory (“Combined” GPU and System memory). This does come with a drop in performance, but might be the only way to get large datasets processed.\nTo use Unified Memory, you can add these additional flags to the alphafold command:\n--env XLA_PYTHON_CLIENT_PREALLOCATE=false \\ --env TF_FORCE_UNIFIED_MEMORY=true \\ --env XLA_CLIENT_MEM_FRACTION=3.2  For example:\npython3 /app/alphafold/run_alphafold.py \\ --model_dir=$ALPHAFOLD_DB/model \\ --db_dir=$ALPHAFOLD_DB \\ --json_path=fold_input.json \\ --env XLA_PYTHON_CLIENT_PREALLOCATE=false \\ --env TF_FORCE_UNIFIED_MEMORY=true \\ --env XLA_CLIENT_MEM_FRACTION=3.2 \\ --output_dir=my_output_folder/  AlphaFold2 Description of AlphaFold2\nLoading the module You can load AlphaFold2 using the following commands:\nmodule load alphafold/2 singularity shell $ALPHAFOLD_SING  You can also run AlphaFold2 with a gpu. If you wish to use a GPU, log into a P100 gpu node and then use the following commands:\nmodule load alphafold/2 singularity shell --nv $ALPHAFOLD_SING  Using Alphafold Databases When running the alphafold command, you will be asked for certain databases. These databases can be found under the path $DATABASE_DIR/alphafold/. They can also be accessed using the $$ALPHAFOLD_DB environment variable that is automatically set after loading the alphafold module.\nHere is an example of how to write your alphafold command using the monomer preset:\npython3 /app/alphafold/run_alphafold.py \\ --model_preset=monomer \\ --db_preset=reduced_dbs \\ --use_gpu_relax=True \\ --data_dir=$ALPHAFOLD_DB \\ --uniref90_database_path=$ALPHAFOLD_DB/uniref90/uniref90.fasta \\ --mgnify_database_path=$ALPHAFOLD_DB/mgnify/mgy_clusters_2018_12.fa \\ --template_mmcif_dir=$ALPHAFOLD_DB/pdb_mmcif/mmcif_files \\ --max_template_date=2020-05-14 \\ --obsolete_pdbs_path=$ALPHAFOLD_DB/pdb_mmcif/obsolete.dat \\ --pdb_seqres_database_path=$ALPHAFOLD_DB/pdb_seqres/pdb_seqres \\ --uniprot_database_path=$ALPHAFOLD_DB/uniprot/uniprot.fasta \\ --small_bfd_database_path=$ALPHAFOLD_DB/small_bfd/bfd-first_non_consensus_sequences.fasta \\ --pdb70_database_path=$ALPHAFOLD_DB/pdb70/pdb70 \\ --fasta_paths=\u003cpath to fasta file here\u003e \\ --output_dir=\u003cpath to output directory\u003e  and an example using the multimer preset:\npython3 /app/alphafold/run_alphafold.py \\ --model_preset=multimer \\ --db_preset=reduced_dbs \\ --use_gpu_relax=True \\ --data_dir=$ALPHAFOLD_DB \\ --uniref90_database_path=$ALPHAFOLD_DB/uniref90/uniref90.fasta \\ --mgnify_database_path=$ALPHAFOLD_DB/mgnify/mgy_clusters_2018_12.fa \\ --template_mmcif_dir=$ALPHAFOLD_DB/pdb_mmcif/mmcif_files \\ --max_template_date=2020-05-14 \\ --obsolete_pdbs_path=$ALPHAFOLD_DB/pdb_mmcif/obsolete.dat \\ --small_bfd_database_path=$ALPHAFOLD_DB/small_bfd/bfd-first_non_consensus_sequences.fasta \\ --uniprot_database_path=$ALPHAFOLD_DB/uniprot/uniprot.fasta \\ --pdb_seqres_database_path=$ALPHAFOLD_DB/pdb_seqres \\ --fasta_paths=\u003cpath to fasta file\u003e \\ --output_dir=\u003cpath to output directory\u003e  Remember to fill in your fasta path and output dir if you wish to use these templates.\nAdditionally, these are not the only two methods of running AlphaFold, and different modes might require different sets of arguments to be passed to alphafold.py. For more details regarding what parameters are available, as well as more examples, please refer to the Alphafold Github Repo.\n","excerpt":"AlphaFold3 Loading the module You can load AlphaFold3 using the …","ref":"/manuals/hpc_cluster/selected_software/alphafold/","title":"AlphaFold Usage on HPCC"},{"body":"Note: for the most current information on exceptions on HPCC’s cluster please consult its Alerts or Twitter pages.\nMar 2025  Finally the HPCC will be moving its computing infrastructure from Genomics to the newly renovated and much larger server room in the SOM-ED1 building. Details are provided here.  Oct 2024  To allow for more users to gain access our GPUs, we have reduced the per-user GPU limit from 8 to 4. Additionally, we have reduced the maximum job runtime for the GPU partition from 30 days down to 7. Should you need an increase in the time limit or number of GPUs you can use in parallel, please email support@hpcc.ucr.edu (please also CC your PI) with a short justification and timeline for the increase.  Oct 2023  The new NSF-MRI funded cluster is now available to users. It includes 2 head (called bluejay and skylark) and 20 compute nodes, each with two 64-core AMD EPYC CPUs and 1TB of RAM. In total this adds 2,816 CPU cores to our HPC infrastructure. In addition, 2 powerful GPU nodes were installed, each with 8x A100 GPUs and two AMD EPYC CPUs. Combined with our existing 8x A100 GPU HGX system, this gives us a total of 24x A100 GPUs which are some of the most powerful GPUs on the market. The new CPU and GPU nodes are connected via the latest NDR Infiniband network (400-800Gb/s). The new CPU nodes can be accessed via a newly added partition (queue) called “epyc”. The A100 GPU nodes (gpu[07-08]) can be accessed via the “gpu” partition. Additional details on using the new partitions and user quota can be found here and here.  Sep 2023  Stata 17 has been superseded with Stata 18. Please update scripts relying on the stata/17 module to use the stata/18 module instead.  Aug 2022  In August 2022 we were awarded an MRI equipment grant (#2215705) by NSF for the acquisition of a Big Data HPC Cluster in the total amount of $942,829.  Feb/Mar 2022  Rollout of Rocky and DUO: Feb 18 through Mar 17, 2022. For details see here.  Mar 2021  Viet Pham joins HPCC as HPC systems administrator. - Welcome Viet!  May/Jun 2020  Melody Asghari joins HPCC as HPC systems administrator assistant. - Welcome Melody!  Mar/Apr 2020  For updates on HPCC’s operation during the COVID-19 crisis please visit the Alerts page of this site.  Sep 2019  Travis Nasser joins HPCC as HPC systems administrator assistant. - Welcome Travis! Major updates applied. - Check here for important changes.  Aug 2019  Charles Forsyth left the position of HPC systems administrator and has moved on to new opportunities. - Good luck Chuck!  Apr 2018  Abraham Park joins HPCC as HPC systems administrator assistant. - Welcome Abe!  Jun 2017  Charles Forsyth joins HPCC as new full-time HPC systems administrator. - Welcome Chuck!  Feb 2017 With funding provided by Michael Pazzani’s office (RED) we were able to purchase and install major hardware upgrades. This included the following hardware resources:\n Added 28 Intel nodes with a total of 896 CPU cores (or 1,792 logical CPU cores) and 512 GB of RAM per node Added 8 NVIDIA K80 GPUs increasing total number of cuda cores in GPU queue to 59,904 Redesign of Infiniband network to support new computer nodes and enhance future scalabilty of IB network to over 1000 nodes  Dec 2016  UCR approval of plans to form HPC Center  Sept 2016  Expansion of GPFS storage system: 2 disk enclosures for 120 8TB drives Expansion of high-memory queue: 4 nodes Install of new Intel batch queue: 12 nodes  Mar 2016  Expansion of batch queues: 14 nodes  Apr 2015  Deployment of new FDR IB network @ 56Gbs Deployment of 28 AMD nodes (2,048 AMD cores), funded by NSF-MRI-2014 Deployment of high-memory Intel nodes (each with 1TB RAM) Deployment of GPU nodes (NVIDIA K80) Deployment of big data GPFS disk storage system, funded by NIH-S10-2014  May 2014 Award of equipment grants from NSF and NIH\n NIH-S10-2014 (1S10OD016290-01A1): $652,816 NSF-MRI-2014 (MRI-1429826): $783,537  ","excerpt":"Note: for the most current information on exceptions on HPCC’s cluster …","ref":"/news/announce/","title":"News and announcements"},{"body":"What software is installed? There are hundreds of software tools installed on HPCC’s systems. Most software is administered under a module system. To find out what software is installed, users want to consult the software listing here or run from a user account the following command.\nmodule avail  More details on this is available on the HPCC manual pages here.\nSoftware install requests Registered users can email software install requests to HPCC’s issue tracking system @ support@hpcc.ucr.edu. Install requests are addressed in the order received. Simple installs are addressed within 1 to a few days. Complex installs may take longer.\n","excerpt":"What software is installed? There are hundreds of software tools …","ref":"/about/software/installs/","title":"Software Installs"},{"body":"Introduction This manual provides an introduction to the usage of the HPCC cluster. All servers and compute resources of the HPCC cluster are available to researchers from all departments and colleges at UC Riverside for a minimal recharge fee (see rates). To request an account, please email support@hpcc.ucr.edu. The latest hardware/facility description for grant applications is available here.\nOverview Storage  Four enterprise class HPC storage systems Approximately 6 PB of total network storage (3,072 TB production and 3,072 TB backup) GPFS (NFS and SAMBA via GPFS) Automatic snapshots and archival backups  Network  Ethernet  1 Gb/s switch x 5 1 Gb/s switch 10 Gig uplink 10 Gb/s switch for Campus wide Science DMZ redundant, load balanced, robust mesh topology   Interconnect  56 Gb/s InfiniBand (FDR)    Head Nodes All users should access the cluster via ssh through cluster.hpcc.ucr.edu, this address will automatically balance traffic to one of the available head nodes.\n Jay  Resources: 64 cores, 512 GB memory Primary function: submitting jobs to the queuing system Secondary function: development; code editing and running small (under 50 % CPU and under 1 GB RAM) sample jobs   Lark  Resources: 64 cores, 512 GB memory Primary function: submitting jobs to the queuing system Secondary function: development; code editing and running small (under 50 % CPU and under 1 GB RAM) sample jobs    Worker Nodes  Batch  c01-c48: each with 64 AMD cores and 512 GB memory   Intel  i01-i40: each with 32 Intel Broadwell cores and 512 GB memory   Epyc  r21-r38: each with 64 AMD EPYC cores and 1 TB memory   Highmem  h01-h06: each with 32 Intel cores and 1024 GB memory   GPU  gpu01-gpu02: each with 32 (HT) cores Intel Haswell CPUs and 2 x NVIDIA Kepler K80 GPUs (12GB and 2496 CUDA cores per GPU) and 128 GB memory gpu03-gpu04: each with 48 (HT) cores Intel Broadwell CPUs and 4 x NVIDIA Kepler K80 GPUs (12GB and 2496 CUDA cores per GPU) and 512 GB memory gpu05: 64 (HT) cores Intel Broadwell CPUs and 2 x NVIDIA Pascal P100 GPUs (16GB and 3584 CUDA cores per GPU) and 256 GB memory gpu06-gpu08: with 64-128 (HT) cores AMD CPUs and 8 x NVIDIA A100 GPUs (80GB and 6912 CUDA cores per GPU) and 1,024 GB memory    ","excerpt":"Introduction This manual provides an introduction to the usage of the …","ref":"/manuals/hpc_cluster/intro/","title":"Introduction"},{"body":"","excerpt":"","ref":"/manuals/linux_basics/","title":"Linux Basics"},{"body":"What is OnDemand? Open OnDemand allows users to access our cluster resources purely through a web browser. No additional client software is required. OnDemand gives users the ability to launch “Interactive Apps” such as Jupyter, RStudio, Matlab, Mathematica, and VSCode and connect to them through your browser.\nUser’s also have the ability to upload/download files to/from the cluster, connect to the cluster via SSH, and create batch job templates.\nThe sections below go over using OnDemand, as well as a couple pieces of popular software.\nAccessing OnDemand Our OnDemand instance is located here: https://ondemand.hpcc.ucr.edu/. Log in with your cluster login details and verify your login with Duo’s two-factor authentication.\nJupyter on OnDemand After logging in, select “Jupyter Notebook” from the “Interactive Apps” tab from the menu bar.\nFrom there, select the resources you need, time you want, partition to run the job on, and click “Launch”.\nYour job will then be queued and eventually start running.\nClick “Connect to Jupyter” to open a new window containing Jupyter and start working!\nRStudio on OnDemand The process of launching RStudio is almost identical to that of starting Jupyter, but selecting “RStudio Server” instead of “Jupyter Notebook” from the menu.\nPlease see the Jupyter section for selecting resources and opening the RStudio window.\nDesktop Session on OnDemand A Desktop session is a Virtual Desktop that is running on the cluster. It will allow you to run programs that require GUIs without going through the steps of forwarding X11 sessions.\nSimilar to Jupyter and RStudio, a Desktop Session can be started by selecting “HPCC Desktop” from the menu dropdown.\nPlease see the Jupyter section for selecting resources and opening the Desktop Window.\nUsing GPUs on OnDemand In many of the interactive session launch pages, the “Additional Slurm Arguments” option is available.\nTo select a GPU, you can use the same --gres argument as you would with the srun command or in sbatch scripts.\nFor example, to get 1x A100 GPU for a job, be sure to select the gpu partition and enter --gres=gpu:a100:1 in the Additional SLurm Arguments box.\nTroubleshooting Jobs RStudio Crashes If your RStudio session crashes with an error similar to the following, first try increasing the memory allocated to your job. If your R program attempts to allocate too much memory it will be killed by Slurm, causing an error similar to the one pictured.\nTo confirm whether or not this is the problem you are encountering:\n Copy the Job ID from OnDemand Delete the job (This will remove the dialog, so make sure you copy the JobID first) Using a terminal, run sacct -j ####  If one of the job steps existed with the reason “OUT_OF_MEM”, then you need to allocate more memory to RStudio.\n","excerpt":"What is OnDemand? Open OnDemand allows users to access our cluster …","ref":"/manuals/hpc_cluster/selected_software/ondemand/","title":"Open OnDemand Usage"},{"body":"Computer cluster  Over 16,000 CPU cores (60% AMD, 40% Intel) 512-1024GB RAM per node GPU: 24x K80, 2x P100, 24x A100 IB network @ 56Gbs - 200Gbps (FDR, HDR and NDR) Queueing system Slurm  Parallel data storage system  5PB GPFS storage (scales to \u003e50PB) Home directories on dedicated system  Backup system  5PB GPFS storage Geographically separated server room  Server room  Genomics Building, Rm 1120A Size 600 sqft Raised floor cooling with redundant AC units Backup power: UPS plus generator  ","excerpt":"Computer cluster  Over 16,000 CPU cores (60% AMD, 40% Intel) …","ref":"/about/hardware/overview/","title":"Hardware Overview"},{"body":"","excerpt":"","ref":"/about/overview/","title":"Overview"},{"body":"PyTorch in a Jupyter Notebook There are many ways to run PyTorch within Jupyter, though some methods are needlessly complicated or are more prone to errors. If you intend to use PyTorch within Jupyter, the following steps should get you up and running.\nSetting Up The Environment Creating a new Conda environment is necessary as we do not provide PyTorch through our global Python installation.\nconda create -n pytorch_env conda activate pytorch_env  After activating the conda environment, install python and ipykernel.\n Warning: The PyTorch instructions provide a method of installing through Conda. Do not use this method as the CUDA packages installed through Conda can conflict with the system installation.\n conda install python=3 ipykernel   At this point run which pip, it should return a path ending in something similar to ...../.conda/envs/pytorch_env/bin/pip. If the output path begins with /opt/linux/... then the environment has not been set up correctly.\n With ipykernel installed, add the environment as a Jupyter Kernel.\npython -m ipykernel install --user --name pytorch_env --display-name \"PyTorch Env\"   For more info on Jupyter Kernels, see the Package Management page.\n PyTorch can now be installed in the Conda environment using Pip\npip3 install torch torchvision torchaudio  From this point, the PyTorch kernel should be ready to use within Jupyter. Because of it’s requirement for a GPU, it will need to be run using OnDemand or through an interactive session.\nRunning on Jupyter The below steps will focus on running it through OnDemand.\nFrom within OnDemand, start a new Jupyter Notebook using the highlighted options below.\nOnce the session has started, be sure to use the “PyTorch Env” kernel that we created earlier when creating a new notebook or within existing notebooks.\nThe following code can be used to verify that the GPU is properly working.\nimport torch dev = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\") print(dev)  If the output prints “cuda”, then you’re good to go. If it prints “cpu”, then please double check that all of the above steps are correct.\n","excerpt":"PyTorch in a Jupyter Notebook There are many ways to run PyTorch …","ref":"/manuals/hpc_cluster/selected_software/pytorch_in_jupyter/","title":"PyTorch In Jupyter"},{"body":"SSH Key Basics This basic introduction into SSH keys might be sufficient for most users on all major OSs.\n","excerpt":"SSH Key Basics This basic introduction into SSH keys might be …","ref":"/manuals/hpc_cluster/sshkeys/sshkeys_simple/","title":"SSH Keys Summary"},{"body":"Using VSCode on the Cluster VSCode is a code editor that can run locally on your computer, or while connected to the cluster.\nWhen using VSCode on the cluster, please do not use Remote SSH as it will launch the code server on a head node, causing unneeded load.\nInstead, we can use a feature of VSCode: Remote Tunnels.\nSetting up VSCode Tunnels Using a tunnel allows us to work on a compute node, rather than on a head node. This allows us to use more resources than we would normally be allowed to on a head node.\nInstalling the Remote Tunnels extension On your local machine, install the “Remote - Tunnels” extension.\nStarting VSCode Tunnel on the Cluster Create an interactive session using srun\nsrun -p epyc -t 5:00:00 --pty -c 4 --mem=4g bash -l # Customize as needed  Load the VSCode module and start the tunnel\nmodule load vscode code tunnel  The program will provide you with a code and ask you to verify on GitHub.com. Follow the steps for authorization. Once you get to the “Congratulations, you’re all set!” page, the terminal will update with a new line asking you to open another link. At this point you have 2 ways to access: via a web browser, or using the extension that we previously installed. Make sure that you keep the server running in the background, as it is what allows the connection to occur.\nUsing A Web Browser After authorizing VSCode, you can use the link given to access your session. The URL should be similar to https://vscode.dev/tunnel/.... The environment is very similar to the desktop program, though some features might be missing.\nUsing the VSCode Extension After install the “Remote - Tunnels” extension on your local machine, connect to the Tunnel session that was previously created using the green “\u003e\u003c” icon in the bottom left of VSCode. Select the “Connect to Tunnel…” option, then select the tunnel we created earlier.\nAfter VSCode connects, you should be able to open Files and Folders on the cluster as if it were your local machine.\nUsing the Built-In Terminal One feature that VSCode integrates is an in-editor terminal. To activate it, you can use the keyboard shortcut Ctrl+`, or via View \u003e Terminal from the status bar.\nBy default, you might be dropped into a basic shell without some of the features that you are used to (eg. with the prompt bash-4.4$ instead of username@node). To fix this, you can type bash -l that should bring you to the terminal environment that you are used to, and from here you can navigate and use the cluster as if it was any other terminal program.\nCleaning Up Once you have finished, make sure to close VSCode (locally or using your web browser). Then stop the Tunnel from running on the cluster using Ctrl+C. Once the program had been stopped, you can exit out of the interactive srun session and close your terminal.\n","excerpt":"Using VSCode on the Cluster VSCode is a code editor that can run …","ref":"/manuals/hpc_cluster/selected_software/vscode/","title":"VSCode Usage on HPCC"},{"body":"Upcoming Events    Date \u0026 Time Location Instructors Title and Description Material    Past Events    Date \u0026 Time Location Instructors Title and Description Material     Fri Apr 19, 2024  2:00pm - 5:00pm Hybrid (Campbell Hall Rm 104 and Zoom) Austin Leong, Emerson Jacobson, Thomas Girke Part 1: Linux Basics and HPCC Cluster Introduction- HPCC infrastructure overview- Accessing HPCC cluster- Linux basics for cluster usage- File exchange and data management- Intro to file permissions- Data storage under home and bigdata- Software environment modules and running software via Slurm- HPCC Support- Exercises Agenda and Instruction Material   Weds Apr 24, 2024  2:00pm - 5:00pm Hybrid (Campbell Hall Rm 104 and Zoom) Austin Leong, Emerson Jacobson, Thomas Girke Part 2: HPC Cluster Usage and Bash Scripting- Permissions and data sharing- SSH session management and remote file editing- Intro to Bash scripting- sbatch scripts- Debugging Slurm submissions- Parallelization- Intermediate Slurm topics: partitions, node architecture, job submissions, monitoring, and quotas- Slurm array jobs- Exercises Agenda and Instruction Material   Fri Apr 26, 2024  2:00pm - 5:00pm Hybrid (Campbell Hall Rm 104 and Zoom) Austin Leong, Emerson Jacobson, Thomas Girke Part 3: Python Scripting- TBD Agenda and Instruction Material   Fri May 3, 2024  2:00pm - 5:00pm Hybrid (Campbell Hall Rm 104 and Zoom) Austin Leong, Emerson Jacobson, Thomas Girke Part 4: HPCC Software and Services- Open OnDemand web portal- Jupyter, RStudio, VSCode, Desktop via OnDemand- Code management with Git and GitHub- Running scripts and compiling from source- Package management with Conda- Singularity-CE/Apptainer Agenda and Instruction Material   Fri Sept 29, 2023  2:00pm - 5:00pm Hybrid (Campbell Hall Rm 104 and Zoom) Austin Leong, Emerson Jacobson, Thomas Girke Part 3: HPC Software Installs and Basic Scripting1. Code management with GitHub 2. Running scripts and compiling software from source  3. Installs and package management with Conda  4. Singularity  5. Basic Shell (Python) scripting  6. Exercises Agenda and Instruction Material   Fri Sept 22, 2023  2:00pm - 5:00pm Hybrid (Campbell Hall Rm 104 and Zoom) Austin Leong, Emerson Jacobson, Thomas Girke Part 2: HPC Cluster Usage1. Permissions and data sharing 2. Software access via module system  3. SSH session management and remote file editing  4. What to consider when parallelizing computing tasks on a cluster? 5. Intermediate Slurm usage: job submissions, monitoring and quotas6. Exercises Agenda and Instruction Material   Fri Sept 15, 2023  2:00pm - 5:00pm Hybrid (Campbell Hall Rm 104 and Zoom) Austin Leong, Emerson Jacobson, Thomas Girke Part 1: Linux Basics and HPCC Cluster Introduction1. HPCC infrastructure overview  2. Accessing HPCC cluster 3. Linux basics for cluster usage 4. File exchange and data management5. Data storage under home and bigdata 6. Navigating and running software via Slurm  7. Exercises Agenda and Instruction Material   Mon March 27 (24), 2023  2:00pm - 5:00pm Hybrid (Genomics 1102A and Zoom) Austin Leong, Thomas Girke Part 3: HPC Software Installs and Basic Scripting1. Code management with GitHub 2. Running scripts and compiling software from source  3. Installs and package management with Conda  4. Singularity  5. Basic Shell (Python) scripting  6. Exercises Agenda and MaterialSlides   Fri March 17, 2023  2:00pm - 5:00pm Hybrid (Genomics 1102A and Zoom) Austin Leong, Tomin Kappiarumalayil, Emerson Jacobson, Thomas Girke Part 2: HPC Cluster Usage1. Permissions and data sharing 2. Software access via module system  3. SSH session management and remote file editing  4. What to consider when parallelizing computing tasks on a cluster? 5. Intermediate Slurm usage: job submissions, monitoring and quotas6. Exercises Agenda and MaterialSlides   Fri March 3, 2023  2:00pm - 5:00pm Hybrid (Genomics 1102A and Zoom) Austin Leong, Tomin Kappiarumalayil, Emerson Jacobson, Thomas Girke Part 1: Linux Basics and HPCC Cluster Introduction1. HPCC infrastructure overview  2. Accessing HPCC cluster 3. Linux basics for cluster usage 4. File exchange and data management5. Data storage under home and bigdata 6. Navigating and running software via Slurm  7. Exercises Agenda and MaterialSlides   September 2, 2021  10:00am - 11:00am Virtually via Zoom Jordan Hayes Coffee Hour - Conda InstallsTopics:1. Why conda?2. Configure3. Install Examples Registration SlidesZoom   June 14, 2021  10:00am - 11:00am Virtually via Zoom Viet Pham Coffee Hour - Remote EditingTopics:1. Linux filesystems2. Basic Linux navigation3. Remote editing Registration SlidesZoom   December 18, 2020  1:00pm - 4:30pm Virtually via Zoom Jordan Hayes, Melody Asghari, Isaac Salinas, Abraham Park \u0026 Thomas Girke Introduction to HPCC’s Cluster UsageTopics:1. HPCC Infrastructure (Slides) 2. Linux Basics for HPC (Slides A, Slides B) 3. Cluster Usage (Slides)  4. Special Topic: R on HPC (Slides) 5. Special Topic: R Use Case from Statistics (Slides)  Registration Agenda Video Recordings   April 15, 2020  2:45pm - 5:00pm Virtually via Zoom Abraham Park, Travis Nasser, Jordan Hayes Intro to HPCC This workshop will cover topics including: * The cluster scheduler, Slurm* Cluster policies and best practices* Basic job submissions to the cluster* and others ..  Registration HPCC Intro Basics:SlidesVideo   April 14, 2020  12:15pm - 2:15pm Virtually via Zoom Travis Nasser, Abraham Park, Jordan Hayes Intro to Linux This workshop will cover topics including: * Basic Linux commands* Linux File systems* Creating Scripts Registration Linux Intro Basics:SlidesVideo   January 29, 2020  1:00pm - 3:15pm Genomics Bldg Auditorium 1102A Jordan Hayes Intro to HPCC This workshop will cover topics including: * The cluster scheduler, Slurm* Cluster policies and best practices* Basic job submissions to the cluster* and others ..  HPCC Intro Basics:Slides   January 27, 2020  1:30pm - 3:15pm Genomics Bldg Auditorium 1102A Abraham Park Intro to Linux This workshop will cover topics including: * Basic Linux commands* Linux File systems* Creating Scripts Linux Intro Basics:Slides\n   November 7, 2019  1:00pm - 3:00pm Genomics Bldg Auditorium 1102A Jordan Hayes \u0026 Abraham Park Intro to Linux and HPCC This workshop will cover topics including: * Connecting to the Cluster* Basic Linux commands* File systems* The cluster scheduler, Slurm* Cluster policies and best practices* Basic job submissions to the cluster* and others ..  Linux Intro Basics:Slides\nHPCC Intro Basics:Slides   June 17, 2019  2:00pm - 5:00pm Genomics Bldg Auditorium 1102A Charles Forsyth Linux Basics and HPCC Cluster Introduction This workshop will cover topics including: * Connecting to the Cluster* Basic Linux commands* File systems* The cluster scheduler, Slurm* Cluster policies and best practices* Basic job submissions to the cluster* and others ..  Registration HPCC Linux/Cluster Basics:Slides\nFiles   April 5, 2019  2:00pm - 5:00pm Genomics Bldg Auditorium 1102A Charles Forsyth Linux Basics and HPCC Cluster Introduction This workshop will cover topics including: * Connecting to the Cluster* Basic Linux commands* File systems* The cluster scheduler, Slurm* Cluster policies and best practices* Basic job submissions to the cluster* and others ..  Registration HPCC Linux/Cluster Basics:Slides\nFiles   February 15, 2019  2:00pm - 5:00pm Genomics Bldg Auditorium 1102A Charles Forsyth HPCC Workshop - Linux Basics and HPCC Cluster Introduction This workshop will cover topics including: * Connecting to the Cluster* Basic Linux commands* File systems* The cluster scheduler, Slurm* Cluster policies and best practices* Basic job submissions to the cluster* and others ..  Registration HPCC Linux/Cluster Basics:Slides\nFiles   December 14, 2018  2:00pm - 5:00pm Genomics Bldg Auditorium 1102A Charles Forsyth HPCC Workshop - Linux Basics and HPCC Cluster Introduction This workshop will cover topics including: * Connecting to the Cluster* Basic Linux commands* File systems* The cluster scheduler, Slurm* Cluster policies and best practices* Basic job submissions to the cluster* and others ..  Registration HPCC Linux/Cluster Basics:Slides\nFiles   November, 2, 2018  9:30pm - 4:30pm Morning: HUB-355; Afternoon: HUB-268/269 Amit Majumadar, Nicole Wolter, Mahidhar Tatineni, Robert Sinkovits, Paul Rodriguez, Mai Nguyen San Diego Supercomputer Center Workshop: Workshop presented by staff from the San Diego Super Computer Center. The morning will provide general information and resources, and the afternoon will split into two parallel tracks (Data Science and Compute Intensive Tasks). Registration  Agenda  Slides \u0026 Tutorials   October 26, 2018  9:00pm - 12:00pm Genomics Bldg Auditorium 1102A Charles Forsyth, Jordan Hayes \u0026 Thomas Girke Research Computing in UCR’s HPC Center: This event will provide an overview of the resources provided by UCR’s HPC Center along with a usage tutorial. This includes a seminar-style introduction to the center’s hardware and software resources, including usage statistics, training events, online tutorials and access options via subscription and/or ownership models. The introduction will be followed by a tutorial about the general usage of the infrastructure covering the following topics: (i) user accounts, (ii) big data storage and backups, (iii) software management, installs and containerization; (iv) queuing system; (v) web-based access options; and (vi) external alternatives including AWS and XSEDE. During the tutorial section participants are welcome to follow along on their laptops. Guest accounts will be provided to new users. Registration  Agenda  Intro Slides  Tutorial Slides   October 18, 2018  1:00pm - 3:30pm Genomics Bldg Auditorium 1102A Charles Forsyth HPCC Workshop - Introduction to Python This workshop will cover topics including: * What is Python* Logic Control* Functions and Classes* Simple Data/File Operations* Using Python in HPC Environments* Installing and Importing Python Modules* and others ..  Registration Python Intro:Slides\nFiles   October 4, 2018  1:00pm - 3:30pm Genomics Bldg Auditorium 1102A Charles Forsyth HPCC Workshop - Linux Basics and HPCC Cluster Introduction This workshop will cover topics including: * Connecting to the Cluster* Basic Linux commands* File systems* The cluster scheduler, Slurm* Cluster policies and best practices* Basic job submissions to the cluster* and others ..  Registration HPCC Linux/Cluster Basics:Slides\nFiles   September 6, 2018  2:00pm - 5:00pm Genomics Bldg Auditorium 1102A Charles Forsyth \u0026 Jordan Hayes HPCC Workshop - Basic Linux/Cluster Intro and Software InstallationPart One: Basic Linux and Cluster Introduction Part Two: Installing software on the HPCC Cluster (Python, R, Perl, Basic Compiled Applications). Registration HPCC Linux/Cluster Basics:Slides\nFilesSoftware Install:SlidesFiles   August 3, 2018  2:00pm - 5:00pm Genomics Bldg Auditorium 1102A Charles Forsyth \u0026 Jordan Hayes HPCC Cluster (Biocluster) Intro WorkshopThree part basic introdution to Linux and the HPCC Cluster along with a user interaction session. Registration Linux Basics:SlidesVideoHPCC Basics:Slides\nFilesVideo   June 15, 2018  1:00pm - 4:00pm Genomics Bldg Auditorium 1102A Charles Forsyth, Jordan Hayes \u0026 Thomas Girke Introduction to HPCC ClusterI. Linux Basics; II. HPCC Cluster; III. How to work in R on HPC systems? Linux Basics:Slides\nHPCC Basics:Slides\nFiles\nR on HPCC:Slides   May 4, 2018  2:00pm - 5:00pm Genomics Bldg Auditorium 1102A Charles Forsyth \u0026 Jordan Hayes HPCC Cluster (Biocluster) Intro WorkshopThree part basic introdution to Linux and the HPCC Cluster along with a user interaction session. Registration Linux Basics:SlidesVideoHPCC Basics:SlidesVideo   April 27, 2018  2:00pm - 3:30pm Genomics Bldg Auditorium 1102A Thomas Girke Annual PI and User Meeting of HPC Center Overview of services, hardware/software resources, usage stats, training and recharging. Followed by discussion of future directions. Slides  Agenda   April 5, 2018  1:00pm - 2:30pm Genomics Bldg Auditorium 1102A Charles Forsyth HPCC Cloud Service PresentationLearn about the HPCC Cloud Service (beta). We will demonstrate how to use HPCC and Amazon Web Service (AWS) to quickly create an on-demand HPC Cluster private to you. We also explain important topics such as billing and cost control, cluster operation, types of clusters (GPU, Highmem, etc.), data management and more. Registration Slides   April 6, 2018  2:00pm - 5:00pm Genomics Bldg Auditorium 1102A Charles Forsyth \u0026 Jordan Hayes HPCC Cluster (Biocluster) Intro WorkshopThree part basic introdution to Linux and the HPCC Cluster along with a user interaction session. Registration Basic LinuxHPCC Cluster Intro   March 9, 2018  2:00pm - 5:00pm Genomics Bldg Auditorium 1102A Charles Forsyth \u0026 Jordan Hayes HPCC Cluster (Biocluster) Intro WorkshopThree part basic introdution to Linux and the HPCC Cluster along with a user interaction session. Registration Basic LinuxHPCC Cluster Intro   February 9, 2018  2:00pm - 5:00pm Genomics Bldg Auditorium 1102A Charles Forsyth \u0026 Jordan Hayes HPCC Cluster (Biocluster) Intro WorkshopThree part basic introdution to Linux and the HPCC Cluster along with a user interaction session. Registration Basic LinuxHPCC Cluster Intro   December 14, 2017  10:00am - 12:00pm Genomics Bldg Auditorium 1102A Randy Ridgley - AWS Solutions Architect Introduction to AWS for Research Introduction to AWS: Overview of Amazon Web Services (AWS), key concepts and terminology for compute, storage, database and networking. AWS for Researchers: With AWS scientists can deploy and test software, analyze their data, and share their results with collaborators around the world. This section will include a demonstration how to build HPC clusters in AWS using cfnCluster and AWSBatch. Registration, Slides   November 3, 2017  2:00pm - 5:00pm Genomics Bldg Auditorium 1102A Charles Forsyth \u0026 Jordan Hayes HPCC Cluster (Biocluster) Intro WorkshopThree part basic introdution to Linux and the HPCC Cluster along with a user interaction session. Registration Basic LinuxHPCC Cluster Intro   October 11, 2017  2:00pm - 4:00pm ULB 104 Charles Forsyth Linux, Scripting and HPCC Cluster Intro WorkshopIntrodution to Linux concepts basic bash scripting and HPCC Cluster intro focused on job submission. Slides   September 1, 2017  2:00pm - 5:00pm Genomics Bldg Auditorium 1102A Charles Forsyth \u0026 Jordan Hayes HPCC Cluster (Biocluster) Intro WorkshopThree part basic introdution to Linux and the HPCC Cluster along with a user interaction session. Registration Basic LinuxHPCC Cluster Intro   July 24, 2017  9:00am - 11:00am Genomics Bldg Auditorium 1102A Charles Forsyth HPCC Cluster (Biocluster) Intro WorkshopBasic introdution to the HPCC Cluster and Services. Registration   Jan 20, 2017  12:00pm - 2:00pm Genomics Bldg 1208 Jordan Hayes Slurm and Basic Linux Q\u0026AJob submissions, account managment and basic Linux scripting. Examples    ","excerpt":"Upcoming Events    Date \u0026 Time Location Instructors Title and …","ref":"/events/events/","title":"Workshops"},{"body":"Scheduled exceptions and downtimes 10-Mar-2025 to 14-Mar-2025\nMove into New Server Room: The HPCC’s long-awaited move to a much larger and newly renovated server room in the SOM-ED1 building is finally happening! This major improvement will unfortunately require about 5 days of downtime to move all HPCC systems to the new server room, and redeploy and test them. We apologize for the disruption this will cause to your research. This is a rare and unique event (once every 20+ years) that requires extensive organization and coordination between multiple parties and is a major logistical undertaking.\nBenefits: The relocation of the HPCC will result in considerably more rack space, electrical and cooling capacity, and more dependable system operation, allowing us to accommodate substantially more CPU/GPU computing resources and data storage now and in the future. These enhancements will translate to significant benefits for all users.\nSchedule: Starting the morning of March 10th, users' access will be stopped, and we will begin shutting down all running jobs and services (see below). Any computing jobs that will still be running during the shutdown will need to be terminated by us. Users can restart or re-submit them after the move. Most services will be available again in the evening of March 14th.\nServices Impacted: All services hosted by the HPCC will be down during this period. This includes, but is not limited to cluster access (SSH, Slurm, data access), including HTTP access to user data; as well as services provided by OnDemand; RStudio/Posit, MATLAB, Jupyter, etc. Please note, email sent to support@hpcc.ucr.edu will also not work (only direct email). During the move, HPCC’s Slack workspace (https://ucr-hpcc.slack.com) will be the main means of group communication with users.\nNotes: As we get closer to the shutdown you might see your jobs get queued with the reason “ReqNodeNotAvail, Reserved for maintenance”. This will happen when the runtime of your job will overlap with the shutdown. If your job can tolerate a shorter runtime, please adjust it to not overlap. If the job requires a longer runtime, then it will need to wait until after our servers are back online.\nPast exceptions 23-Feb-2025: AC Issues\n 8:00AM: The condensers on the roof of the Genomics building both lost power, which caused both CRAC units in our server room to stop cooling. Facilities is on site and are working to bring the AC units back online. 9:00AM: Facilities was able to bring one of the condensers back online, and are working on bringing one of the AC units back to alleviate some of the heat. To prevent our power infrastructure from overheating, we’ve had to kill running jobs on the cluster. We apologize for this inconvenience. 12:00PM: Emergency repairs on the AC system are complete, and load-shedded servers have been brought back online. Thank you for your patience.  02-Nov-2024: AC Issues\n 11:00AM: One of our 2 redundent AC units has began blowing hot air into the server room. While facilities diagnoses and resolves the issue we have paused the Slurm queue. We are sorry for the interruption. 12:00PM: Update: to avoid overheating the power in the HPCC had to be shut down. This affects all HPCC services hosted from the Genomics Server room, including SSH login, Slurm jobs, and e-mail ticket system. We’re working on bringing services up safely and quickly. 4:00PM: Currently, the cluster is still down because of a problem with one of the AC units. Physical plant and the systems administrator are working hard to bring everything back online again. As of now, it is hard to tell how long it will take until things are back to normal again. Please be patient and check the alert page here as well as your Slack messages. 5:00AM (Nov 3): Headnodes are available for login. The /rhome and /bigdata storage systems are fully functional, so you may retrieve your files and do some light work. A limited number of compute and GPU nodes may come online depending on sysadmin discretion and cooling constraints. AC repairs will begin on Monday at the earliest, and these repairs must be completed before we can bring the cluster back fully online. Again, we apologize for the disruption. 12:00PM (Nov 3): A small number of nodes from each partition have been made available. We will slowly release more nodes as we monitor the temperature and work on the AC unit continues. 12:00PM (Nov 4): A temporary fix is in place as facilities awaits parts for a complete fix. We are slowly increasing the number of available compute nodes, though with limited job runtimes. Currently any job with a runtime that will end before Nov 10 at Midnight will run. As temperatures in the room hold, we will increase the time limit and number of nodes in operation. 11:00AM (Nov 7): Facilities is in the process of obtaining the equipment necessary for the repairs. We’ve been told by them that the repair will not require shutting down the AC units, so we can resume operations as normal. We have removed the existing reservation, so long-running jobs may begin. We will continue to release the remaining offline nodes throughout the day as well.  24-Jun-2024: AC Work Completed The AC repairs have been completed by facilities. Over the next few hours we will slowly begin to bring nodes back online.\n22-Jun-2024: Update on AC Unit Problems\nUCR Facilities was able to return the affected AC unit to working order. However, the unit is still in suboptimal condition, and will have to be powered off for full maintenance on Monday (Jun 24). Until then, HPCC will have to limit the capacity of the Slurm cluster. This compromise will allow maintenance to be safely performed without resorting to a full emergency cancellation of all jobs.\n21-Jun-2024: Network Outage and HVAC Problem\nStarting 5:15 PM the network connection to the HPCC server room is down. This might be related to a larger network problem on campus, see here. Update 7:30 PM: the network connection is working again, but as it turns out there also is a problem with one of the HVAC units in the server room. It is still unclear how the two problems are connected. Currently, facilities is working on the HVAC problem. To avoid overheating, the Slurm queue has been paused by the sys admins. Current jobs will continue to run, while pending jobs will need to wait until the situation improves.\nCANCELLED: June 14th Shutdown Ahead of Genomics Electrical Maintenance\nStarting on Friday, June 14th at 8am and extending to Saturday, June 15th at 11pm UCR HPCC will be powering down the cluster ahead of a scheduled electrical shutdown of the UCR Genomics Building. To make the most of this downtime, HPCC will offline the cluster on the 14th in order to install routine software updates and perform other minor maintenance tasks. During the shutdown, most of our online services hosted from the UCR Genomics Building will be unavailable including, but is not limited to: SSH, Slurm, Rstudio, JupyterHub, OnDemand, and web file access. Please save your work on any of these services before the maintenance window. E-mail support may be temporarily interrupted, but should otherwise remain online. We recommend checking our Slack channel for any minor status updates during the shutdown.\nIf you submit a Slurm or OnDemand job that extends into the maintenance window, you will receive an error containing “ReqNodeNotAvail”. Your job will queue, but will not start, until maintenance is over. If you want your job to start sooner, cancel the job request, and resubmit your job with a shorter –time duration, such that your job will finish before the maintenance window begins.\nWe apologize for the disruption to your research and teaching workflows. Thanks for your understanding.\n7-May-2024: AC Unit Repairs, Slurm Paused\nAfter investigating a High Temperature warning on one of our AC units, campus Facilities determined a component in one of the AC units appears to be faulty and needs to be replaced. They are planning on receiving the replacement part the morning of the 8th, but in order to do the replacement both AC units will need to be taken offline. We’ve began putting nodes into a “draining” state in an attempt to bring the load on the cluster as low as possible to best manage heat in the server room. Existing jobs will continue to run, but newly scheduled jobs will be put in the queue until we can confirm that the repair has been completed and the AC units are online again. We know this is disruptive to your research and teaching, and we apologize for this development.\nUpdate 1: Facilities was unable to complete the job in their time allotted for today, and will continue work tomorrow the morning (May 9th).\nUpdate 2: While performing the repair, facilities identified a leak which needs to be patched before refilling the refrigerant, otherwise any refrigerant would immediately escape. The ETA for this repair, as given to us by facilities, is May 10th.\nUpdate 3: Facilities has contacted us to say that they are finishing up the repairs and that we can begin to start loading the servers again. As such we’ve released ~50% of the nodes to begin running jobs again.\nUpdate 4: Facilities has completed the repairs, and all nodes have been released.\n08-Mar-2024 AC Unit Refrigerant Leak, Queue Paused\nThe UCR HPCC Slurm queue has been paused due to another AC malfunction in the server room. Earlier on Thursday (March 7th) afternoon, UCR Facilities was called in to check an AC alarm and determined that one of our AC units had low refrigerant. While attempting to recharge the AC refrigerant a leak occurred, forcing personnel to evacuate the room for their own safety. They intend to start the repair first thing on Friday (March 8th) morning. But because that AC unit is still not running at full capacity, cooling is limited. As such, the Slurm queue has been paused as a precaution to keep the room within a safe temperature. We understand that this will be disruptive to your work, and we apologize for this development.\nUpdate 1: After speaking with the person performing the repair, they said that the repair could take all day. We will make further announcements as we receive updates.\nUpdate 2: The repair has been completed and the Slurm queue has been released.\n22-Feb-2023: Upgrades of OS and GPFS\nStarting Thursday, February 22nd at 8:00am and lasting until Friday, February 23rd at 8:00pm, the cluster will be unavailable due to an upgrade of the Operating System as well as our central storage system. Any submitted jobs that overlap with the shutdown time will be queued with the reason “ReqNodeNotAvail” and will need to be requeued with a shorter time or wait until the maintenance is over at which point they will automatically start.\nDuring the shutdown services will be unavailable including, but not limited to: SSH, RStudio, JupyterHub, OnDemand, and web-based file access. If you need additional information or help, you can reach us at our Slack or outside of the shutdown through email (support@hpcc.ucr.edu).\nUpdate: The shutdown had finished and the Slurm queue has been reopened.\n19-Jan-2024: AC Unit Repair Followup\nDue to followup maintenance required following the January 17th repairs, the slurm queue will once again need to be haulted in order to manage heat output in the server room. Maintenance should only take a few hours, after which the queue will be released again.\nUpdate: The repair has been completed. Nodes will be resumed and queue opened.\n17-Jan-2024: AC Unit repair\nThe AC units in the HPCC server room are experiencing issues. To keep the operating temperature within safe limits, the job queue has been halted. New and pending jobs will stay queued, while currently running jobs will be allowed to finish. We apologize for the inconvenience.\nUpdate: The maintenance has completed, but a followup repair will be required in order to return to 100% operation of the AC units.\n14 Aug, 2023: Login and GPFS storage issues\n The login service on the primary headnode failed on Monday afternoon. A new headnode with updated software is now online, so users may access their files and do some light work. The GPFS cluster file system also exhibited some performance stalls. The Slurm scheduler has been paused until this issue subsides.  4-Aug-2023: Upgrades of OS, GPFS, Slurm, Head Nodes and Power Distribution\n  HPCC staff will be performing maintenance work on Aug 4th to Aug 5th that will require a shut down of the HPCC cluster. Objectives for this event include the following upgrades: GPFS, Slurm, OS upgrade to Rocky Linux 8.8, electrical reconfiguration of PDUs, and various other maintenance tasks.\n  Update Aug 6th: Due to underestimated workload on planned maintenance tasks, services are still offline. Basic functionality should be restored by the end of the day.\n  Update Aug 7th: Maintenance is still ongoing. Currently, we are redeploying computer nodes and other services. User login is restricted until the central data storage system has been re-mounted.\n  Update Aug 8th: SSH login has been restored. Users can access their data and perform light work on the head nodes until access to a larger number of compute nodes has been restored. In addition, web-based file sharing and JupyterHub are available again. RStudio Server will be restored next.\n    Update Aug 9th: Slurm has been redployed and is operational. A larger number of computer nodes have been redeployed and are available to users again.\n  Final update Aug 11th: All remaining services are available again. The maintenance is complete. This includes remaining CPU and GPU nodes, etc. We apologize for the extended time it took us to bring all services online again. Thank you for your understanding.\n  17-18 Jul, 2023: Slurm halted\n Facilities had to perform maintenance on the AC units due to the formation of condensation water by the AC units, possibly caused by hot summer days. During the maintenance Slurm jobs had to be halted to avoid overheating. Running jobs will be allowed to continue, provided that the server room does not get too hot.  10-Apr-2023: Bigdata back\n 5:30 PM - Bigdata is back. Thank for for your patience.  10-Apr-2023: Bigdata down\n 4:30 PM - Bigdata is currently down. Please be patient…  22-Dec-2022: Network outage\n 9:00 AM - Due to a network outage the cluster was inaccessible for several hours.  1-Nov-2022: Network router repair\n 9:00 PM - ITS had to repair a router in the Genomics Building. Around 4:00 AM in the morning (Nov 2nd) network access to the Genomics Building became available again. During the affected time window the cluster was not accessible (e.g. via ssh). Processes running on the cluster were not affected.  25-Jun-2021: Bigdata storage repaired\n 5:00 PM - Server running our bigdata storage have been recovered, and all functions of bigdata directory is now back to normal.  25-Jun-2021: Bigdata storage failed\n 3:30 PM - Server running our bigdata storage crashed, and bigdata directory went down with it.  12-Jan-2020: AC unit repaired\n 5:00 PM - AC repairs have been completed. The reservation has been removed, and new Slurm jobs are now no longer suspended.  11-Jan-2020: AC unit failed\n 3:00 PM - One of our AC units is under emergency repairs. A Slurm reservation was put in place to suspend new jobs from running.  5-6 May, 2023: Maintenance and Electrical Power Upgrades in Server Room\n  UCR Facilities Services will be upgrading our electrical capacity in the Genomics server room. To take advantage of the unavoidable system downtime, the HPCC will perform general maintenance and various upgrades on the cluster. The shutdown is expected to last 2 days. It has been scheduled during a weekend to minimize disruptions for users.\n  Update: since Sat/Sun night, most systems are back online again. If users notice any missing functionality, please let us know at support@hpcc.ucr.edu, or at our Slack channel (https://ucr-hpcc.slack.com). Thanks you for your patience and understanding.\n  28-Oct-2020: Cluster jobs failed due to storage suspension\n 3:00 PM - During a routine extension of the bigdata filesystem, there were some complications and disk i/o had to be suspended. 5:30 PM - We have repaired the issue, and everything should be functioning as usual. However, this means that all computing jobs running during timeframe were stopped and will need to be restarted.  19-Aug-2020: Cluster inaccessible due to power outage in Genomics Bdg\n 11:30 PM - All systems were restored by Jordan Hayes and are opterational again. 10:30 PM - HPC systems admin Jordan Hayes is trying to restart the network, storage and cluster again. 10:00 PM - Facilities was able to bring up the power and cooling again. 8:30 PM - Facilities is investigating and trying to reactivate power and cooling.  10-Aug-2020: Cluster inaccessible due to power outage in Genomics Bdg\nAt 5:10 PM: Facilities has restored power and cooling systems in the server room. HPC systems admin Jordan Hayes is restarting the cluster and storage systems. At 10:10 PM: All HPCC services were restored (computing cluster, storage systems, web services).\n22-Mar-2020: Cluster inaccessible due to campus-wide network outage\nDue to a campus-wide network outage at UCR, many HPCC services were not accessible between 8:00 AM and 1:00 PM. Currently, most HPCC services are accessible again. Note, running jobs on the cluster should not have been affected by this disruption. Updates about the current situations can be found here.\n13-Mar-2020: Routine maintenance shutdown\nWe have scheduled an HPCC Cluster Maintenance Shutdown for Friday, March 13th. This will require a shutdown of ALL services: Slurm (queuing system), hosted websites, virtual environments/machines, storage systems, backup systems, and network services. We ask that you please make sure that you do not have any jobs running in the queue, and that you completely logout of the cluster (pigeon, pelican, parrot) before the shutdown.\n08-Jan-2020: Storage outage\nWe had some issues with our storage systems this evening that may have caused disruptions in your work. These issues should be resolved. We’re continuing to monitor the situation to ensure everything is operational, and we apologize for any inconveniences this may have caused. Please let us know at support@hpcc.ucr.edu if you require any assistance regarding job status and recovery.\n21-Nov-2019: Routine filesystem maintenance and diagnostics\nWe have scheduled an HPCC Cluster Maintenance Shutdown for this Thursday, November 21st. This will require a shutdown of ALL services: Slurm (queuing system), hosted websites, virtual environments/machines, storage systems, backup systems, and network services. We ask that you please make sure that you do not have any jobs running in the queue, and that you completely logout of the cluster (pigeon, pelican, parrot) before the shutdown.\n23-Aug-2019: Routine maintenance shutdown\nWe have scheduled an HPCC Cluster Maintenance Shutdown for Friday, Aug 23, 2019. This will require a shutdown of ALL services: Slurm (queuing system), hosted websites, virtual environments/machines, storage systems, backup systems, and network services. We ask that you please make sure that you do not have any jobs running in the queue, and that you completely logout of the cluster (pigeon, pelican, globus) before the shutdown. Status: completed. For user changes related to this maintenance please see here.\n01-Mar-2019: Routine Maintenance Shutdown\nWe have scheduled an HPCC Cluster Maintenance Shutdown for Friday, March 1st. This will require a shutdown of ALL services: Slurm (queuing system), hosted websites, virtual environments/machines, storage systems, backup systems, and network services. We ask that you please make sure that you do not have any jobs running in the queue, and that you completely logout of the cluster (pigeon, pelican, globus) before the shutdown. Status: successfully completed.\n1:00 PM, 20-Dec-18: Outage due to AC failure\nAll systems were down for 3 hours due to a failure of the AC units in our server room. Electricians and AC technicians have repaired the units.\n2:30 PM, 11-Jul-18: Storage Issues\nFor the past several weeks we have been observing slower storage access. In some cases the /bigdata storage was inaccessible for several minutes and caused some jobs to terminate prematurely. We have identified the issue and have taken steps to ensure that this problem does not reoccur.\n6:00 PM, 02-Jul-18: Storage Issues\nStorage issues on the afternoon of July 2, 2018 caused disruptions in some cluster services. The issues should be resolved, but we’re continuing to monitor the situation for any other developments.\n12:00 AM, 31-Jan-18: routine maintenance shutdown\nFor routine maintenance and upgrades we have scheduled an HPCC (Biocluster) shutdown for 12:00AM, Jan-31-2018 to 12:00AM, Feb-01-2018. (complete)\n12:00 AM, 05-Dec-17: NFS \u0026 SMB issues\nNFS and SMB services have been suspended temporarily. This will cause many of our web services to not function properly. These include, but not limited to:\n https://rstudio.bioinfo.ucr.edu \u0026 https://rstudio2.bioinfo.ucr.edu https://galaxy.bioinfo.ucr.edu https://dashboard.bioinfo.ucr.edu https://biocluster.ucr.edu/~username (.html directories) mysql://bioclusterdb.int.bioinfo.ucr.edu (databases)  Note, this issue was resolved soon after it occurred.\n11:00 AM, 13-Aug-17: Cooling problem\nSince Sat morning one of the HVAC units is not working properly. To avoid overheating, we have shut down most of the idle nodes (1:30PM, Sun). As soon as the HVAC unit is repaired we will power these nodes back on. Note, this issue was resolved on 17-Aug-17. UCR facility services has repaired the broken HVAC unit and serviced the second one.\n12:00 AM, 16-Jun-17 to 17-Jun-17: maintenance shutdown\nTo sustain future growth, the power load in the HPCC server room needs to be optimized. For this we have scheduled an HPCC (Biocluster) shutdown in four weeks from now which will start at noon on June 16th and last until noon June 17th. This will require a shutdown of ALL services: Slurm (queuing system), hosted websites, virtual environments/machines, storage access, backup systems and network services. We ask that you please make sure that you do not have any jobs running in the queue, and that you completely logout of the cluster (pigeon, owl, penguin, pelican, globus) before the shutdown.\n10:02 AM, 13-Apr-17: UPS failure\nOur UPS unit went down some time last night causing a power failure on all systems. Jordan is bypassing the UPS to bring things back up in the next few hours. Nationwide Power will come in asap to repair the UPS. Note, this failure has not resulted in any overheating problems since the AC units are running on a different power cricuit.\n11:22 AM, 13-Apr-17: Cluster back up running\nSituation is resolved for now and things are working. We are currently discussing the situation with our electricians to avoid future instances.\nUnannounced exceptions None currently observed.\nStandard Operating Procedures SOP for unscheduled outages When unforeseen issues arise they are categorized by severity:\nGreen - Normal operation, no current issues Yellow - Minor issue[s], likely not observed by users (ie. jobs are not affected) Orange - Medium issue[s], likely observed by users but not fatal (ie. jobs may perform slower than usual) Red - Critical issue[s], major service or entire cluster is not functioning as expected (ie. jobs have terminated prematurely)  Email notifications are only sent to users if there is a Red critical issue.\nSOP for scheduled shutdowns The following outlines the timeline for advance email notifications on scheduled shutdowns of the HPCC cluster and other exceptions:\n Four weeks advance notice Followed by weekly reminders Final reminder the day before the outage  Twitter feed For additional news and information, please consult the HPCC Twitter site. Also see the Tweets window at the bottom of this and other pages of the HPCC website.\nTeam collaborations with Slack Sign up and use Slack Team Collaboration app here: ucr-hpcc.slack\n","excerpt":"Scheduled exceptions and downtimes 10-Mar-2025 to 14-Mar-2025\nMove …","ref":"/news/alerts/","title":"User alerts for HPCC's computing resources"},{"body":"Basics Syntax and Notes   Remember the UNIX/Linux command line is case sensitive!\n  The hash (pound) sign # indicates end of a command and the start of a comment.\n  The notation \u003c...\u003e refers to variables and file names that need to be specified by the user. The symbols \u003c and \u003e need to be excluded.\n  No need to memorize all of these commands, by using these commands you will naturally memorize the most frequently used.\n  When specifying file names:\n The . (dot) refers to the present working directory The ~ (tilde) refers to user’s home directory    Commands Navigation and Exploration pwd # \"Print working directory\"; show your current path ls # \"List\" contents of current directory ls -l # Similar to ls, but provides additional info on files and directories ls -a # List all files, including hidden files (.name) as well ls -R # Lists subdirectories recursively ls -t # Lists files in chronological order cd \u003cdir_name\u003e # \"Change directory\" to specified path cd # Brings you to your home directory cd ~ # Also bring you to your home directory cd .. # Moves one directory up cd ../../ # Moves two directories up (and so on) cd - # Go back to you were previously (before the last directory change)  Informative file \u003cfile-name\u003e # Show type of file (text, binary, compressed, etc...) id # Shows your user name and associated groups hostname # Shows the name of the machine your shell is currently on  Files and Directories mkdir \u003cdir_name\u003e # Creates specified directory rmdir \u003cdir_name\u003e # Removes empty directory rm \u003cfile_name\u003e # Removes file_name rm -r \u003cdir_name\u003e # Removes directory including its contents, but asks for confirmation rm -rf \u003cdir_name\u003e # Same as above, but turns confirmation off. Use with caution cp \u003cname\u003e \u003cpath\u003e # Copy file/directory as specified in path (-r to include content in directories) mv \u003cname1\u003e \u003cname2\u003e # Renames directories or files mv \u003cname\u003e \u003cpath\u003e # Moves file/directory as specified in path  Copy and paste The methods to copy and paste on the command line differ depending on your operating systems (ie. Mac OSX, MS Windows, Linux) and your SSH application (ie. Terminal, MobaXTerm).\n Linux (xterm)  # Copy CTRL+SHIFT+C # Paste CTRL+SHIFT+V   MS Windows (MobaXTerm)  # Copy by highlighting with mouse # Paste SHIFT+INSERT   Mac OSX (Terminal)  # Copy COMMAND+c # Paste COMMAND+v  Shortcuts Command History  ↑ # Up arrow key scrolls backwards through command history ↓ # Down arrow key scrolls forwards through command history history # Shows all commands you have used recently  Auto-completion The tab (⇥) key auto completes commands or file names if there is only one option. Hitting the tab (⇥) key twice will list multiple options. Keep in mind that there are no spaces between the tab (⇥) keys and the partial names of commands or files.\nShow all directories under my home that I can cd into:\ncd ~/⇥⇥\nShow all files that I can ls with names that start with “myfile”:\nls myfile⇥⇥\nShow all commands that I can run with names that start with “sp”:\nsp⇥⇥\nCursor Ctrl+a # Cursor to beginning of command line Ctrl+e # Cursor to end of command line Ctrl+w # Cut last word Ctrl+k # Cut to the end of the line Ctrl+y # Paste (\"yank\") content that was cut earlier (by Ctrl-w or Ctrl-k)  Other Useful Unix Commands df -h /scratch # Show local disk space for /scratch, do not use for /rhome or /bigdata free -h # Show memory of current machine bc # Command-line calculator (to exit type 'quit') wget \u003cURL\u003e # Download a file or directory from the web ln -s \u003cFILENAME1\u003e \u003cFILENAME2\u003e # Creates symbolic link (shortcut, or alias) for file or directory du -sh . # Shows size of current directory du -sh \u003cFILENAME\u003e # Shows size of individual file du -s * | sort -nr # Shows size of each file within current directory, sorted by size  Help Not all command have help documentation available, however one of these methods will likely work:\nhelp \u003cCOMMAND\u003e # Show help for a Bash command man \u003cCOMMAND\u003e # Show the manual page for a program (press the 'q' key to exit) \u003cCOMMAND\u003e --help # Show help documentation for command \u003cCOMMAND\u003e -h # Show help documentation for command  Online help: Google is your friend.\nUniversally available Linux commands, with detailed examples and explanations: https://www.linuxconfig.org/linux-commands\n","excerpt":"Basics Syntax and Notes   Remember the UNIX/Linux command line is case …","ref":"/manuals/linux_basics/cmdline_basics/","title":"Command Line Basics"},{"body":"The following packages are available in the base conda environment.\n   Package Version     _libgcc_mutex 0.1   _openmp_mutex 4.5   biopython 1.81   boltons 23.0.0   brotli-python 1.0.9   brotlipy 0.7.0   bzip2 1.0.8   c-ares 1.19.1   ca-certificates 2023.08.22   certifi 2023.7.22   cffi 1.15.1   charset-normalizer 2.1.1   cmake 3.26.4   conda 23.9.0   conda-libmamba-solver 23.9.3   conda-package-handling 2.2.0   conda-package-streaming 0.9.0   contourpy 1.0.6   cryptography 41.0.3   cycler 0.11.0   cython 0.29.32   datetime 4.7   decorator 5.1.1   distro 1.8.0   docopt 0.6.2   et-xmlfile 1.1.0   filelock 3.12.2   flatbuffers 22.11.23   fmt 9.1.0   fonttools 4.38.0   goatools 1.3.1   humanfriendly 10.0   icu 73.1   idna 3.4   importlib-metadata 5.1.0   jinja2 3.1.2   joblib 1.2.0   jsonpatch 1.32   jsonpointer 2.1   keyutils 1.6.1   kiwisolver 1.4.4   krb5 1.20.1   ld_impl_linux-64 2.38   libarchive 3.6.2   libcurl 8.4.0   libedit 3.1.20221030   libev 4.33   libffi 3.4.4   libgcc-ng 12.1.0   libiconv 1.16   libmamba 1.5.1   libmambapy 1.5.1   libnghttp2 1.57.0   libnsl 2.0.0   libsolv 0.7.24   libsqlite 3.40.0   libssh2 1.10.0   libstdcxx-ng 11.2.0   libuuid 1.41.5   libxml2 2.10.4   libzlib 1.2.13   lit 16.0.6   llvm-openmp 14.0.6   lz4-c 1.9.4   lzo 2.10   mappy 2.24   markupsafe 2.1.3   matplotlib 3.6.2   mpmath 1.2.1   natsort 8.3.1   ncurses 6.4   networkx 2.8.8   numpy 1.23.5   openpyxl 3.1.2   openssl 3.0.11   packaging 21.3   pandas 1.5.2   patsy 0.5.3   pcre2 10.42   pillow 9.3.0   pip 22.3.1   pip-autoremove 0.10.0   pluggy 1.0.0   progressbar33 2.4   psutil 5.9.5   pybind11-abi 4   pycosat 0.6.6   pycparser 2.21   pydot 1.4.2   pyopenssl 23.2.0   pyparsing 3.0.9   pysam 0.20.0   pysocks 1.7.1   python 3.9.18   python-dateutil 2.8.2   python_abi 3.9   pytz 2022.6   pyyaml 6.0   readline 8.2   reproc 14.2.4   reproc-cpp 14.2.4   requests 2.31.0   ruamel.yaml 0.17.21   ruamel.yaml.clib 0.2.6   seaborn 0.12.1   setuptools 68.0.0   six 1.16.0   sqlite 3.41.2   statsmodels 0.14.0   sympy 1.11.1   tabulate 0.9.0   thop 0.1.1-2209072238   threadpoolctl 3.1.0   tk 8.6.12   toml 0.10.2   toolz 0.12.0   tqdm 4.65.0   triton 2.0.0   typing-extensions 4.4.0   tzdata 2023c   urllib3 1.26.18   wheel 0.38.4   xlsxwriter 3.1.2   xz 5.4.2   yaml-cpp 0.7.0   zipp 3.11.0   zlib 1.2.13   zope-interface 5.5.2   zstandard 0.19.0   zstd 1.5.5    ","excerpt":"The following packages are available in the base conda environment. …","ref":"/about/software/conda_packages/","title":"Conda Packages"},{"body":"Storage  Four enterprise class HPC storage systems Approximately 3 PB production and 3 PB backup storage (total 6 PB or 6,144 TB) GPFS (NFS and SAMBA via GPFS) Automatic snapshots and archival backups  Network  Ethernet  5 x 1 Gb/s switch 5 x 1 Gb/s switch 10 Gig uplink 1 x 10 Gb/s switch for campus high performance research network Redundant, load balanced, robust mesh topology   Interconnect  56/200/400 Gb/s InfiniBand (FDR/HDR/NDR)    Head Nodes All users should access the cluster via SSH through cluster.hpcc.ucr.edu. This address will automatically balance traffic to one of the available head nodes.\n Jay  Resources: 64 cores, 512 GB memory Primary function: submitting jobs to the queuing system Secondary function: development; code editing and running small (under 50 % CPU and under 30 % RAM) sample jobs   Lark (Currently acting as a “hot-spare”)  Resources: 64 cores, 512 GB memory Primary function: submitting jobs to the queuing system Secondary function: development; code editing and running small (under 50 % CPU and under 30 % RAM) sample jobs    Worker Nodes  Batch  c01-c48: each with 64 AMD cores and 512 GB memory   Intel  i01-i54: each with 32 Intel Broadwell cores and 256-512 GB memory   Epyc  r21-r38: each with 256 AMD EPYC cores and 1 TB memory   Highmem  h01-h06: each with 32 Intel cores and 1024 GB memory   GPU  gpu01-gpu02: each with dual 8c/16t Intel E5-2630 v3 Haswell CPUs and 2 x NVIDIA Tesla K80 GPUs (12GB and 2496 CUDA cores per GPU) and 128 GB memory gpu03-gpu04: each with dual 12c/24t Intel E5-2650 v4 Intel Broadwell CPUs and 4 x NVIDIA Tesla K80 GPUs (12GB and 2496 CUDA cores per GPU) and 512 GB memory gpu05: with dual 16c/32t Intel E5-2683 v4 Broadwell CPUs and 2 x NVIDIA Tesla P100 GPUs (16GB and 3584 CUDA cores per GPU) and 256 GB memory gpu06-gpu08: with dual 32c/32t AMD EPYC 7543 CPUs and 8 x NVIDIA A100 GPUs (80GB and 6912 CUDA cores per GPU) and 1,024 GB memory    ","excerpt":"Storage  Four enterprise class HPC storage systems Approximately 3 PB …","ref":"/about/hardware/details/","title":"Hardware Details"},{"body":"HPC To be added soon.\nBig data processing To be added soon.\nCloud computing To be added soon.\n","excerpt":"HPC To be added soon.\nBig data processing To be added soon.\nCloud …","ref":"/events/large/","title":"Annual workshop on HPC and big data processing"},{"body":"Login from Mac, Linux, MobaXTerm The initial login brings users into the cluster head node (i.e. jay, lark). From there, users can submit jobs via srun/sbatch to the compute nodes to perform intensive tests. Since all machines are mounting a centralized file system, users will always see the same home directory on all systems. Therefore, there is no need to copy files from one machine to another.\nOpen the terminal and type\nssh -X username@cluster.hpcc.ucr.edu  Login from Windows Please refer to the login instructions of our Linux Basics manual.\nChange Password  Login via SSH using the Terminal on Mac/Linux or MobaXTerm on Windows   Once you have logged in type the following command:  passwd   Enter the old password (the random characters that you were given as your initial password) Enter your new password  The password minimum requirements are:\n Total length at least 8 characters long Must have at least 3 of the following:  Lowercase character Uppercase character Number Punctuation character    Modules All software used on the HPC cluster is managed through a simple module system. You must explicitly load and unload each package as needed. More advanced users may want to load modules within their bashrc, bash_profile, or profile files.\nAvailable Modules To list all available software modules, execute the following:\nmodule avail  This should output something like:\n------------------------ /opt/linux/rocky/8.x/x86_64/modules ------------------------- AAFTF/0.5.0 workspace/scratch \u003caL\u003e abyss/2.3.4 wtdbg2/2.5 almabte/1.3.2 xpdf/4.03 alphafold/2.3.0 xsv/0.13.0 amber/22_mpi_cuda yq/4.35.1 amptk/1.6 zoem/21-341 ...  Using Modules To load a module, run:\nmodule load \u003csoftware name\u003e[/\u003cversion\u003e]  For example, to load R version 4.1.2, run:\nmodule load R/4.1.2  To load the default version of the tophat module, run:\nmodule load tophat  Show Loaded Modules To show what modules you have loaded at any time, you can run:\nmodule list  Depending on what modules you have loaded, it will produce something like this:\nCurrently Loaded Modulefiles: 1) vim/7.4.1952 3) slurm/16.05.4 5) R/3.3.0 7) less-highlight/1.0 9) python/3.6.0 2) tmux/2.2 4) openmpi/2.0.1-slurm-16.05.4 6) perl/5.20.2 8) iigb_utilities/1  Unloading Software Sometimes you want to no longer have a piece of software in path. To do this you unload the module by running:\nmodule unload \u003csoftware name\u003e  Databases Loading Databases NCBI, PFAM, and Uniprot, do not need to be downloaded by users. They are installed as modules on the cluster.\nmodule load db-ncbi module load db-pfam module load db-uniprot  Specific database release numbers can be identified by the version label on the module:\nmodule avail db-ncbi ----------------- /usr/local/Modules/3.2.9/modulefiles ----------------- db-ncbi/20140623(default)  Using Databases In order to use the loaded database users can simply provide the corresponding environment variable (NCBI_DB, UNIPROT_DB, PFAM_DB, etc…) for the proper path in their executables.\nThis is the old deprecated BLAST and it may not work in the near future, however if you require it:\nblastall -p blastp -i proteins.fasta -d $NCBI_DB/nr -o blastp.out  You can can also use this method if you require the old version of BLAST (old BLAST with legacy support):\nBLASTBIN=`which legacy_blast.pl | xargs dirname` legacy_blast.pl blastall -p blastp -i proteins.fasta -d $NCBI_DB/nr -o blast.out --path $BLASTBIN  This is the preferred/recommended method (BLAST+):\nblastp -query proteins.fasta -db $NCBI_DB/nr -out proteins_blastp.txt  Usually, we store the most recent release and 2-3 previous releases of each database. This way time consuming projects can use the same database version throughout their lifetime without always updating to the latest releases.\nAdditional Features There are additional features and operations that can be done with the module command. Please run the following to get more information:\nmodule help  Quotas CPU and Memory Please refer to our Queue Policies page for details regarding CPU and Memory limits.\nData Storage A standard user account has a storage quota of 20GB. Much more storage space, in the range of many TBs, can be made available in a user account’s bigdata directory. The amount of storage space available in bigdata depends on a user group’s annual subscription. The pricing for extending the storage space in the bigdata directory is available here.\nWhat’s Next? You should now know the following:\n Basic orginization of the cluster How to login to the cluster How to use the Module system to gain access to the cluster software CPU, storage, and memory limitations (quotas and hardware limits)  Now you can start using the cluster.\nThe HPCC cluster uses the Slurm queuing system and thus the recommended way to run your jobs (scripts, pipelines, experiments, etc…) is to submit them to this queuing system by using sbatch. Please DO NOT RUN ANY computationally intensive tasks on any head node (i.e. jay, lark). If this policy is violated, your process will either run very slow or be killed automatically. The head nodes (login nodes) are a shared resource and should be accessible by all users. Negatively impacting performance would affect all users on the system and will not be tolerated.\n","excerpt":"Login from Mac, Linux, MobaXTerm The initial login brings users into …","ref":"/manuals/hpc_cluster/start/","title":"Getting Started"},{"body":"Overview Globus is a file sharing and transfer service used by many research facilities worldwide. It aides collaboration by simplifying the process of securely sharing research data.\nFile transfers are accomplished by running the Globus client (Globus Connect Personal, GCP) on the HPCC cluster, and using a web browser on another computer to initiate the transactions.\nNote: Transfers between two sites (endpoints) require at least one endpoint to have a paid Globus subscripion. To inquire about subscriptions, visit the Globus Subscription Inquiry page.\nSetup This page explains how to setup the GCP client on the HPCC cluster. For setting up GCP on your personal computer, please follow the official documentation:\nhttps://docs.globus.org/globus-connect-personal/\nLoad the GCP module HPCC provides a module with the GCP software. Load it with the following command:\nmodule load globusconnect  Create a new personal HPCC endpoint In order to create an endpoint on the HPCC cluster, you’ll need to log into Globus. Because of UCR’s Duo two-factor authentication system, the login process will need to be done in a web browser. You should use the web browser on your local device rather than launching a browser on the cluster. (External users: depending on your home institution’s login system, you may be able to log into Globus directly on the cluster. The following steps should still work with a few modifications).\nStart the login process with the following command:\nglobusconnect -setup # add `--no-gui` to use command-line only  You will be given a long URL to open in your browser, followed by a prompt for an auth code:\nGlobus Connect Personal needs you to log in to continue the setup process. We will display a login URL. Copy it into any browser and log in to get a single-use code. Return to this command with the code to continue setup. Login here: ----- https://auth.globus.org/v2/oauth2/authorize?client_id=... ----- Enter the auth code:  Copy-paste the URL into your browser. Select the appropriate organization name:\nComplete the UCR login and Duo process:\nReview the requested permissions, and the Globus Terms of Service. You’ll need to accept them to use the service. Edit the “label for future reference” if you’d like.\nYou’ll be given the auth code. Copy-paste the code back into the prompt on the cluster terminal:\nEnter the auth code: SFyA**************************  Finally, give the endpoint a name, such as “ucr-hpcc”:\n== starting endpoint setup Input a value for the Endpoint Name: ucr-hpcc registered new endpoint, id: XXXXXXXX-XXXX-XXXX-XXXX-XXXXXXXXXXXX setup completed successfully  At this point, Globus will now recognize your HPCC cluster storage as an endpoint. You may now launch the GCP client without having to tediously log in.\nRunning the GCP client The GCP client must be running on the endpoint in order to send or receive files there. It is recommended to run the client in the background (the ampersand at the end creates a background process):\nIn order for bigdata folders to appear in the Globus interface, you must use the “-restrict-paths” argument when running Globus to allow it access to other folders. If you omit this, then only your home directory will be available.\nglobusconnect -start -restrict-paths rw/rhome,rw/bigdata \u0026  Locating your new endpoint in the Globus Web App Login to Globus in your web browser. Open the “File Manager”.\nClick on “Search” in the “Collection” text box.\nClick on “Your Collections”. Your personal endpoint should be listed here.\nStopping the GCP client When you are done with your Globus session, you should stop the client:\nglobusconnect -stop  ","excerpt":"Overview Globus is a file sharing and transfer service used by many …","ref":"/manuals/hpc_cluster/data/globus/","title":"Globus Connect Personal"},{"body":"","excerpt":"","ref":"/manuals/hpc_cluster/","title":"HPC Cluster"},{"body":"\nOverview The High-Performance Computing Center (HPCC) provides state-of-the-art research computing infrastructure and training accessible to all UCR researchers and affiliates at low cost. Currently, it supports over 150 research groups with more than 800 active users. Its resources are also heavily used for instructing undergraduate and graduate classes in a wide range of computational, statistical, life science and engineering disciplines.\nNews This year the HPCC was awarded an MRI equipment grant (#2215705) by NSF for the acquisition of a Big Data HPC Cluster in the total amount of $942,829. For details see here.  Quick start The following lists the most frequently visted pages of the HPCC site. They can also be accessed via the navigation system outlined below.\nNavigating and searching this site  Top menu located in bar on top of each page provides links to the main content categories Section menu to the left links to subpages of each main category Table of content to the right links to sections within each page  Gain access  User account creation for accessing HPCC’s infrastructure Latest recharging rates Log in instructions  HPC usage  Usage instructions are provided in the manual section To efficiently navigate the Manuals pages, use the Manual dropdown in the menu bar on the top of this site. Event schedule for workshops and user meetings  Infrastructure description  Infrastructure description of HPCC’s clusters and parallel storage systems Facility description document for grant applications and related purposes  Help and contacts  For Support please consider asking on our Forums. For requesting user accounts, password help, or software requests, please email support@hpcc.ucr.edu. You can also join our Slack at ucr-hpcc. Contact information  ","excerpt":"\nOverview The High-Performance Computing Center (HPCC) provides …","ref":"/about/overview/introduction/","title":"Welcome to the HPC Center (HPCC)"},{"body":"The following packages are available on the cluster as modules.\nTo load the default version of a module, use: module load [package]\nTo load a specific version of a module, use: module load [package]/[version]\nTo view this list at the command line, use: module avail\n   Package Name Versions     AAFTF 0.3.0   AAFTF 0.3.1   AAFTF 0.3.3   AAFTF 0.3.4   AAFTF 0.4.1   AAFTF 0.5.0   abyss 2.3.4   almabte 1.3.2   alphafold 2.1.2   alphafold 2.3.0   amber 22   amber 22_mpi_cuda   amptk 1.5   amptk 1.6   anaconda 2021.11-py39_0   angsd 0.937   ant 1.10.12   antismash 5.1.2   antismash 6.0.0   antismash 6.1.1   antismash 7.0.0   anvio 7.1   anvio 8.0   arcs 1.2.3   aria2 1.36.0   aspera 3.6.0   aspera 4.2.4.265   aspera 4.2.6.393   ASTRAL-Pro 1.1.3   ASTRAL 4.7.8   ASTRAL 4.7.12   ASTRAL 4.10.7   ASTRAL 4.10.12   ASTRAL 5.5.6   ASTRAL 5.6.1   ASTRAL 5.6.3   ASTRAL 5.7.1   ASTRAL 5.14.3   ASTRAL 5.15.4   ASTRAL 5.15.5   augustus 3.3.3   augustus 3.4.0   augustus 3.5.0   autodockfr 1.0   automake 1.14   autometa 1.0.3   autometa 2.0.2   aws 2.7.31   bakta 1.5.1   bamtools 2.5.2   barrnap 0.9   bat 0.18.3   BBMap 38.95   bcftools 1.15   bcftools 1.16   bcftools 1.17   bcftools 1.18   bcl2fastq 2.20.0.422   bclconvert 4.1.7   bclconvert 4.2.4   beagle-lib 3.1.2   beagle 4.1   beagle 5.2   beast 1.10.4   beast 2.6.6   beast 2.7.1   bedops 2.4.40   bedtools 2.29.1   bedtools 2.30.0   bigscape 1.1.5   bin3C 2020-12-15   bioperl 1.7   bioperl 1.7.8   biopython 1.70   biopython 1.79   biopython 1.81   bismark 0.22.3   bismark 0.24.1   blobtools 1.1.1   blobtools2 3.1.0   bonito 0.6.2(default)   bonito dev   boost 1.62.0_openmpi   boost 1.78.0   boost 1.80.0   bowtie 1.3.1   bowtie2 2.4.5   bracken 2.6.2   braker 3.0.3   busco 5.3.0   busco 5.3.2   busco 5.4.3   busco 5.4.4   bwa-mem2 2.2.1   bwa 0.7.17   cafe 5.0.0   canu 2.2   cap3 2005.06.07.amd64   cap3 2005.06.07.intel   cap3 2015-02-10   cblaster 1.3.12   cblaster 1.3.18   cd-hit 4.8.1   cellprofiler 4.2.5   cellranger 7.1.0   centos 7.9   cgal 0.9.6   charliecloud 0.32   charliecloud 0.33   checkm 1.2.1   checkv 1.0.1   clinker 1.3.12   clinker 1.3.18   clipkit 1.3.0   clust 1.17.0   cmake 3.27.1   cnvnator 0.4.1   CodingQuarry 2.0   concoct 1.1.0   cp2k 7.1_oneapi-2022.1.2.146   cplex-studio 22.1.1   csvkit 1.0.7   cuda 10.1   cuda 10.2   cuda 11.4(default)   cuda 11.7   cuda 11.8   cuda 12.1   cudnn 8.2.4.15   cudnn 8.9.5.29   cufflinks 2.2.1   cutadapt 3.7   cutadapt 4.1   cytoscape 3.10.0   cyvcf2 0.30.15   db-cazy 3.0   db-cazy 4.0   db-cazy 5.0   db-cazy 9.0   db-cazy 10.0   db-cazy 11.0   db-checkv 0.6   db-diamond 20161103   db-diamond 20190125   db-diamond 20210403(default)   db-diamond 20230130   db-kaiju 20170113-e   db-merops 110   db-merops 120(default)   db-merops 2017-02-02   db-ncbi 20140623   db-ncbi 20150112   db-ncbi 20160708   db-ncbi 20170707   db-ncbi 20171212   db-ncbi 20190121   db-ncbi 20190709   db-ncbi 20190805   db-ncbi 20220131(default)   db-ncbi v5.20190517   db-panther 10.0(default)   db-panther 13.1   db-pfam 27.0   db-pfam 28.0   db-pfam 30.0   db-pfam 31.0   db-pfam 32.0   db-pfam 33.0   db-pfam 33.1   db-pfam 34.0   db-pfam 35.0(default)   db-pfam 2013-03-14   db-pfam 2015-07-27   db-pfam 2016-09-15   db-pfam 2017-06-11   db-pfam 2018-10-18   db-pfam 2020-03-19   db-pfam 2020-05-05   db-pfam 2021-03-19   db-pfam 2021-11-25   db-priam mar15(default)   db-rfam 11.0   db-rfam 14.5   db-rfam 14.7   db-swissprot 2021_01   db-trinotate r20150720(default)   db-uniprot 2014_12   db-uniprot 2015_01   db-uniprot 2015_02   db-uniprot 2015_03   db-uniprot 2015_04   db-uniprot 2015_05   db-uniprot 2016_11   db-uniprot 2018_03(default)   ddradseqtools 0.45   dedalus 79bbb80   deepte 122520-cpu   deepte 122520-gpu   deeptools 3.5.1   deML 1.1.3   dftbplus 23.1   dgenies 1.4.0   diamond 2.0.13   diamond 2.0.14   diamond 2.1.0   diamond 2.1.7   dislin 11.5   dorado 0.3.0   dorado 0.4.0   dssp 3.0.0   du-dust 0.7.5   du-dust 0.8.6   edta 2.1.0   edta 2.1.0-fix   eggnog-mapper 2.1.7   eggnog-mapper 2.1.9   elpa 2020.11.001_oneapi-2022.1.2.146   elsi 2.9.1   emboss 6.6.0   entap 0.10.8-beta   espresso 7.0_oneapi-2022.1.2.146   ete3 3.1.2   exonerate 2.2.0   exonerate 2.4.0   extra 1   fasta 36.3.8h   fastani 1.33   fastme 2.1.6.3   fastp 0.23.2   fastq-screen v0.15.2   fastq_demux 0.0.1   fastqc 0.11.9   faststructure 1.0.1   fasttree 2.1.11   fastx_toolkit 0.0.13   ffmpeg 5.0   fftw 2.1.5   fftw 3.3.10   figaro 0.1   filtlong 0.2.1   firefox 115.0.2   Flye 2.9   freebayes 1.3.6   freefem 4.10   funannotate 1.8(default)   funannotate dev_live   funannotate development   funannotate2 2.0alpha   fzf 0.28.0   g_mmpbsa 1.6   gadget2 2.0.7   gadgetviewer 1.1.2   gap 4.11.1   gatk 3   gatk 3.3-0   gatk 3.3.0   gatk 3.4-0   gatk 3.4-46   gatk 3.4.0   gatk 3.4.46   gatk 3.5   gatk 3.6   gatk 3.7   gatk 3.8   gatk 4   gatk 4.0.1.2   gatk 4.0.4.0   gatk 4.0.8.1   gatk 4.0.12.0   gatk 4.1.1.0   gatk 4.1.4.1   gatk 4.1.8.1   gatk 4.2.0.0   gatk 4.2.5.0   gatk 4.2.6.1   gatk 4.3.0.0   gatk 4.4.0.0   gaussian 16   gaussian 16_AVX2   gaussian 16_SSE4   gcc 6.5.0   gcc 9.2.1   gcloudsdk 387.0.0   gcloudsdk 421.0.0   gcloudsdk 422.0.0   gemma 0.98.5   genemarkESET 4.69_lic   genemarkESET 4.71_lic   genomemapper 0.4.4   genometools 1.6.2   getorganelle 1.7.6   git-lfs 3.2.0   globusconnect 3.2.0   gmap 2021-12-17   gmap 2023-04-28   gmt 4.5.8   gocryptfs 2.2.1   golang 1.17.6   gridss 2.13.2   gromacs 2022.3   gsel_vec 0.0.1   gtdbtk 2.0.0   gtdbtk 2.1.0   gtdbtk 2.2.4   gtdbtk 2.3.0   gtotree 1.7.10   gtotree 1.8.1   guppy 5.0.11   guppy 5.0.11-gpu   guppy 6.0.1   guppy 6.0.1-gpu   guppy 6.1.7   guppy 6.1.7-gpu   guppy 6.2.1   guppy 6.2.1-gpu   guppy 6.3.4   guppy 6.3.4-gpu   guppy 6.3.7   guppy 6.3.7-gpu   guppy 6.3.8   guppy 6.3.8-gpu   guppy 6.4.2   guppy 6.4.2-gpu   guppy 6.4.6   guppy 6.4.6-gpu   guppy 6.4.8   guppy 6.4.8-gpu   guppy 6.5.7   guppy 6.5.7-gpu   gurobi 10.0.0   gutsmash 5.0.0   hal 2.2   hdf5 1.8.22_oneapi-2022.3.0.8767   hdf5 1.8.22_openmpi   hdf5 1.10.9_openmpi   hdf5 1.12.1   hdf5 1.12.1_oneapi-2022.1.2.146   hdf5 1.12.2   hdf5 1.12.2_oneapi-2022.3.0.8767   hic-pro 3.1.0   hifiasm 0.19.7   hisat2 2.1.0   hisat2 2.2.0   hisat2 2.2.1   hmmer 1.8.5   hmmer 2   hmmer 2.3.2   hmmer 3   hmmer 3.3.2   hmmer 3.3.2-mpi   homer 4.11   hpcc_user_utils current   hpcc_workshop 1.0   hpcc_workshop 2.0   htslib 1.14   htslib 1.15   htslib 1.16   htslib 1.17   htslib 1.18   hub 2.14.2   i-adhore 3.0   igv 2.12.3   infernal 1.1.4   interproscan 5.54-87.0   interproscan 5.55-88.0   interproscan 5.60-92.0   interproscan 5.62-94.0   iprscan 5.54-87.0   iprscan 5.55-88.0   iprscan 5.60-92.0   iprscan 5.62-94.0   IQ-TREE 1.6.12   IQ-TREE 2.1.1   IQ-TREE 2.1.1_mpi   IQ-TREE 2.1.3   IQ-TREE 2.2.0   IQ-TREE 2.2.1   IQ-TREE 2.2.2.6   iqtree 1.6.12   iqtree 2.1.1   iqtree 2.1.1_mpi   iqtree 2.1.3   iqtree 2.2.0   iqtree 2.2.1   iqtree 2.2.2.6   isa-l 8b7c1b8   itpp 4.3.1   ITSx 1.1.3   itsxpress 1.8   iupred 3   jags 4.3.0   java 17.0.2   jbrowse 1.16.8   jbrowse2 1.6.7   jellyfish 2.3.0   jemalloc 5.2.1   jorg 1.0.0   juicebox 2.13.07   julia 1.7.2   julia 1.8.4   jupyterhub 4.0.2   jupyterlab 3.3.1   jupyterlab 4.0.1   kallisto 0.46.1   kallisto 0.48.0   KEGG-Decoder 1.3   kent-tools 427   kofamscan 1.3.0   kraken2 2.1.2   kraken2 2.1.3   KronaTools 2.8.1   last 1352   last 1453   lastz 1.04.22   less 608   libdeflate 1.10   libdeflate 1.14   libdeflate 1.17   liftoff 1.6.3   LINKS 1.8.4   longranger 2.2.2   longstitch 1.0.3   lsaBGC 1.0   ltr_finder 1.0.7   ltr_finder_parallel 1.1   ltr_retriever 2.9.1   macs2 2.2.6   macs2 2.2.7   macs3 3.0.0a7   mafft 7.490   mafft 7.505   mash 2.3   masurca 4.0.9   masurca 4.1.0   mathematica 12.3.1   matlab R2021b   mcclintock 2.0.0   mcl 14-137   MCScanX r51_g97e74f4   medaka 1.6   medaka 1.6-gpu   megahit 1.2.9   megalodon 2.5.0   megalodon 2.5.0-gpu   meme 5.4.1   metabat2 2.15   metahipmer v2.1.0.2   metahipmer v2.1.0.2-gpu   metaphor 1.7.7   methylmap 0.3.4   miniasm 0.3   miniconda3 py39_4.10.3   miniconda3 py39_4.12.0   minimap2 2.24   miniprot 0.2   mira-assembler 5rc2   mirage 2.0.0   MMseqs2 13-45111   mmseqs2 13-45111   mmv 2.3   modeltest-ng 0.1.7   mosdepth 0.3.3   mosh 1.4.0   mothur 1.47.0   mpich 4.0.1   mpich 4.0.1_gcc-9.2.1   mpich 4.0.1_gcc-9.2.1_slurm23(default)   mpich 4.0.1_slurm23   mrbayes 3.2.7   mrbayes 3.2.7-sse   mummer 3.23   mummer 4.0.0   muscle 3.8.31   muscle 5.1   mutt 2.1.5   mutt 2.2.9   mzmine 3.7.0   nanomethphase 1.0   nasm 2.15.05   ncbi-asn_tools 2022-04-25   ncbi-blast 2.2.22+   ncbi-blast 2.2.25+   ncbi-blast 2.2.26   ncbi-blast 2.2.26+   ncbi-blast 2.2.29+   ncbi-blast 2.2.30+   ncbi-blast 2.2.31+   ncbi-blast 2.3.0+   ncbi-blast 2.4.0+   ncbi-blast 2.5.0+   ncbi-blast 2.6.0+   ncbi-blast 2.7.1+   ncbi-blast 2.8.0+   ncbi-blast 2.8.1+   ncbi-blast 2.9.0+   ncbi-blast 2.11.0+   ncbi-blast 2.12.0+   ncbi-blast 2.13.0+   ncbi-blast 2.14.0+   ncbi-blast 2.14.1+   ncbi-fcs 0.4.0   ncbi-ngs-tools 2.11.2   ncbi-rmblast 2.11.0   ncbi-rmblast 2.13.0   ncbi-rmblast 2.14.0   ncbi-table2asn 2022-06-10   ncbi-table2asn 2023-07-13   ncbi_datasets 13.4.0   ncbi_datasets 14.10.0   ncbi_datasets 15.11.10   ncbi_edirect 16.8.20220329   nccl 2.18.1-1   NECAT 0.0.1   neovim 0.6.0   netcdf-c 4.4.4.1_oneapi-2022.3.0.8767   netcdf-c 4.8.1_oneapi-2022.1.2.146   netcdf-c 4.9.0_oneapi-2022.3.0.8767   netcdf-c 4.9.0_openmpi-4.1.2   netcdf-fortran 4.4.4_oneapi-2022.3.0.8767   netcdf-fortran 4.5.4_onaapi-2022.1.2.146   netcdf-fortran 4.6.0_oneapi-2022.3.0.8767   netcdf-fortran 4.6.0_openmpi-4.1.2   netlogo 6.3.0   NextDenovo 2.5.0   nextflow 23.04   nextflow 23.08   NextPolish 1.4.0   NextPolish 1.4.1   nf-core 2.9   ninja 1.11.1   nQuire 2018-04-05-a990a88   oneapi 2022.1.2.146   oneapi 2022.3.0.8767(default)   openbabel 3.1.1   openmpi 4.1.2   openmpi 4.1.2-slurm-19.05.0   openmpi 4.1.2_slurm-21.08.5   openmpi 4.1.2_slurm-21.08.5_mpi1-compat   openmpi 4.1.2_slurm-23.02.2_mpi1-compat(default)   orca 5.0.4   orthofinder 2.5.4   osg-wn-client 3.6.220426-1   packmol v20.14.2   palemoon 32.3.1   pandoc 2.17.1.1   parallel-fastq-dump 0.6.7   parallel 20220122   PASA 2.5.2   pastis 62123b5   pathracer 2020-12-20   paup 4a168   PBX 0.2.2   pbzip2 1.1.13   pear 0.9.6   pear 0.9.11   phame 1.0.4   phasegenomics 2022-06   phobius 1.01   phred-phrap-consed 0.990329   phykit 1.11   phyling 2.0   phyloFlash 3.4   phyml 3.3.20220715   phyml 3.3.20220715-mpi   picard 2.26.11   pilon 1.24   pixy 1.2.7   plass 4.1-live   plink 1.90b6.25   plumed 2.9.0   pmix 3.1.3rc2   pnetcdf 1.3.1_oneapi-2022.3.0.8767   pnetcdf 1.3.1_openmpi-4.1.2-mpi1-compat   pnetcdf 1.12.3_oneapi-2022.3.0.8767   pnetcdf 1.12.3_openmpi-4.1.2   pod5 0.0.41   pod5 0.2.0   polypolish 0.5.0   primer3 2.6.1   prodigal 2.6.3   prokka 1.14.5   protobuf v23.4   prymetime 0.2   purge_dups 1.2.6   purge_haplotigs 1.1.2   pychopper 2.7.2   pyrodigal-gv 0.2.0   pyvcf 0.6.8   qiime2 2022.2   qiime2 2022.11   qiime2 2023.5   quast 5.1.0rc1   quickmerge 0.3   r-spieceasi 1.1.1   R 4.1.2   R 4.1.3   R 4.2.0   R 4.2.2   R 4.3.0(default)   racon 1.5.0   racon 1.5.0-gpu   RADinitio 1.1.1   ragtag 2.1.0   raisd v2.9   raven 1.8.1   raven 1.8.1-gpu   raxml-ng 1.1.0   raxml-ng 1.2.0   RAxML 8.2.12   raxml 8.2.12   RAxML_NG 1.1.0   RAxML_NG 1.2.0   rclone 1.58.0   rclone 1.59.0   rclone 1.63.0   REDUCE_suite 2.2   reduce_suite 2.2   relocate2 2.0.1   RepeatMasker 4-1-2   RepeatMasker 4-1-4   RepeatMasker 4-1-5   RepeatModeler 2.0.3   RepeatModeler 2.0.4   RepeatModeler 2.0.5   rmate 1.5.10   rnaz 2.1.1   root 6.26.4   rsem 1.3.3   rstudio-server 2022.02.0-443   rstudio-server 2023.09.1-494   ruby 2.7.5   ruby 3.1.1   run_dbcan 3.0   run_dbcan 4.0   salmon 1.7.0   SALSA 2.3   samblaster 0.1.26   samtools 1.14   samtools 1.16   samtools 1.17   samtools 1.18   seqkit 2.4.0   seqtk 1.3   seqtk 1.4   shore 0.9.3   shoremap v3.8   shovill 1.1.0   signalp 3   signalp 3.0   signalp 4   signalp 4.1c   signalp 5   signalp 5.0b   signalp 6   signalp 6-gpu   signalp 6.0g(default)   signalp 6.0g-gpu   signalp 6.0h   signalp 6.0h-gpu   singularity-ce 3.9.3   singularity 3.9.3   sirius 5.8.1   slow5tools 0.7.0   slurm-drmaa 1.1.3   slurm 19.05.0   slurm 21.08.5   slurm 21.08.8   slurm 23.02.2(default)   slurm 23.02.4   snakemake 7.14   snakemake 7.18   snpEff 4.1G   snpEff 4.1K   snpEff 4.3m   snpEff 4.3P   snpEff 4.3r   snpEff 4.3t   snpEff 5.0e   snpEff 5.1   sourmash 4.6.1   spades 3.12.0   spades 3.15.4   sparsehash 2.0.4   spart 1.5.0   sratoolkit 3.0.0   srnascanner latest   sspace 3.0   stacks 2.60   star 2.7.10a   star 2.7.10b   star 2.7.11a   stata 16   stata 18   stringtie 2.2.1   subopt-kaks 1.0.0   subread 2.0.3   subread 2.0.6   swarm 3.1.3   syri 1.6.3   tbb 2020.3   tbb 2021.5.0   tbb 2021.6.0   tcoffee 13.45.65   texlive 20210325   texlive 20220403   tidk 0.2.1   tmhmm 2.0c   tmux 3.3(default)   tmux 3.3a   tophat 2.1.1   transdecoder 5.5.0   trf 4.09   trim_galore 0.6.7   trimal 1.4.1   trimmomatic 0.39   trinity-rnaseq 2.13.2   trinity-rnaseq 2.13.2_sing   trinity-rnaseq 2.14.0   trinity-rnaseq 2.15.1   trinotate 3.2.2   trnascan-se 2.0.9   unicycler 0.5.0   unimap 0.1   upcxx 2020.3.2   upcxx 2020.3.2-gpu   upcxx 2023.3.0   upcxx 2023.3.0-gpu   usearch 8   usearch 8.0.1623_32bit   usearch 8.1.1861_32bit   usearch 9   usearch 9.0.2132_i86linux32   usearch 9.1.13_i86linux32   usearch 9.2.64_i86linux32   usearch 10   usearch 10.0.240_i86linux32   usearch 11   usearch 11.0.667_i86linux32   usher 0.5.6   valgrind 3.18.1   vapid 1.2   vasp 5.4.1_oneapi-2022.1.2.146   vcftools 0.1.16-18   veba 1.0.0-annotate   veba 1.0.0-assembly   veba 1.0.0-binning-eukaryotic   veba 1.0.0-binning-prokaryotic   veba 1.0.0-binning-viral   veba 1.0.0-classify   veba 1.0.0-cluster   veba 1.0.0-database   veba 1.0.0-mapping   veba 1.0.0-phylogeny   veba 1.0.0-preprocess   velvet 1.2.10   verkko 1.3.1   veryfasttree 4.0.2   viennarna 2.4.17   viennarna 2.5.0   vmd 1.9.3   vscode 1.76.1   vscode 1.79.0   vscode 1.79.2   vscode 1.82.3   vsearch 2.21.1   vt 0.57721   wannier90 3.1.0_oneapi-2022.1.2.146   wgd 2   workspace scratch   wtdbg2 2.5   xpdf 4.03   xsv 0.13.0   yq 4.23.1   yq 4.35.1   zoem 21-341    ","excerpt":"The following packages are available on the cluster as modules.\nTo …","ref":"/about/software/modules/","title":"Software Modules"},{"body":"SSH Keys on macOS What are SSH Keys? SSH (Secure Shell) keys are an access credential that is used in the SSH protocol.\nThe private key (ie. id_rsa) remains on the system being used to access the HPCC cluster and is used to decrypt information that is exchanged in the transfer between the HPCC cluster and your system.\nA public key (ie. id_rsa.pub) is used to encrypt information, and is stored on the cluster. The authorized keys file that is stored on the HPCC cluster (ie. ~/.ssh/authorized_keys) contains one or more public keys (ie. id_rsa.pub).\nWhy do you need SSH Keys? HPCC supports two authentication methods; Password+DUO and SSH Keys. The Password+DUO method requires a UCR NetID, if you do not have one then you will need to use SSH keys in order to access the HPCC cluster.\nCreating SSH Keys from the Command-line By far the easiest way to create SSH keys on macOS systems is from the command-line following the instructions here. Users who prefer to do this in a graphical user interface can follow the instructions below.\nGUI-based SSH Key Creation Filezilla You will need to install Filezilla in order to transfer the public SSH key to the HPCC cluster.\n Download the Filezilla Client for Mac OS X here.  Make sure your Mac OS X system is updated to the latest version.   Follow the install wizard to complete the install of Filezilla.  Sourcetree You will need to install Sourcetree in order to generate your SSH keys (or use the command line method mentioned here.\n Download Sourcetree from here Click on Download for Mac OS X Install Sourcetree  Create SSH Keys (Sourcetree)   Open the Sourcetree application and under the top Sourcetree menu click on the Preferences... sub-menu item.\n  Navigate to Accounts category and click on Add....\n  Click on Auth Type: and change the drop down menu from OAuth to Basic. Make sure Protocol: is set to SSH in the drop down menu.\n  Enter id_rsa in the Username field.\n  Click the Generate Key button.\n  Press Cancel to exit out of the window.\n  SSH Keys Location By default, your key files are created in the path: /Users/macOSUsername/.ssh/.\nTo verify that the keys were created, do the following:\n  Open a new finder window. Click on your home directory on the left side pane.\n  Press the 3-button combo Command+Shift+. together (visualized below) to see hidden folders:\n  You will now be able to see your .ssh folder, open it by double-clicking.\n  You should see your newly generated pair of SSH key files in the folder.\n  Sourcetree adds the -Bitbucket to the end of the SSH key file names. Remove this by clicking on the file you want to rename and press the Enter key which allows us to rename the file before the extension.\n  After you have removed the -Bitbucket suffix from each of the SSH key file names, your new SSH key file names should be id_rsa and id_rsa.pub.\n  Configure SSH Keys Public SSH Key Now that you have created your SSH keys, and renamed them, you will need to place your public key (id_rsa.pub) on the cluster.\nIf you do not have a UCR NetID, or prefer not to use Password+DUO authentication, then email your public key (id_rsa.pub) to support and skip to Private SSH Key. If you already have configured Password+DUO authentication, then proceed with the following:\n  Start the Filezilla application.\n  Open Site Manager window by clicking the upper left most button in the top bar of icons.\n  Click on New Site, which will unlock the fields to the right.\n  From the newly unlocked fields in the General tab, fill in the following:\n Protocol: SFTP - SSH File Transfer Protocol Host: cluster.hpcc.ucr.edu Logon Type: Interactive User: Your HPCC Username    When using Password+DUO authentication, you must also set the maximum number of connections. Navigate to the Transfer Settings tab and set the following:\n Limit Number of simultaneous connections: checked Maximum number of connections: 1  Then click on Connect.\n  If a pop up prompts you to save your password, select the Save passwords option, then click the OK button.\n  Then enter in your password for the cluster, and click OK.\n  If the next pop up prompts you, then check the box that states Always trust this host, add this key to the cache, then click the OK button.\n  You should now see the DUO authentication dialog, ensure your User is correct then enter the number for the preferred option from the list presented, then click OK.\n  Now that you are connected with Filezilla, transfer your public SSH key from your MacOS system by dragging the file /Users/macOSUsername/.ssh/id_rsa.pub and dropping it into the HPCC cluster direcotry /rhome/username/.ssh/.\n  If the /rhome/username/.ssh/ directory does not exits, create it.\nOnce the id_rsa.pub file is transferred to the cluster, be sure to rename it to authorized_keys.  Private SSH Key Once your public key is in place, now you can configure Filezilla to use your private SSH key and connect to the cluster through the cluster.hpcc.ucr.edu server.\n  Start the Filezilla application\n  Open Site Manager window by clicking the button in the top bar of icons.\n  Click on New Site, rename it (optional) and press enter.\n  Fill in the following fields from the General tab:\n Protocol: SFTP - SSH File Transfer Protocol Host: cluster.hpcc.ucr.edu Logon Type: Key file User: Your HPCC username Key file: /Users/macOSUsername/.ssh/id_rsa  Be sure to select the previously created private key (/Users/macOSUsername/.ssh/id_rsa) for the Key file field using the Browse... button.\n  Navigate to the folder you saved your key file in (default location is /Users/macOSUsername/.ssh) and open the private key file id_rsa.\n  You should see the added keyfile in the Key file: box, then click Connect.\nSubsequnt connections can be done from the Quickconnect history by clicking on the down arrow to the right side of the Quickconnect button.\n  Remember to select the cluster.hpcc.ucr.edu address.\n  Transfer files by double clicking or drag-n-drop. For more details regarding file transfers vist Filezilla Usage.\n  ","excerpt":"SSH Keys on macOS What are SSH Keys? SSH (Secure Shell) keys are an …","ref":"/manuals/hpc_cluster/sshkeys/sshkeys_macos/","title":"SSH Keys Apple macOS"},{"body":"User account requests To create new user or lab accounts, please follow these instructions:\n Please email user account requests to support@hpcc.ucr.edu. Include the full name, NetID and email address of both users and PI. Users need to be members of the PI’s group. Preferentially, user account requests should come from the corresponding PI directly. If the request comes from a new user then the PI needs to be CC’ed in the email exchange. If a PI’s lab is not registered yet, please provide in the same email a COA (formerly FAU) required to pay for the annual subscription fee, and optionally for additional data storage (see here). If additional storage is needed, mention how much and the COA to be used for the additional recharge.  After receiving the access information for a new account, users want to follow the login instructions here.\nRecharging rates HPCC’s recharging rate structure is outlined below. A more formal summary is available in the most recent Recharging Rate PDF here.\nPI-based Registration Fee An annual registration fee of $1,000 gives all members of a UCR lab access to our high-performance computing infrastructure. The registration provides access to the following resources:\n Over 16,000 CPU cores (60% AMD and 40% Intel), ~230,000 cuda cores (Nvidia A100, P100 and K80 GPUs), ~5PB parallel GPFS-based disk space, 512GB-1TB of memory/node, etc. More details are available on the hardware pages. Over 1000 software packages and community databases. Details are available on the software page. Free attendance of workshops offered by HPCC staff Free consultation services (up to 1 hour per month) Note: there is no extra charge for CPU usage but each user and lab have CPU quotas of 384 and 768 CPU cores, respectively. Computing jobs exceeding these quotas can be submitted but will stay in a queued state until resources within the quota limits become available.  Big data storage For data storage the HPCC uses a central parallel GPFS storage system that scales to many thousands of TBs. This high-availability storage system is directly attached (mounted) to all its CPU and GPU nodes, meaning users can immediately process their data with high-performance computing hardware without moving them from one location (e.g. a data archival system) to another.\n  Rented big data storage\n  Standard user accounts have a storage quota of 20 GB. To gain access to much larger storage pools, PIs have the option to rent or own storage space.\n  Storage rental option\n $1000 per 10TB of usable and backed up storage space per year. Smaller units than 10TB are also available (e.g. 100GB units). For details see here. In comparison, the maintenance cost for the same amount of owned storage is $260 per year (see below). Since the HPCC backs up all user data to a secondary server room and uses snapshotting as an additional data security measure, 10TB of usable backed up space is the equivalent of almost 30TB of raw disk space. Thus, the cost for rented storage is $33.33 for 1TB/yr raw disk space. The rented storage pool can be shared among all user accounts of a registered lab.      Ownership models   Owned big data storage\n A lab/PI purchases storage hardware (e.g. hard drives) according to the specifications of the facility. Owned hard drives will be added to the facility’s parallel GPFS storage systems including production and backup storage. There is no extra charge for the additional storage infrastructure required for operation, including hard drive enclosures (servers) and high-speed network. The annual support fee for owned disk storage is $260 per 10TB of usable and backed up storage space. Since we back everything up to a secondary server room and use snapshotting as an additional data security measure, 10TB of usable backed up space is the equivalent of almost 30TB of raw disk space. Thus, the maintenance cost for owned storage is $8.67 for 1TB/yr raw disk space. Note, owned storage space is only available to the users of a PI’s group or those a PI wishes to give access to. The owned storage pool can be shared among all user accounts of a registered lab. Owned storage can be attractive for labs with storage needs above 40TBs. For smaller amounts the rental option is often a better and more flexible choice (e.g. available within a few days).    \n  Computer nodes\n A lab/PI purchases compatible computer nodes (e.g. with supported network cards). Examples of popular high-density architecture are quad node systems shown here. A quad node system includes 4 nodes where each node can be configured, for example, with two 64 core AMD or Intel chips (providing 128 cores per node or 512 cores per quad node system), 1,024GB of RAM, 2TB SSD and NDR-IB interconnect (additional example). Similar options are available for GPU nodes. Nodes are administered under a priority queueing system that gives users from an owner lab priority and also increases that lab’s overall CPU quota (see above) by the number of owned CPU cores. Owned computer nodes are an attractive solution for labs requiring 24/7 access to hundreds of CPU cores with no or only minor waiting times in queue.    Software install  Registered users can email software install requests to HPCC’s issue tracking system @ support@hpcc.ucr.edu. Install requests are addressed in the order received. Simple installs are addressed within 1 to a few days. Complex installs may take longer.  Department cluster membership with owned computing nodes This option addresses the need of department-level HPC access where the standard PI-based membership is not practical, e.g. provide cluster access to large number of undergraduate students in classes. Under this model a department purchases computer nodes that will be administered similarly as described above under the Ownership model. Due to the large number of expected users from departments, the CPU quota per user is usually lower compared to the PI-based model.\nUsing HPCC cluster for classes To use the HPCC cluster for teaching UCR classes, please coordinate with the systems administrators (support@hpcc.ucr.edu) at least 4 weeks prior to the start of a class so that there is enough time for planning. Details that need be discussed includes the number of user accounts required, special software requirements, creation of a class-specific Slurm partition, data storage reservations, as well as other needs that may vary for different classes.\nExternal user accounts Accounts for external customers can only be granted if a lab has a strong affiliation with UC Riverside, such as a research collaboration with UCR researchers. Both the corresponding UCR PI and external collaborator need to maintain an HPCC subscription. External accounts are subject to an annual review and approval process. To be approved, the external and internal PIs have to complete this External Usage Justification.\nFacility description  The latest hardware/facility description (e.g. for grant applications) is available here.  ","excerpt":"User account requests To create new user or lab accounts, please …","ref":"/about/overview/access/","title":"Access"},{"body":"Learn how to establish an Amazon Web Services (AWS) account under the UC Agreement. A UC-wide agreement for Amazon Web Services (AWS) has been established. The agreement provides Data Egress Fee waiver for up to 15% of total monthly AWS fees.\nPlease follow the instructions below to ensure that an AWS account for UCR business is covered under this agreement and in compliance with UC policy and the law.\nUnderstand Appropriate Use Review the following to understand the applicable terms and conditions, and allowable data use for AWS:\n University of California AWS Enterprise Customer Agreement  Applies to all AWS accounts UC-wide, except those used for:  Commercial Web Hosting Media Streaming Massive Open Online Courses (MOOCs)   80%+ of data egress must be via an approved National Research and Education Network (NREN). This includes CENIC and Internet2, so normal UC usage meets this requirement.   Determine whether the data you are working with should be hosted in the cloud. HIPAA Business Associate Agreement (BAA)  In order to cover your locations AWS accounts under the terms of the UC AWS Enterprise Agreement (EA) and HIPAA Business Associate Agreement (BAA), follow the instructions found on the UCOP Website (PDF).    Connect AWS Account to UC Agreement and PO Send an email to UCs AWS account representative, Devinder Narula dsnarula@amazon.com to:\n Activate your new AWS account (if pertinent) Include your AWS account under the terms of the UC-wide AWS agreement Connect your AWS account to your Purchase Order number  To accomplish this, the email needs to include the following information:\n AWS 12-Digit Account Number Company Name = University of California, Riverside (UCR) Your Name  AWS Account Activation You will receive an email response from AWS confirming that your AWS account is now set up for invoicing under the UC AWS agreement, and providing final instructions to activate your account. Follow the instructions in the AWS activation email and your account will be active and ready for use.\n","excerpt":"Learn how to establish an Amazon Web Services (AWS) account under the …","ref":"/manuals/ext_cloud/aws/egress/","title":"Account Egress Waiver"},{"body":"","excerpt":"","ref":"/manuals/ext_cloud/","title":"Cloud/External"},{"body":"UCR Library  Software Carpentry Workshop - Working with Data  The Unix shell 7-24-20 Version Control with Git 7-31-20   Software Carpentry Workshop - (Python, git, and bash) 3-25-19, 3-26-19  Graduate Quantitative Methods Center - GradQuant  Drop-in Hours Tuesday - Thursday Experimental Design 4-4-19 Hypothesis Testing and Statistical Power 4-9-19 Data Management in Python 4-11-19 JAVA for Beginners 4-15-19 Introduction to General Linear Models 4-17-19 Computational Bayesian Inference 4-24-19 Stata Workshop with Dr. Chuck Huber 4-25-19 Web Scraping with R 4-30-19 Introduction to SAS 5-2-19 Data Visualization in Python 5-7-19 Machine Learning in R 5-8-19 A Primer on Neural Networks with Python 5-13-19 Writing Articles in LaTeX 5-28-19 Machine Learning in MATLAB 6-03-19  Big data analysis programming in genome biology  R/Bioconductor workshops including annual 5-day events Graduate courses using HPC  Data Analysis in Genome Biology: GEN242 Computational Analysis of High Throughput Biological Data: GEN220    Others  10th consecutive workshop on analyzing sequencing data, ANGUS 2019! July 1 - July 12, 2019  This intensive two week summer course introduces attendees with a strong biology background to the practice of analyzing big shotgun sequencing data sets (Illumina, PacBio, and Nanopore). We introduce students to computational thinking and large-scale data analysis on UNIX platforms, and cover genome and transcriptome analysis. We also cover computational topics including R scripting, software installation with bioconda, cloud computing, and building efficient and automated workflows. We use hands-on tutorials, live coding, group notes, and in-class exercises to support an effective learning experience.    Please send us URLs of events we should list here.\n","excerpt":"UCR Library  Software Carpentry Workshop - Working with Data  The Unix …","ref":"/events/related/","title":"Related HPC and big data analysis events"},{"body":"File Systems The file system in Linux is where you can save data, files, scripts, etc. There are different storage pools based on the path. In Linux you can provide any storage pool from any directory, not like MS Windows systems, where a drive letter is assigned to each storage pool (ie. “C:”,“D:\"). This means that by navigating through nested directories, you may find different capacity limits, depending on where you are.\nLocations Most unix system, including Linux, have a common directory hierarchy. The following is called the root level, since it is at the “top” like roots of a inverted tree:\n/ |-- bigdata |-- bin |-- boot |-- dev |-- etc |-- home |-- lib |-- lib64 |-- media |-- mnt |-- opt |-- proc |-- rhome |-- root |-- run |-- sbin |-- srv |-- sys |-- tmp |-- usr `-- var  The two most important directories are /rhome and /bigdata, since this is where your code and data will be stored. These two directories are IBM Spectrum Scale (GPFS) pools, so storage quotas apply. Your home directory lives directly under /rhome and your groups shared storage lives under /bigdata (if extra storage was purchased). These two “bigdata” directories /bigdata/groupname/username and /bigdata/groupname/shared are symlinked (alias/shortcut) to your home directory for convenience, as seen here:\n/ |-- bigdata |-- groupname (Quota based on purchase) |-- username \u003c-------------| |-- shared \u003c----------| | |-- bin | | |-- boot | | |-- dev | | |-- etc | | |-- home | | |-- lib | | |-- lib64 | | |-- media | | |-- mnt | | |-- opt | | |-- proc | | |-- rhome | | |-- username (20GB Quota) | | |-- shared ----------\u003e| | |-- bigdata --------------\u003e| |-- root |-- run |-- sbin |-- srv |-- sys |-- tmp |-- usr `-- var  For more information regarding these locations, and others, visit HPCC Cluster: Data Storage.\nCase sensitive All paths and commands are case sensitive, an uppercase letter is not the same as a lowercase letter.\nPath Types An absolute path is a full path from top to bottom, from the root to the leaf:\n/rhome/username/example_dir/example_file  A relative path is a partial path with the current working directory is the starting point:\nexample_dir/example_file  Commands Here are many common commands related to files and file systems (run man \u003ccommand\u003e for more information):\npwd # Print working directory ls # List files in directory touch # Make an empty file mkdir # Make a directory cd # Change to directory cp # Copy file[s] from a directory to a directory mv # Move file[s] from a directory to a directory rm # Remove a file rmdir # Remove an empty directory df # Check size of storage pool du # Check size of file or directory check_quota # Check quota for home and bigdata   Note: CTRL+c will cancel a running command\n File Transfers This section has moved to the Data Sharing page.\n","excerpt":"File Systems The file system in Linux is where you can save data, …","ref":"/manuals/linux_basics/filesystems/","title":"File Systems and Transfers"},{"body":"What is a Job? Submitting and managing jobs is at the heart of using the cluster. A ‘job’ refers to the script, pipeline or experiment that you run on the nodes in the cluster.\nPartitions Jobs are submitted to so-called partitions (or queues). Each partition is a group of nodes, often with similar hardware specifications (e.g. CPU or RAM configurations). The quota policies applying to each partitions are outlined on the Queue Policies page.\n epyc  Nodes: r21-r38 CPU: AMD Supported Extensions1: AVX, AVX2, SSE, SSE2, SSE4 RAM: 1 GB default Time (walltime): 168 hours (7 days) default   intel  Default partition Nodes: i01-02,i17-i40 CPU: Intel Supported Extensions1: AVX, AVX2, SSE, SSE2, SSE4 RAM: 1 GB default Time (walltime): 168 hours (7 days) default   batch  Nodes: c01-c48 CPU: AMD Supported Extensions1: AVX, SSE, SSE2, SSE4 RAM: 1 GB default Time (walltime): 168 hours (7 days) default   highmem  Nodes: h01-h06 CPU: Intel Supported Extensions1: AVX, SSE, SSE2, SSE4 RAM: 100 GB to 1000 GB Time (walltime): 48 hours (2 days) default   gpu  Nodes: gpu01-gpu06 CPU: AMD/Intel GPUs: NVIDIA K80, A100, P100 RAM: 1 GB default Time (walltime): 48 hours (2 days) default   short  Nodes: Mixed set of nodes from batch, intel, and group partitions Cores: AMD/Intel RAM: 1 GB default Time (walltime): 2 hours Maximum   Lab Partitions  If your lab has purchased nodes then you will have a priority partition with the same name as your group (ie. girkelab).    In order to submit a job to different partitions add the optional ‘-p’ parameter with the name of the partition you want to use:\nsbatch -p batch SBATCH_SCRIPT.sh sbatch -p highmem SBATCH_SCRIPT.sh sbatch -p epyc SBATCH_SCRIPT.sh sbatch -p gpu SBATCH_SCRIPT.sh sbatch -p intel SBATCH_SCRIPT.sh sbatch -p mygroup SBATCH_SCRIPT.sh  Slurm Slurm is used as a queuing system across all head nodes. SSH directly into the cluster and your connection will be automatically load balanced to a head node:\nssh -XY cluster.hpcc.ucr.edu  Resources and Limits To see your limits you can do the following:\nslurm_limits  Check total number of cores used by your group in the all partitions:\ngroup_cpus  However this does not tell you when your job will start, since it depends on the duration of each job. The best way to do this is with the “–start” flag on the squeue command:\nsqueue --start -u $USER  Submitting Jobs There are 2 basic ways to submit jobs; non-interactive and interactive. Slurm will automatically start within the directory where you submitted the job from, so keep that in mind when you use relative file paths.\nNon-interactive Submission Non-interactive jobs are submitted as SBATCH scripts, an example is as follows:\nsbatch SBATCH_SCRIPT.sh  Here is an example of an SBATCH script:\n#!/bin/bash -l #SBATCH --nodes=1 #SBATCH --ntasks=1 #SBATCH --cpus-per-task=10 #SBATCH --mem=10G #SBATCH --time=1-00:15:00 # 1 day and 15 minutes #SBATCH --mail-user=useremail@address.com #SBATCH --mail-type=ALL #SBATCH --job-name=\"just_a_test\" #SBATCH -p epyc # You can use any of the following; epyc, intel, batch, highmem, gpu # Print current date date # Load samtools module load samtools # Concatenate BAMs samtools cat -h header.sam -o out.bam in1.bam in2.bam # Print name of node hostname  The above job will request 1 node, 10 cores (parallel threads), 10GB of memory, for 1 day and 15 minutes. An email will be sent to the user when the status of the job changes (Start, Failed, Completed). For more information regarding parallel/multi core jobs refer to Parallelization.\nInteractive Submission Interactive jobs are submitted using srun. An example is as follows:\nsrun --pty bash -l  If you do not specify a partition then the “hpcc_default” partition is used by default.\nHere is a more complete example:\nsrun --mem=1gb --cpus-per-task 1 --ntasks 1 --time 10:00:00 --x11 --pty bash -l  The above example enables X11 forwarding and requests 1GB of memory and 1 core for 10 hours within an interactive session.\nFeature Constraints Using the --constraint (or -C flag) allows you to fine-tune what type of machine your job can run on, mainly useful on the “short” partitions. Below is a list of nodes as well as what features exist for each node type.\n   Nodes Features     c[01-47] amd, abu_dhabi   h[01-06]    gpu[01-04] gpu_legacy   gpu05 gpu_prev   gpu[06-08] gpu_latest, gpu_highmem   gpu09 gpu_latest   i[01-62] intel, broadwell   r[01-06] amd, rome   r[07-38] amd, milan   r[41-43] amd, milan   x[01-06] intel, cascade    Constraint Examples Since jobs on the “short” partition can run on any node, jobs can be narrowed down using constraints.\nIf you require an Intel node of any generation:\nsrun -p short -t 2:00:00 -c 8 --mem 8GB --constraint intel --pty bash -l  If you require an AMD node, but want it to be Rome or Milan generation (ie. not Abu Dhabi):\nsrun -p short -t 2:00:00 -c 8 --mem 8GB --constraint \"amd\u0026(rome|milan)\" --pty bash -l  If you want to run on a modern GPU machine:\nsrun -p short_gpu -t 2:00:00 -c 8 --mem 8GB --gpu:1 --constraint \"gpu_latest\" --pty bash -l   When using constraints with GPUs, make sure to request a generic GPU\n Monitoring Jobs To check on your jobs states, run the following:\nsqueue -u $USER --start  To list all the details of a specific job (the JOBID can be found using squeue), run the following:\nscontrol show job JOBID  To view past jobs and their details, run the following:\nsacct -u $USER -l  You can also adjust the start -S time and/or end -E time to view, using the YYYY-MM-DD format. For example, the following command uses start and end times:\nsacct -u $USER -S 2018-01-01 -E 2018-08-30 -l | less -S # Type 'q' to quit  Custom command for summarizing activity of all users on cluster\njobMonitor # or qstatMonitor  Canceling Jobs In cancel/stop your job run the following:\nscancel JOBID  You can also cancel multiple jobs:\nscancel JOBID1 JOBID2 JOBID3  If you want to cancel/stop/kill ALL your jobs it is possible with the following:\n# Be very careful when running this, it will kill all your jobs. squeue --user $USER --noheader --format '%i' | xargs scancel  For more information please refer to Slurm scancel documentation.\nOptimizing Jobs After a job has been completed, you can use seff ## (\"##\" being your Slurm Job ID) to check how many resources your job consumed during it’s run. seff is only useful after a job has completed, and will not give useful information on currently-running jobs.\nFor example:\n$ seff 123123 Job ID: 123123 Cluster: hpcc User/Group: your_username/yourlab State: COMPLETED (exit code 0) Nodes: 1 Cores per node: 20 CPU Utilized: 03:26:14 CPU Efficiency: 95.04% of 03:37:00 core-walltime Job Wall-clock time: 00:10:51 Memory Utilized: 81.20 GB Memory Efficiency: 81.20% of 100.00 GB  In the above example, we can see good utilization of the CPU cores (95%) as well as good utilization of memory usage (81%).\nIf CPU Efficiency is low, make sure that the program(s) you are running makes use of multi-threading correctly. Requesting more cores for a job will not make your program run faster if it does not properly take advantage of them.\nIf Memory Efficiency is low, then you can try reducing the requested memory for a job. Note: Just because you see your job uses 81.20GB of memory does not mean that next time you should request exactly 81.20GB of memory. Variations in input data will cause different memory usage characteristics. You should try to aim to request ~20% higher memory then will actually be used to account for any spikes in memory usage. Slurm might miss some quick spikes of memory usage, but the Operating System will not. In this regard it’s better to overestimate on initial runs, and scale back once you find a good limit.\nSlurm Job Reason/Error Codes If a job is stuck in the queue or fails to start, there are typically Slurm error codes assigned that explain the reason. Typically these are a bit hard to parse, so below is a table of common error codes and how to work around them.\n   Error Code Reason Fix     Resources This isn’t an error, but rather why your job can’t start immediately. Once requested resources are available, then your job will start.   Priority This isn’t an error, but rather why your job can’t start immediately. You have likely submitted many jobs in a short period of time and Slurm’s Fair-Share algorithm is allowing other higher priority jobs to run first.   QOSMaxWallDurationPerJobLimit The time limit requested on the selected partition goes over the limits. For example, requesting 2 days on the “short” partition. Make sure that you are within the partition’s time limit. Please refer to the Queue Policies page for the per-partition time limits.   AssocGrpCpuLimit You are exceeding the Per-User CPU limit on a specific partition. You must wait until jobs finish within a partition to free up resources to allow additional jobs to run.   AssocGrpMemLimit You are exceeding the Per-User Memory limit on a specific partition. You must wait until jobs finish within a partition to free up resources to allow additional jobs to run.   MaxSubmitJobLimit You are trying to submit more than 5000 jobs. There is a 5000 job limit per-user for queued and running jobs. Wait until some of your jobs finish, then you can continue submitting jobs.   ReqNodeNotAvail, Reserved for maintenance The time limit of your job would cause it to overlap with an upcoming maintenance. You can either reduce your job’s runtime or wait for the maintenance to complete.    This is only a small number of the most common reasons. For a full list please see Slurm’s Job Reason Codes page. If you are confused as to why you’re getting a specific reason, please reach out to support.\nAdvanced Jobs There is a third way of submitting jobs by using steps. Single Step submission:\nsrun \u003ccommand\u003e  Under a single step job your command will hang until appropriate resources are found and when the step command is finished the results will be sent back on STDOUT. This may take some time depending on the job load of the cluster. Multi Step submission:\nsalloc -N 4 bash -l srun \u003ccommand\u003e ... srun \u003ccommand\u003e exit  Under a multi step job the salloc command will request resources and then your parent shell will be running on the head node. This means that all commands will be executed on the head node unless preceeded by the srun command. You will also need to exit this shell in order to terminate your job.\nArray Jobs If a large batch of fairly similar jobs need to be submitted, an Array Job might be a good option. For an array job, include the --array parameter in your sbatch script, similar to the following:\n#!/bin/bash #SBATCH --nodes=1 #SBATCH --ntasks=1 #SBATCH --cpus-per-task=2 # This will be the number of CPUs per individual array job #SBATCH --mem=1G # This will be the memory per individual array job #SBATCH --time=0-00:15:00 # 15 minutes #SBATCH --array=1-2500 #SBATCH --job-name=\"just_a_test\" echo \"I have array ID ${SLURM_ARRAY_TASK_ID}\"  Within each job, the SLURM_ARRAY_TASK_ID environment variable is set and can be used to slightly change how each job is run.\nNote that there is a 2500 job limit for array jobs.\nMore information can be found on the Slurm Documentation including other Environment Variables that are set per-job.\nHighmem Jobs The highmem partition does not have a default amount of memory set, however it does has a minimum limit of 100GB per job. This means that you need to explicity request at least 100GB or more of memory.\nNon-Interactive:\nsbatch -p highmem --mem=100g --time=24:00:00 SBATCH_SCRIPT.sh  Interactive\nsrun -p highmem --mem=100g --time=24:00:00 --pty bash -l  Of course you should adjust the time argument according to your job requirements.\nGPU Jobs GPU nodes have multiple GPUs, and vary in type (K80, P100, or A100). This means you need to request how many GPUs and of what type that you would like to use.\nTo request a gpu of any type, only indicate how many GPUs you would like to use.\nNon-Interactive:\nsbatch -p gpu --gres=gpu:1 --mem=100g --time=1:00:00 SBATCH_SCRIPT.sh  Interactive\nsrun -p gpu --gres=gpu:4 --mem=100g --time=1:00:00 --pty bash -l  Since the HPCC Cluster has three types of GPUs installed (K80s, P100s, and A100s), GPUs can be requested explicitly by type.\nNon-Interactive:\nsbatch -p gpu --gres=gpu:k80:1 --mem=100g --time=1:00:00 SBATCH_SCRIPT.sh sbatch -p gpu --gres=gpu:p100:1 --mem=100g --time=1:00:00 SBATCH_SCRIPT.sh sbatch -p gpu --gres=gpu:a100:1 --mem=100g --time=1:00:00 SBATCH_SCRIPT.sh  Interactive\nsrun -p gpu --gres=gpu:k80:1 --mem=100g --time=1:00:00 --pty bash -l srun -p gpu --gres=gpu:p100:1 --mem=100g --time=1:00:00 --pty bash -l srun -p gpu --gres=gpu:a100:1 --mem=100g --time=1:00:00 --pty bash -l  Of course you should adjust the time argument according to your job requirements.\nOnce your job starts your code must reference the environment variable “CUDA_VISIBLE_DEVICES” which will indicate which GPUs have been assigned to your job. Most CUDA enabled software, like MegaHIT, will check this environment variable and automatically limit accordingly.\nFor example, after reserving 4 GPUs for a NAMD2 job:\necho $CUDA_VISIBLE_DEVICES 0,1,2,3 namd2 +idlepoll +devices $CUDA_VISIBLE_DEVICES MD1.namd  Each group is limited to a maximum of 8 GPUs on the gpu partition. Please be respectful of others and keep in mind that the GPU nodes are a limited shared resource. Since the CUDA libraries will only run with GPU hardware, development and compiling of code must be done within a job session on a GPU node.\nHere are a few more examples of jobs that utilize more complex features (ie. array, dependency, MPI etc): Slurm Examples\nWeb Browser Access Ports Some jobs require web browser access in order to utilize the software effectively. These kinds of jobs typically use (bind) ports in order to provide a graphical user interface (GUI) through a web browser. Users are able to run jobs that use (bind) ports on a compute node. Any port can be used on any compute node, as long as the port number is greater than 1000 and it is not already in use (bound).\nTunneling Once a job is running on a compute node and bound to a port, you may access this compute node via a web browser. This is accomplished by using 2 chained SSH tunnels to route traffic through our firewall. This acts much like 2 runners in a relay race, handing the baton to the next runner, to get past a security checkpoint.\nRunning the following command on your local machine will create a tunnel that goes though a headnode and connect to a compute node on a particular port.\nssh -NL 8888:NodeName:8888 username@cluster.hpcc.ucr.edu  Port 8888 (first) is the local port you will be using on your local machine. NodeName is the compute node where where job is running, which can be found by using the squeue -u $USER command. Port 8888 (second) is the remote port on the compute node. Again, the NodeName and ports will be different depending on where your job runs and what port your job uses.\nAt this point you may need to provide a password to make the SSH tunnel. Once this has succeeded, the command will hang (this is normal). Leave this session connected, if you close it your tunnel will be closed.\nThen open a browser on your local computer (PC/laptop) and point it to:\nhttp://localhost:8888  If your job uses TSL/SSL, so you may need to try https if the above does not work:\nhttps://localhost:8888  Examples   A perfect example of this method is used for Jupyter Lab/Notebook. For more details please refer to the JupyterLab Usage page.\n  RStudio Server instances can also be started directly on a compute node and accessed via an SSH tunnel. For details see here.\n  Desktop Environments VNC Server (cluster) Start VNC Server\nLog into the cluster:\nssh username@cluster.hpcc.ucr.edu  The VNC programs are only available on Compute Nodes, and additionally the first time you run the vncserver it will need to be configured:\nsrun -p epyc -c 2 --mem 4GB -t 10:00 --pty bash -l # Start compute session vncserver -fg # Configure VNC exit # Leave compute session  You should set a password for yourself, and the read-only password is optional.\nAfter your vncserver is configured, submit a vncserver job to get it started:\nsbatch -p epyc --cpus-per-task=4 --mem=10g --time=2:00:00 --wrap='vncserver -fg' --output='vncserver-%j.out'   Note: Appropriate job resources should be requested based on the processes you will be running from within the VNC session.\n Check the contents of your job log to determine the NodeName and Port you were assigned:\ncat vncserver-*.out  The contents of your slurm job log should be similar to the following:\nvncserver New 'i54:1' desktop is i54:1 Creating default startup script /rhome/username/.vnc/xstartup Starting applications specified in /rhome/username/.vnc/xstartup Log file is /rhome/username/.vnc/i54:1.log  The VNC Port used should be 5900+N, N being the display number mentioned above in the format NodeName:DisplayNumber (ie. i54:1). In this example (default), the port is 5901, if this Port were already in use then the vncserver will automatically increment the DisplayNumber and you might find something like i54:2 or i54:3 and so on.\nStop VNC Server\nTo stop the vncserver, you can click on the logout option from the upper right hand menu from within your VNC desktop environment. If you want to kill your vncserver manually, then you will need to do the following:\nssh NodeName 'vncserver -kill :DisplayNumber'  You will need to replace NodeName with the node name of your where your job is running, and the DisplayNumber with the DisplayNumber from your slurm job log.\nVNC Client (Desktop/Laptop) After you know the NodeName and VNC Port you should be able to create an SSH tunnel to your vncserver, like so:\nssh -N -L Port:NodeName:Port cluster.hpcc.ucr.edu  Now let us create an SSH tunnel on your local machine (desktop/laptop) using the NodeName and VNC Port from above:\nssh -L 5901:i54:5901 cluster.hpcc.ucr.edu  After you have logged into the cluster with this shell, log into the node where your VNC server is running:\nssh NodeName  After you have logged into the correct NodeName, just let this terminal sit here, do not close it.\nThen launch vncviewer on your local system (laptop/workstation), like so:\nvncviewer localhost:5901  After launching the vncviewer, and providing your VNC password (not your cluster password), you should be able to see a Linux desktop environment.\nFor more information regarding tunnels and VNC in MS Windows, please refer More VNC Info.\nParallelization There are 3 major ways to parallelize work on the cluster:\n Batch Thread MPI  Parallel Methods For batch jobs, all that is required is that you have a way to split up the data and submit multiple jobs running with the different chunks. Some data sets, for example a FASTA file is very easy to split up (ie. fasta-splitter). This can also be more easily achieved by submitting an array job. For more details please refer to Advanced Jobs.\nFor threaded jobs, your software must have an option referring to “number of threads” or “number of processors”. Once the thread/processor option is identified in the software, (ie. blastn flag -num_threads 4) you can use that as long as you also request the same number of CPU cores (ie. slurm flag --cpus-per-task=4).\nFor MPI jobs, your software must be MPI enabled. This generally means that it was compiled with MPI libraries. Please refer to the user manual of the software you wish to use as well as our documentation regarding MPI. It is important that the number of cores used is equal to the number requested.\nIn Slurm you will need 2 different flags to request cores, which may seem similar, however they have different purposes:\n The --cpus-per-task=N will provide N number of virtual cores with locality as a factor. Closer virtual cores can be faster, assuming there is a need for rapid communication between threads. Generally, this is good for threading, however not so good for independent subprocesses nor for MPI. The --ntasks=N flag will provide N number of physical cores on a single or even multiple nodes. These cores can be further away, since the need for physical CPUs and dedicated memory is more important. Generally this is good for independent subprocesses, and MPI, however not so good for threading.  Here is a table to better explain when to use these Slurm options:\n   Slurm Flag Single Threaded Multi Threaded (OpenMP) MPI only MPI + Multi Threaded (hybrid)     --cpus-per-task  X  X   --ntasks   X X    As you can see:\n A single threaded job would use neither Slurm option, since Slurm already assumes at least a single core. A multi threaded OpenMP job would use --cpus-per-task. A MPI job would use --ntasks. A Hybrid job would use both.  For more details on how these Slurm options work please review Slurm Multi-core/Multi-thread Support.\nMPI MPI stands for the Message Passing Interface. MPI is a standardized API typically used for parallel and/or distributed computing. The HPCC cluster has a custom compiled versions of MPI that allows users to run MPI jobs across multiple nodes. These types of jobs have the ability to take advantage of hundreds of CPU cores symultaniously, thus improving compute time.\nMany implementations of MPI exists, however we only support the following:\n Open MPI MPICH IMPI  For general information on MPI under Slurm look here. If you need to compile an MPI application then please email support@hpcc.ucr.edu for assistance.\nWhen submitting MPI jobs it is best to ensure that the nodes are identical, since MPI is sensitive to differences in CPU and/or memory speeds. The batch and intel partitions are designed to be homogeneous, however, the short partition is a mixed set of nodes. When using the short partition for MPI append the constraint flag for Slurm.\nShort Example\nHere is an example that shows how to ensure that your job will only run on intel nodes from the short partition:\nsbatch -p short --constraint=intel myJobScript.sh  NAMD Example\nTo run a NAMD2 process as an OpenMPI job on the cluster:\n  Log-in to the cluster\n  Create SBATCH script\n#!/bin/bash -l #SBATCH -J c3d_cr2_md #SBATCH -p epyc #SBATCH --ntasks=32 #SBATCH --mem=16gb #SBATCH --time=01:00:00 # Load needed modules # You could also load frequently used modules from within your ~/.bashrc module load slurm # Should already be loaded module load openmpi # Should already be loaded module load namd # Run job utilizing all requested processors # Please visit the namd site for usage details: http://www.ks.uiuc.edu/Research/namd/ mpirun --mca btl ^tcp namd2 run.conf \u0026\u003e run_namd.log    Submit SBATCH script to Slurm queuing system\nsbatch run_namd.sh    Maker Example\nOpenMPI does not function properly with Maker, you must use MPICH. Our version of MPICH does not use the mpirun/mpiexec wrappers, instead use srun:\n#!/bin/bash -l #SBATCH -p epyc #SBATCH --ntasks=32 #SBATCH --mem=16gb #SBATCH --time=01:00:00 # Load maker module load maker/2.31.11 mpirun maker # Provide appropriate maker options here  More examples The range of differing jobs and how to submit them is endless:\n1. Singularity containers 2. Database services 3. Graphical user interfaces 4. Etc ...  For a growing list of examples please visit HPCC Slurm Examples.\n  These only list the most common CPU Extensions for each platform. A full list of supported extensions can be found using the lscpu command on the respective node type. ↩︎\n   ","excerpt":"What is a Job? Submitting and managing jobs is at the heart of using …","ref":"/manuals/hpc_cluster/jobs/","title":"Managing Jobs"},{"body":"SSH Keys on MS Windows What are SSH keys? SSH (Secure Shell) keys are an access credential that is used in the SSH protocol.\nThe private key remains on the system being used to access the HPCC cluster and is used to decrypt information that is exchanged in the transfer between the HPCC cluster and your system.\nA public key file is used to encrypt information, and is stored on your own system. The public key file is stored on the HPCC cluster and contains a list of authorized public keys.\nWhy do you need SSH keys? HPCC supports two authentication methods; Password+DUO and SSH Keys. The Password+DUO method requires a UCR NetID, if you do not have this then you will need to use SSH keys in order to access the HPCC cluster.\nWhat you need MobaXterm You will need to install MobaXterm in order to generate your SSH keys and also to transfer the keys to the cluster.\n Download MobaXterm from here. Unzip Double click portable version of exe and run the MobaXterm application.  Persistent Home Directory By default, MobaXterm will not have a presistent home directory, meaning any files saved there will be lost the next time you start it. To use a persistent home directory, go to “Setting \u003e General \u003e Persistent Home Directory”. If this shows “\u003c Temp Directory \u003e”, then change it to where you would like your home directory to be stored on your computer.\nFinding Files on Windows To find where a file is located on windows from within a MobaXterm terminal, you can use the open command. For example open . to open the directory you are currently in, or open ~ to open your home directory.\nFileZilla If you choose to upload you SSH key to the HPCC cluster with a GUI app, you will need to install FileZilla or a similar sFTP/SCP client. Note, FileZilla is not required if you use the command-line approach below.\n Download the FileZilla Client for Windows here. * Make sure your Windows system is updated to the latest version. Follow the install wizard to complete the install of Filezilla.  Create SSH Keys (MobaXterm) The following provides instructions for both (A) command-line-based and (B) GUI-based SSH key creation. Users need to choose which option is more suitable for them. Usually, the command-line based approach is much quicker even for users without command-line experience since it only requires to copy and paste a few lines of code.\n(A) Command-line-based SSH key creation Creating SSH keys in MobaXterm from the command-line is straightforward and almost identical to creating SSH keys under macOS and Linux (see here). To create the SSH key pair from the command-line, open the MobaXterm terminal and then execute the following commands. This can be done by a simple copy and paste rather than typing, and then pressing the enter key. Users who wish to use WinSCP instead of FileZilla as sFTP client need to follow the key generation instructions of this software as outlined here.\nmkdir -p ~/.ssh # creates SSH directory ssh-keygen -t rsa -f ~/.ssh/id_rsa # creates key pair (private and public)  Next, check the content of the newly created .ssh directory with ls -al .ssh/. It should contain files for the private and public keys that are named id_rsa and id_rsa.pub, respectively. Importantly, this private key file should not be shared.\nNote, when using PuTTY (and WinSCP) instead of MobaXterm for generating SSH keys, then the private key is stored in PuTTY’s proprietary key format, which is indicated by a .ppk file extension. A key of this format is required when using PuTTY as SSH client, and it cannot be used with other SSH client tools.\nThe public key is the one that needs to be uploaded to the remote system one wishes to connect to. On the HPCC cluster it needs to be saved in a file located under this location of your home directory: ~/.ssh/authorized_keys. The upload can be performed with an sFTP/SCP GUI app like the one built into MobaXterm or FileZilla (see GUI section below). Copying the key from MobaXterm into the clipboard (e.g. in less) and then pasting it into the corresponding file opened on the remote system with a code editor like vim is another but more advanced option. The following shows how to upload the private SSH key from the command-line in MobaXterm to the HPCC cluster using the scp command, where it is important that users replace \u003cusername\u003e with their own username on the HPCC cluster. Importantly, only one of the following two commands should be used. The first one should be used if an authorized_keys file does not exist yet, e.g. when a user configures SSH key accees on the HPCC system for the first time. The second one should be used to append a new public SSH key to an already existing authorized_keys file.\n  Create new authorized_keys file\nscp .ssh/id_rsa.pub \u003cusername\u003e@cluster.hpcc.ucr.edu:.ssh/authorized_keys    Append SSH key to already existing authorized_keys file\nscp .ssh/id_rsa.pub \u003cusername\u003e@cluster.hpcc.ucr.edu:tmpkey \u0026\u0026 ssh username@cluster.hpcc.ucr.edu \"cat tmpkey \u003e\u003e ~/.ssh/authorized_keys \u0026\u0026 rm tmpkey\"    Note, prior to setting up SSH key access both of the above scp commands require functional password/DUO credentials. Users who do not have password/DUO access (e.g. non-UCR users) will need to email their public SSH key to support@hpcc.ucr.edu so that the systems admin can add their public SSH key to ~/.ssh/authorized_keys of the corresponding user account.\n(B) GUI-based SSH key creation Please use the following GUI-based instructions for generating SSH keys at your own risk. The above command-line approach is preferred since it is much easier and reliable.\n  Begin by clicking on the tools drop down on the upper menu bar\n  Find and click on the MobaKeyGen (SSH key generator) option\n  A window should appear to create a new SSH key. Click on generate to create a new SSH key pair. Follow the on menu instructions.\n  Once your key has been created, enter a password in the key passphrase field to password protect your key. Click on conversions in the tool bar and click on Export OpenSSH Key. Save this key as id_rsa and put the file in an easy to access location. Click on Save private key to save the private key with an extension of .ppk to use with MobaXterm or FileZilla. Save the key as mobaxterm_privkey and put the file in an easy to access location.\n  Highlight EVERYTHING in the box labeled “Public key for pasting into OpenSSH authorized_keys file” then right-click on it and choose Copy. Open Notepad and paste the copied text. Save the file as id_rsa.pub and put the file in an easy to access location.\n  Keys Location SSH keys should be saved under the location C:\\Users\\username\\.ssh.\nConfigure SSH Keys Public SSH Key Now that you have created your SSH keys, and renamed them, you will need to placed the public key (id_rsa.pub) on the cluster using the cluster.hpcc.ucr.edu\n  Start the Filezilla application.\n  Open the Site Manager button in the top bar of icons.\n  Click on New Site, rename it (optional) and press enter.\n  Make sure to use the following settings for the site:\n Protocol: should be set to SFTP - SSH File Transfer Protocol Host: type in cluster.hpcc.ucr.edu Port: type 22 Logon Type: set to Interactive User: type in your HPCC username    Click “Connect”. If the next pop up prompts you, then check the box that states Always trust this host, add this key to the cache, then click the OK button.\n  You will need to create a .ssh directory, if it doesn’t already exist, to hold your SSH keys. On the right hand side, right click and click on the Create directory option under your home folder location.   A window will appear to name the new directory. Name should be the following format: /rhome/username/.ssh. After naming the new directory click on OK.   Right click on the new .ssh directory that has been created. Find and click on File permissions.   A window with the directory permissions will appear. The .ssh directory needs exact permissions in order for it to function properly. Follow the image below to apply the permissions.   Now that you are connected to Filezilla transfer your public SSH key from your system by dragging the file id_rsa.pub and dropping it into the HPCC cluster direcotry /rhome/username/.ssh/.\n  Once the file is transferred to the cluster, be sure to rename id_rsa.pub to authorized_keys. Alternatively, if an authorized_keys file already exists, then you can edit the authorized_keys file (Right Click \u003e View/Edit) and place the contents of the id_rsa.pub file inside of it.  Private SSH Key Once your public key is in place, now you can configure Filezilla to use your private SSH key and connect to the cluster through the cluster.hpcc.ucr.edu server.\n  Open Filezilla Site Manager button in the top bar of icons.\n  Click on the HPCC created in the “Public SSH Key” section\n  Change the settings to the following:\n Protocol: should be set to SFTP - SSH File Transfer Protocol Host: type in cluster.hpcc.ucr.edu Port: type 22 Logon Type: set to Key file User: type in your HPCC username  After these fields are finalized, click the Browse.. button.\n  Navigate to the folder you saved your private key file in and open the private key file mobaxterm_privkey.ppk. You should see the added keyfile in the Key file: box, then click Connect.\n  Subsequnt connections can be done from the Quickconnect history by clicking on the down arrow to the right side of the Quickconnect button. Remember to select the cluster.hpcc.ucr.edu address.\n  Transfer files by double clicking or drag-n-drop. For more details regarding file transfers vist Filezilla Usage.\n  ","excerpt":"SSH Keys on MS Windows What are SSH keys? SSH (Secure Shell) keys are …","ref":"/manuals/hpc_cluster/sshkeys/sshkeys_winos/","title":"SSH Keys Microsoft Windows"},{"body":"The following packages are provided as part of the CentOS 8 operating system. They can be used without loading any modules.\n   Package Name Version     GConf2 3.2.6-22   GMT 6.1.0-1   GMT-common 6.1.0-1   GraphicsMagick 1.3.38-1   ImageMagick 6.9.12.86-1   ImageMagick-c++ 6.9.12.86-1   ImageMagick-c++-devel 6.9.12.86-1   ImageMagick-devel 6.9.12.86-1   ImageMagick-libs 6.9.12.86-1   LibRaw 0.19.5-3   ModemManager-glib 1.20.2-1   NetworkManager 1:1.40.16-4   NetworkManager-libnm 1:1.40.16-4   NetworkManager-team 1:1.40.16-4   NetworkManager-tui 1:1.40.16-4   NetworkManager-wifi 1:1.40.16-4   OpenEXR-devel 2.2.0-12   OpenEXR-libs 2.2.0-12   SuperLU 5.2.0-7   abattis-cantarell-fonts 0.0.25-6   accountsservice 0.6.55-4   accountsservice-libs 0.6.55-4   acl 2.2.53-1.el8   adcli 0.9.2-1   adobe-mappings-cmap 20171205-3   adobe-mappings-cmap-deprecated 20171205-3   adobe-mappings-pdf 20180407-1   adwaita-cursor-theme 3.28.0-3   adwaita-gtk2-theme 3.22.3-4   adwaita-icon-theme 3.28.0-3   alsa-lib 1.2.8-2   annobin 10.94-1   apr 1.6.3-12   apr-util 1.6.1-6.el8_8   apr-util-bdb 1.6.1-6.el8_8   apr-util-openssl 1.6.1-6.el8_8   ar_mgr 1.0-0.2.MLNX20201014.g8577618   armadillo 10.8.2-1   arpack 3.7.0-1   aspell 12:0.60.6.1-22   at-spi2-atk 2.26.2-1   at-spi2-core 2.28.0-1   atk 2.28.1-1   atk-devel 2.28.1-1   atlas 3.10.3-8.el8   audit 3.0.7-4   audit-libs 3.0.7-4   authselect 1.2.6-1   authselect-libs 1.2.6-1   autoconf 2.69-29   automake 1.16.1-7   avahi-glib 0.7-20   avahi-libs 0.7-20   basesystem 11-5   bash 4.4.20-4   bc 1.07.1-5   bind-export-libs 32:9.11.36-8.el8_8   bind-libs 32:9.11.36-8.el8_8   bind-libs-lite 32:9.11.36-8.el8_8   bind-license 32:9.11.36-8.el8_8   bind-utils 32:9.11.36-8.el8_8   binutils 2.30-119   biosdevname 0.7.3-2   bison 3.0.4-10   blas 3.8.0-8   blas-devel 3.8.0-8   bluez 5.63-1   bluez-libs 5.63-1   bluez-obexd 5.63-1   bolt 0.9.1-1   boost 1.66.0-13   boost-atomic 1.66.0-13   boost-chrono 1.66.0-13   boost-container 1.66.0-13   boost-context 1.66.0-13   boost-coroutine 1.66.0-13   boost-date-time 1.66.0-13   boost-devel 1.66.0-13   boost-fiber 1.66.0-13   boost-filesystem 1.66.0-13   boost-graph 1.66.0-13   boost-iostreams 1.66.0-13   boost-locale 1.66.0-13   boost-log 1.66.0-13   boost-math 1.66.0-13   boost-program-options 1.66.0-13   boost-random 1.66.0-13   boost-regex 1.66.0-13   boost-serialization 1.66.0-13   boost-signals 1.66.0-13   boost-stacktrace 1.66.0-13   boost-system 1.66.0-13   boost-test 1.66.0-13   boost-thread 1.66.0-13   boost-timer 1.66.0-13   boost-type_erasure 1.66.0-13   boost-wave 1.66.0-13   brotli 1.0.6-3   bubblewrap 0.4.0-1   bzip2 1.0.6-26   bzip2-devel 1.0.6-26   bzip2-libs 1.0.6-26   c-ares 1.13.0-6.el8_8   ca-certificates 2022.2.54-80.2   cairo 1.15.12-6   cairo-devel 1.15.12-6   cairo-gobject 1.15.12-6   cfitsio 3.47-1   checkpolicy 2.9-1   cheese-libs 2:3.28.0-4   chkconfig 1.19.1-1   chrony 4.2-1.el8.rocky.1   clutter 1.26.2-8   clutter-gst3 3.0.26-1   clutter-gtk 1.8.4-3   cmake 3.20.2-5   cmake-data 3.20.2-5   cmake-filesystem 3.20.2-5   cmake-rpm-macros 3.20.2-5   cogl 1.22.2-10   color-filesystem 1-20   colord 1.4.2-1   colord-gtk 0.1.26-8   colord-libs 1.4.2-1   compat-libgfortran-48 4.8.5-36.1   coreutils 8.30-15   coreutils-common 8.30-15   cpio 2.12-11   cpp 8.5.0-18   cracklib 2.9.6-15   cracklib-dicts 2.9.6-15   crda 3.18_2020.04.29-1   createrepo_c 0.17.7-6   createrepo_c-libs 0.17.7-6   cronie 1.5.2-8   cronie-anacron 1.5.2-8   crontabs 1.11-17.20190603git   crypto-policies 20221215-1.gitece0092   crypto-policies-scripts 20221215-1.gitece0092   cryptsetup-libs 2.3.7-5   ctags 5.8-23   cups-libs 1:2.2.6-51   cups-pk-helper 0.2.6-5   curl 7.61.1-30.el8_8   cyrus-sasl 2.1.27-6   cyrus-sasl-devel 2.1.27-6   cyrus-sasl-gssapi 2.1.27-6   cyrus-sasl-lib 2.1.27-6   dapl 2.1.10.1.mlnx-OFED.4.9.0.1.4   dapl-devel 2.1.10.1.mlnx-OFED.4.9.0.1.4   dapl-devel-static 2.1.10.1.mlnx-OFED.4.9.0.1.4   dapl-utils 2.1.10.1.mlnx-OFED.4.9.0.1.4   dbus 1:1.12.8-24.el8_8   dbus-common 1:1.12.8-24.el8_8   dbus-daemon 1:1.12.8-24.el8_8   dbus-glib 0.110-2   dbus-libs 1:1.12.8-24.el8_8   dbus-tools 1:1.12.8-24.el8_8   dbus-x11 1:1.12.8-24.el8_8   dconf 0.28.0-4   dcw-gmt 1.1.4-1   dejavu-fonts-common 2.35-7   dejavu-sans-fonts 2.35-7   dejavu-sans-mono-fonts 2.35-7   desktop-file-utils 0.26-1   device-mapper 8:1.02.181-9   device-mapper-libs 8:1.02.181-9   dhcp-client 12:4.3.6-49   dhcp-common 12:4.3.6-49   dhcp-libs 12:4.3.6-49   diffutils 3.6-6   dmidecode 1:3.3-4   dnf 4.7.0-16   dnf-data 4.7.0-16   dnf-plugins-core 4.0.21-19   dos2unix 7.4.0-3   dosfstools 4.1-6   doxygen 1:1.8.14-12   dracut 049-223.git20230119   dracut-config-rescue 049-223.git20230119   dracut-network 049-223.git20230119   dracut-squash 049-223.git20230119   drpm 0.4.1-3   dump_pr 1.0-0.2.MLNX20201014.g8577618   duo_unix 1.12.1-5   duo_unix-selinux 1.12.1-5   dwz 0.12-10   e2fsprogs 1.45.6-5   e2fsprogs-libs 1.45.6-5   efi-srpm-macros 3-3   efivar-libs 37-4   eigen3-devel 3.3.4-6   elfutils 0.188-3   elfutils-debuginfod-client 0.188-3   elfutils-default-yama-scope 0.188-3   elfutils-libelf 0.188-3   elfutils-libelf-devel 0.188-3   elfutils-libs 0.188-3   elinks 0.12-0.58.pre6   emacs 1:26.1-10.el8_8   emacs-common 1:26.1-10.el8_8   emacs-filesystem 1:26.1-10.el8_8   enchant2 2.2.3-3   environment-modules 5.0.1-1   eog 3.28.4-1   esmtp 1.2-15   ethtool 2:5.13-2   evolution-data-server 3.28.5-20   evolution-data-server-langpacks 3.28.5-20   exempi 2.4.5-2   exiv2 0.27.5-2   exiv2-libs 0.27.5-2   expat 2.2.5-11   expat-devel 2.2.5-11   fail2ban 1.0.2-3   fail2ban-firewalld 1.0.2-3   fail2ban-selinux 1.0.2-3   fail2ban-sendmail 1.0.2-3   fail2ban-server 1.0.2-3   fftw 3.3.5-11   fftw-devel 3.3.5-11   fftw-libs 3.3.5-11   fftw-libs-double 3.3.5-11   fftw-libs-long 3.3.5-11   fftw-libs-quad 3.3.5-11   fftw-libs-single 3.3.5-11   fftw2 2.1.5-40   fftw2-devel 2.1.5-40   file 5.33-24   file-libs 5.33-24   filesystem 3.8-6   findutils 1:4.6.0-20   firewalld 0.9.3-13   firewalld-filesystem 0.9.3-13   flac-libs 1.3.2-9   flatpak 1.10.7-1   flatpak-selinux 1.10.7-1   flatpak-session-helper 1.10.7-1   flex 2.6.1-9   fontconfig 2.13.1-4   fontconfig-devel 2.13.1-4   fontpackages-filesystem 1.44-22   freeglut 3.0.0-8   freetype 2.9.1-9   freetype-devel 2.9.1-9   freexl 1.0.6-4   fribidi 1.0.4-9   fribidi-devel 1.0.4-9   fstrm 0.6.1-3   ftp 0.17-78   fuse 2.9.7-16   fuse-common 3.3.0-16   fuse-libs 2.9.7-16   gawk 4.2.1-4   gc 7.6.4-3   gc-devel 7.6.4-3   gcc 8.5.0-18   gcc-c++ 8.5.0-18   gcc-gfortran 8.5.0-18   gcc-objc 8.5.0-19   gcc-objc++ 8.5.0-19   gcc-plugin-annobin 8.5.0-18   gcc-toolset-9 9.0-4   gcc-toolset-9-annobin 9.08-4   gcc-toolset-9-binutils 2.32-17   gcc-toolset-9-dwz 0.12-1.1   gcc-toolset-9-dyninst 10.1.0-1   gcc-toolset-9-elfutils 0.176-5   gcc-toolset-9-elfutils-libelf 0.176-5   gcc-toolset-9-elfutils-libs 0.176-5   gcc-toolset-9-gcc 9.2.1-2.3   gcc-toolset-9-gcc-c++ 9.2.1-2.3   gcc-toolset-9-gcc-gdb-plugin 9.2.1-2.3   gcc-toolset-9-gcc-gfortran 9.2.1-2.3   gcc-toolset-9-gdb 8.3-1   gcc-toolset-9-libquadmath-devel 9.2.1-2.3   gcc-toolset-9-libstdc++-devel 9.2.1-2.3   gcc-toolset-9-ltrace 0.7.91-1   gcc-toolset-9-make 1:4.2.1-2   gcc-toolset-9-perftools 9.0-4   gcc-toolset-9-runtime 9.0-4   gcc-toolset-9-strace 5.1-6   gcc-toolset-9-systemtap 4.1-4   gcc-toolset-9-systemtap-client 4.1-4   gcc-toolset-9-systemtap-devel 4.1-4   gcc-toolset-9-systemtap-runtime 4.1-4   gcc-toolset-9-toolchain 9.0-4   gcc-toolset-9-valgrind 1:3.15.0-9   gcr 3.28.0-1   gd 2.2.5-7   gdal 3.0.4-11   gdal-devel 3.0.4-11   gdal-libs 3.0.4-11   gdb-headless 8.2-19   gdbm 1:1.18-2   gdbm-devel 1:1.18-2   gdbm-libs 1:1.18-2   gdisk 1.0.3-11   gdk-pixbuf2 2.36.12-5   gdk-pixbuf2-devel 2.36.12-5   gdk-pixbuf2-modules 2.36.12-5   gdm 1:40.0-27   geoclue2 2.5.5-2   geoclue2-libs 2.5.5-2   geocode-glib 3.26.0-3   geolite2-city 20180605-1   geolite2-country 20180605-1   geos 3.7.2-1   geos-devel 3.7.2-1   gettext 0.19.8.1-17   gettext-libs 0.19.8.1-17   ghc-srpm-macros 1.4.2-7   ghostscript 9.27-6   giflib 5.1.4-3   git 2.39.3-1   git-core 2.39.3-1   git-core-doc 2.39.3-1   gjs 1.56.2-5   gl-manpages 1.1-15.20161227   glib-networking 2.56.1-1.1   glib2 2.56.4-161   glib2-devel 2.56.4-161   glibc 2.28-225   glibc-common 2.28-225   glibc-devel 2.28-225   glibc-gconv-extra 2.28-225   glibc-headers 2.28-225   glibc-langpack-en 2.28-225   glx-utils 8.4.0-5.20181118git1830dcb   gmp 1:6.1.2-10   gmp-c++ 1:6.1.2-10   gmp-devel 1:6.1.2-10   gnome-autoar 0.2.3-2   gnome-bluetooth 1:3.34.3-1   gnome-bluetooth-libs 1:3.34.3-1   gnome-classic-session 3.32.1-33   gnome-control-center 3.28.2-37   gnome-control-center-filesystem 3.28.2-37   gnome-desktop3 3.32.2-1.el8_8   gnome-keyring 3.28.2-1   gnome-keyring-pam 3.28.2-1   gnome-menus 3.13.3-11   gnome-online-accounts 3.28.2-4   gnome-session 3.28.1-20   gnome-session-wayland-session 3.28.1-20   gnome-session-xsession 3.28.1-20   gnome-settings-daemon 3.32.0-19   gnome-shell 3.32.2-50   gnome-shell-extension-apps-menu 3.32.1-33   gnome-shell-extension-common 3.32.1-33   gnome-shell-extension-desktop-icons 3.32.1-33   gnome-shell-extension-horizontal-workspaces 3.32.1-33   gnome-shell-extension-launch-new-instance 3.32.1-33   gnome-shell-extension-places-menu 3.32.1-33   gnome-shell-extension-window-list 3.32.1-33   gnome-terminal 3.28.3-3   gnome-themes-standard 3.22.3-4   gnupg2 2.2.20-3   gnupg2-smime 2.2.20-3   gnuplot 5.2.4-3   gnuplot-common 5.2.4-3   gnutls 3.6.16-6   go-srpm-macros 2-17   gobject-introspection 1.56.1-1   google-droid-sans-fonts 20120715-13   gperftools-devel 1:2.7-9   gperftools-libs 1:2.7-9   gpfs 5.1   gpfs 5.1   gpfs 5.1   gpfs 5.1   gpfs 8.0.55-19   gpfs 5.1.8-0   gpfs 5.1   gpfs 5.1   gpgme 1.13.1-11   gpgme-devel 1.13.1-11   gpm-libs 1.20.7-17   gpsbabel 1.6.0-3   graphite2 1.3.10-10   graphite2-devel 1.3.10-10   graphviz 2.40.1-44   grep 3.1-6   grilo 0.3.6-3   groff-base 1.22.3-18   grub2-common 1:2.02-148.el8_8.1.rocky.0   grub2-pc 1:2.02-148.el8_8.1.rocky.0   grub2-pc-modules 1:2.02-148.el8_8.1.rocky.0   grub2-tools 1:2.02-148.el8_8.1.rocky.0   grub2-tools-efi 1:2.02-148.el8_8.1.rocky.0   grub2-tools-extra 1:2.02-148.el8_8.1.rocky.0   grub2-tools-minimal 1:2.02-148.el8_8.1.rocky.0   grubby 8.40-47   gsettings-desktop-schemas 3.32.0-6   gshhg-gmt-nc4 2.3.7-1   gshhg-gmt-nc4-full 2.3.7-1   gshhg-gmt-nc4-high 2.3.7-1   gsl 2.5-1   gsl-devel 2.5-1   gsm 1.0.17-5   gssproxy 0.8.0-21   gstreamer1 1.16.1-2   gstreamer1-plugins-bad-free 1.16.1-1   gstreamer1-plugins-base 1.16.1-2   gstreamer1-plugins-good 1.16.1-3   gtk-update-icon-cache 3.22.30-11   gtk2 2.24.32-5   gtk2-devel 2.24.32-5   gtk3 3.22.30-11   guile 5:2.0.14-7   gvfs 1.36.2-14   gvfs-client 1.36.2-14   gzip 1.9-13   hardlink 1:1.3-6   harfbuzz 1.7.5-3   harfbuzz-devel 1.7.5-3   harfbuzz-icu 1.7.5-3   hcoll 4.4.2970-1   hdf 4.2.14-5   hdf5 1.10.5-4   hdf5-devel 1.10.5-4   hdparm 9.54-4   hicolor-icon-theme 0.17-2   hostname 3.20-6   htop 3.2.1-1   httpd-tools 2.4.37-56.module+el8.8.0+1284+07ef499e   hunspell 1.6.2-1   hunspell-en 0.20140811.1-12   hunspell-en-GB 0.20140811.1-12   hunspell-en-US 0.20140811.1-12   hwdata 0.314-8.16   hwloc 2.2.0-3   hwloc-devel 2.2.0-3   hwloc-libs 2.2.0-3   hyphen 2.8.8-9   ibacm 41mlnx1-OFED.4.3.3.0.0   ibdump 6.0.0-1   ibsim 0.10-1   ibus 1.5.19-14   ibus-gtk2 1.5.19-14   ibus-gtk3 1.5.19-14   ibus-libs 1.5.19-14   ibus-setup 1.5.19-14   ibutils2 2.1.1-0.121.MLNX20200324.g061a520   iio-sensor-proxy 2.4-3   ilmbase 2.2.0-13   ilmbase-devel 2.2.0-13   ima-evm-utils 1.3.2-12   infiniband-diags 5.6.0.MLNX20200211.354e4b7-0.1   infiniband-diags-compat 5.6.0.MLNX20200211.354e4b7-0.1   info 6.5-7   initscripts 10.00.18-1   ipcalc 0.2.4-4   ipmitool 1.8.18-18   iproute 5.18.0-1.1   iprutils 2.4.19-1   ipset 7.1-1   ipset-libs 7.1-1   iptables 1.8.4-24   iptables-ebtables 1.8.4-24   iptables-libs 1.8.4-24   iputils 20180629-10   irqbalance 2:1.9.0-4   iser 4.9-OFED.4.9.7.1.0.1.kver.4.18.0_477.21.1.el8_8   isert 4.9-OFED.4.9.7.1.0.1.kver.4.18.0_477.21.1.el8_8   isl 0.16.1-6   iso-codes 3.79-2   iw 4.14-5   iwl100-firmware 39.31.5.1-114.el8_8   iwl1000-firmware 1:39.31.5.1-114.el8_8   iwl105-firmware 18.168.6.1-114.el8_8   iwl135-firmware 18.168.6.1-114.el8_8   iwl2000-firmware 18.168.6.1-114.el8_8   iwl2030-firmware 18.168.6.1-114.el8_8   iwl3160-firmware 1:25.30.13.0-114.el8_8   iwl5000-firmware 8.83.5.1_1-114.el8_8   iwl5150-firmware 8.24.2.2-114.el8_8   iwl6000-firmware 9.221.4.1-114.el8_8   iwl6000g2a-firmware 18.168.6.1-114.el8_8   iwl6050-firmware 41.28.5.1-114.el8_8   iwl7260-firmware 1:25.30.13.0-114.el8_8   jansson 2.14-1   jasper-devel 2.0.14-5   jasper-libs 2.0.14-5   jbig2dec-libs 0.16-1   jbigkit-libs 2.1-14   json-c 0.13.1-3   json-glib 1.4.4-1   kbd 2.0.4-10   kbd-legacy 2.0.4-10   kbd-misc 2.0.4-10   kernel 4.18.0-477.15.1   kernel 4.18.0-477.21.1   kernel-core 4.18.0-477.15.1   kernel-core 4.18.0-477.21.1   kernel-devel 4.18.0-477.15.1   kernel-devel 4.18.0-477.21.1   kernel-headers 4.18.0-477.21.1   kernel-mft 4.15.1-9.kver.4.18.0_477.21.1.el8_8   kernel-modules 4.18.0-477.15.1   kernel-modules 4.18.0-477.21.1   kernel-rpm-macros 131-1   kernel-tools 4.18.0-477.21.1   kernel-tools-libs 4.18.0-477.21.1   kexec-tools 2.0.25-5.el8_8   keyutils 1.5.10-9   keyutils-libs 1.5.10-9   keyutils-libs-devel 1.5.10-9   kmod 25-19   kmod-libs 25-19   knem 1.1.4.90mlnx2-OFED.23.04.0.5.2.1   knem-modules 1.1.4.90mlnx2-OFED.23.04.0.5.2.1.kver.4.18.0_477.21.1.el8_8   kpartx 0.8.4-37   krb5-devel 1.18.2-25   krb5-libs 1.18.2-25   krb5-workstation 1.18.2-25   ksh 20120801-257   lame-libs 3.100-6   langpacks-en 1.0-12   lapack 3.8.0-8   lapack-devel 3.8.0-8   lapack-static 3.8.0-8   lcms2 2.9-2   lcms2-devel 2.9-2   less 530-1   lftp 4.8.4-2   libICE 1.0.9-15   libICE-devel 1.0.9-15   libSM 1.2.3-1   libSM-devel 1.2.3-1   libX11 1.6.8-5   libX11-common 1.6.8-5   libX11-devel 1.6.8-5   libX11-xcb 1.6.8-5   libXau 1.0.9-3   libXau-devel 1.0.9-3   libXaw 1.0.13-10   libXcomposite 0.4.4-14   libXcomposite-devel 0.4.4-14   libXcursor 1.1.15-3   libXcursor-devel 1.1.15-3   libXdamage 1.1.4-14   libXdmcp 1.1.3-1   libXext 1.3.4-1   libXext-devel 1.3.4-1   libXfixes 5.0.3-7   libXfixes-devel 5.0.3-7   libXfont2 2.0.3-2   libXft 2.3.3-1   libXft-devel 2.3.3-1   libXi 1.7.10-1   libXi-devel 1.7.10-1   libXinerama 1.1.4-1   libXinerama-devel 1.1.4-1   libXmu 1.1.3-1   libXmu-devel 1.1.3-1   libXp 1.0.3-3   libXp-devel 1.0.3-3   libXpm 3.5.12-9   libXrandr 1.5.2-1   libXrandr-devel 1.5.2-1   libXrender 0.9.10-7   libXrender-devel 0.9.10-7   libXt 1.1.5-12   libXt-devel 1.1.5-12   libXtst 1.2.3-7   libXv 1.0.11-7   libXxf86misc 1.0.4-1   libXxf86vm 1.1.4-9   libacl 2.2.53-1.el8   libaec 1.0.2-3   libaec-devel 1.0.2-3   libaio 0.3.112-1   libaio-devel 0.3.112-1   libappstream-glib 0.7.14-3   libarchive 3.3.3-5   libassuan 2.5.1-3   libasyncns 0.8-14   libatasmart 0.19-14   libatomic 8.5.0-18   libatomic_ops 7.6.2-3   libattr 2.4.48-3   libavc1394 0.5.4-7   libbabeltrace 1.5.4-4   libbasicobjects 0.1.1-40   libblkid 2.32.1-42   libblockdev 2.28-2   libblockdev-crypto 2.28-2   libblockdev-fs 2.28-2   libblockdev-loop 2.28-2   libblockdev-mdraid 2.28-2   libblockdev-part 2.28-2   libblockdev-swap 2.28-2   libblockdev-utils 2.28-2   libbpf 0.5.0-1   libbsd 0.11.7-2   libbytesize 1.4-3   libcanberra 0.30-18   libcanberra-gtk3 0.30-18   libcap 2.48-5   libcap-ng 0.7.11-1   libcdio 2.0.0-3   libcdio-paranoia 10.2+0.94+2-3   libcollection 0.7.0-40   libcom_err 1.45.6-5   libcom_err-devel 1.45.6-5   libcomps 0.1.18-1   libcroco 0.6.12-4.el8_2   libcurl 7.61.1-30.el8_8   libcurl-devel 7.61.1-30.el8_8   libdaemon 0.14-15   libdap 3.19.1-2   libdatrie 0.2.9-7   libdb 5.3.28-42   libdb-utils 5.3.28-42   libdhash 0.5.0-40   libdnf 0.63.0-14   libdrm 2.4.114-1   libdv 1.0.0-27   libdvdnav 5.0.3-8   libdvdread 5.0.3-9   libedit 3.1-23.20170329cvs   libepoxy 1.5.8-1   libesmtp 1.0.6-18   libestr 0.1.10-3   libev 4.24-6   libevdev 1.10.0-1   libevent 2.1.8-5   libevent-devel 2.1.8-5   libexif 0.6.22-5   libfastjson 0.99.9-1   libfdisk 2.32.1-42   libffi 3.1-24   libffi-devel 3.1-24   libfontenc 1.1.3-8   libgcc 8.5.0-18   libgcrypt 1.8.5-7   libgdata 0.17.9-4   libgeotiff 1.5.1-1   libgexiv2 0.10.8-4   libgfortran 8.5.0-18   libglvnd 1:1.3.4-1   libglvnd-core-devel 1:1.3.4-1   libglvnd-devel 1:1.3.4-1   libglvnd-egl 1:1.3.4-1   libglvnd-gles 1:1.3.4-1   libglvnd-glx 1:1.3.4-1   libglvnd-opengl 1:1.3.4-1   libgnomekbd 3.26.0-4   libgomp 8.5.0-18   libgpg-error 1.31-1   libgpg-error-devel 1.31-1   libgs 9.27-6   libgs-devel 9.27-6   libgsf 1.14.41-5   libgta 1.2.1-1   libgtop2 2.38.0-3   libgudev 232-4   libgusb 0.3.0-1   libgweather 3.28.2-4   libgxps 0.3.0-5   libibcm 41mlnx1-OFED.4.1.0.1.0   libibcm-devel 41mlnx1-OFED.4.1.0.1.0   libibmad 5.4.0.MLNX20190423.1d917ae-0.1   libibmad-devel 5.4.0.MLNX20190423.1d917ae-0.1   libibmad-static 5.4.0.MLNX20190423.1d917ae-0.1   libibumad 43.1.1.MLNX20200211.078947f-0.1   libibumad-devel 43.1.1.MLNX20200211.078947f-0.1   libibumad-static 43.1.1.MLNX20200211.078947f-0.1   libibverbs 41mlnx1-OFED.4.9.3.0.0   libibverbs-devel 41mlnx1-OFED.4.9.3.0.0   libibverbs-devel-static 41mlnx1-OFED.4.9.3.0.0   libibverbs-utils 41mlnx1-OFED.4.9.3.0.0   libical 3.0.3-3   libicu 60.3-2   libicu-devel 60.3-2   libidn 1.34-5   libidn2 2.2.0-1   libidn2-devel 2.2.0-1   libiec61883 1.2.0-18   libijs 0.35-5   libimobiledevice 1.2.0-16   libini_config 1.3.1-40   libinput 1.16.3-3   libipa_hbac 2.8.2-3   libipt 1.6.1-8   libiptcdata 1.0.4-21   libjpeg-turbo 1.5.3-12   libjpeg-turbo-devel 1.5.3-12   libkadm5 1.18.2-25   libkcapi 1.2.0-2   libkcapi-hmaccalc 1.2.0-2   libkml 1.3.0-24   libksba 1.3.5-9   libldb 2.6.1-1   liblockfile 1.14-2   libmaxminddb 1.2.0-10   libmcpp 2.7.2-20   libmd 1.1.0-1   libmemcached 1.0.18-17   libmemcached-devel 1.0.18-17   libmemcached-libs 1.0.18-17   libmetalink 0.1.3-7   libmlx4 41mlnx1-OFED.4.7.3.0.3   libmlx4-devel 41mlnx1-OFED.4.7.3.0.3   libmlx5 41mlnx1-OFED.4.9.0.1.2   libmlx5-devel 41mlnx1-OFED.4.9.0.1.2   libmnl 1.0.4-6   libmodman 2.0.1-17   libmodulemd 2.13.0-1   libmount 2.32.1-42   libmpc 1.1.0-9.1   libndp 1.7-6   libnetfilter_conntrack 1.0.6-5   libnfnetlink 1.0.1-13   libnfsidmap 1:2.3.3-59   libnftnl 1.1.5-5   libnghttp2 1.33.0-3.el8_3   libnl3 3.7.0-1   libnl3-cli 3.7.0-1   libnma 1.8.38-1   libnotify 0.7.7-6   libnsl 2.28-225   libnsl2 1.2.0-2.20180605git4a062cf   liboauth 1.0.3-9   libobjc 8.5.0-19   libogg 2:1.3.2-10   libosinfo 1.9.0-3   libotf 0.9.13-11   libpaper 1.1.24-22   libpath_utils 0.2.1-40   libpcap 14:1.9.1-5   libpciaccess 0.14-1   libpeas 1.22.0-6   libpeas-gtk 1.22.0-6   libpipeline 1.5.0-2   libpkgconf 1.4.2-1   libplist 2.0.0-10   libpng 2:1.6.34-5   libpng-devel 2:1.6.34-5   libpng12 1.2.57-5   libpq 13.5-1   libpq-devel 13.5-1   libproxy 0.4.15-5.2   libpsl 0.20.2-6   libpwquality 1.4.4-6   libquadmath 8.5.0-18   libquadmath-devel 8.5.0-18   libquvi 0.9.4-12   libquvi-scripts 0.9.20131130-9   libraqm 0.7.0-4   libraw1394 2.1.2-5   librdmacm 41mlnx1-OFED.4.7.3.0.6   librdmacm-devel 41mlnx1-OFED.4.7.3.0.6   librdmacm-utils 41mlnx1-OFED.4.7.3.0.6   libref_array 0.1.5-40   librepo 1.14.2-4   libreport-filesystem 2.9.5-15.el8.rocky.6   libretls 3.7.0-1   librsvg2 2.42.7-4   librsvg2-devel 2.42.7-4   libseccomp 2.5.2-1   libsecret 0.18.6-1.el8.0   libselinux 2.9-8   libselinux-devel 2.9-8   libselinux-utils 2.9-8   libsemanage 2.9-9   libsepol 2.9-3   libsepol-devel 2.9-3   libserf 1.3.9-9.module+el8.7   libshout 2.2.2-19   libsigsegv 2.11-5   libsmartcols 2.32.1-42   libsmbclient 4.17.5-3   libsndfile 1.0.28-13   libsolv 0.7.20-4   libsoup 2.62.3-3   libspatialite 5.0.0-1   libsrtp 1.5.4-8   libss 1.45.6-5   libssh 0.9.6-10   libssh-config 0.9.6-10   libsss_autofs 2.8.2-3   libsss_certmap 2.8.2-3   libsss_idmap 2.8.2-3   libsss_nss_idmap 2.8.2-3   libsss_sudo 2.8.2-3   libstdc++ 8.5.0-18   libstdc++-devel 8.5.0-18   libstdc++-static 8.5.0-18   libstemmer 0-10.585svn   libsysfs 2.1.0-25   libtalloc 2.3.4-1   libtasn1 4.13-4   libtdb 1.4.7-1   libteam 1.31-4   libtevent 0.13.0-1   libthai 0.1.27-2   libtheora 1:1.1.1-21   libtiff 4.0.9-28   libtiff-devel 4.0.9-28   libtirpc 1.1.4-8   libtirpc-devel 1.1.4-8   libtool 2.4.6-25   libtool-ltdl 2.4.6-25   libtraceevent 1.5.3-1   libudisks2 2.9.0-13   libunistring 0.9.9-3   libunwind 1.3.1-3   libusb 1:0.1.5-12   libusbmuxd 1.0.10-9   libusbx 1.0.23-4   libuser 0.62-25   libutempter 1.1.6-14   libuuid 2.32.1-42   libuuid-devel 2.32.1-42   libuv 1:1.41.1-1   libv4l 1.14.2-3   libverto 0.3.2-2   libverto-devel 0.3.2-2   libverto-libev 0.3.2-2   libvisual 1:0.4.0-25   libvorbis 1:1.3.6-2   libvpx 1.7.0-8   libwacom 1.6-3   libwacom-data 1.6-3   libwayland-client 1.21.0-1   libwayland-cursor 1.21.0-1   libwayland-egl 1.21.0-1   libwayland-server 1.21.0-1   libwbclient 4.17.5-3   libwebp 1.0.0-8   libwebp-devel 1.0.0-8   libwmf-lite 0.2.9-8   libwpe 1.10.0-4   libxcb 1.13.1-1   libxcb-devel 1.13.1-1   libxcrypt 4.1.1-6   libxcrypt-devel 4.1.1-6   libxkbcommon 0.9.1-1   libxkbcommon-x11 0.9.1-1   libxkbfile 1.1.0-1   libxklavier 5.4-11   libxml2 2.9.7-16.el8_8   libxml2-devel 2.9.7-16.el8_8   libxshmfence 1.3-2   libxslt 1.1.32-6   libyaml 0.1.7-5   libzstd 1.4.4-1   linux-firmware 20230404-114.git2e92a49f   llvm-libs 15.0.7-1.module+el8.8   lmdb-libs 0.9.24-2   logrotate 3.14.0-6   lshell 0.9   lshw B.02.19.2-6   lsof 4.93.2-1   lsscsi 0.32-3   lua 5.3.4-12   lua-expat 1.3.0-12.el8   lua-json 1.3.2-9   lua-libs 5.3.4-12   lua-lpeg 1.0.1-6   lua-socket 3.0-0.17.rc1   lz4-libs 1.8.3-3   lzo 2.08-14   m17n-db 1.8.0-3   m17n-lib 1.8.0-2   m4 1.4.18-7   make 1:4.2.1-11   man-db 2.7.6.1-18   mariadb-connector-c 3.1.11-2   mariadb-connector-c-config 3.1.11-2   mariadb-connector-c-devel 3.1.11-2   mariadb-devel 3:10.3.35-1.module+el8.6   mcpp 2.7.2-20   mdadm 4.2-7   memcached 1.5.22-2   memstrack 0.2.4-2   mesa-dri-drivers 22.3.0-2   mesa-filesystem 22.3.0-2   mesa-libEGL 22.3.0-2   mesa-libGL 22.3.0-2   mesa-libGLU 9.0.0-15   mesa-libGLU-devel 9.0.0-15   mesa-libgbm 22.3.0-2   mesa-libglapi 22.3.0-2   mft 4.15   microcode_ctl 4:20220809-2.20230214.1   minizip 2.8.9-2   mlnx-ethtool 5.4-1   mlnx-fw-updater 4.9-7.1.0   mlnx-iproute2 5.4.0-1   mlnx-ofa_kernel 4.9-OFED.4.9.7.1.0.1   mlnx-ofa_kernel-devel 4.9-OFED.4.9.7.1.0.1   mlnx-ofa_kernel-modules 4.9-OFED.4.9.7.1.0.1.kver.4.18.0_477.21.1.el8_8   mlnxofed-docs 4.9-7.1.0   mobile-broadband-provider-info 20210805-1   mokutil 1:0.3.0-12   moreutils 0.63-1   motif 2.3.4-19   motif-devel 2.3.4-19   motif-static 2.3.4-19   mozjs60 60.9.0-4   mpfr 3.1.6-1   mpfr-devel 3.1.6-1   mpg123-libs 1.25.10-2   mpi-selector 1.0.3-1   mpitests_openmpi 3.2.20-e1a0676   mstflint 4.14.0-3   mtdev 1.1.5-12   munge 0.5.13-2   munge-libs 0.5.13-2   mutter 3.32.2-70   nano 2.9.8-1   nautilus 3.28.1-23   nautilus-extensions 3.28.1-23   ncurses 6.1-9.20180224   ncurses-base 6.1-9.20180224   ncurses-c++-libs 6.1-9.20180224   ncurses-compat-libs 6.1-9.20180224   ncurses-devel 6.1-9.20180224   ncurses-libs 6.1-9.20180224   net-tools 2.0-0.52.20160912git   netcat 1.225-1   netcdf 4.7.0-3   netcdf-devel 4.7.0-3   nettle 3.4.1-7   newt 0.52.20-11   nfs-utils 1:2.3.3-59   nfs4-acl-tools 0.3.5-3   nftables 1:0.9.3-26   nm-connection-editor 1.26.0-1   nmap-ncat 2:7.70-8   npth 1.5-4   nspr 4.34.0-3   nss 3.79.0-11   nss-softokn 3.79.0-11   nss-softokn-freebl 3.79.0-11   nss-sysinit 3.79.0-11   nss-util 3.79.0-11   numactl 2.0.12-13   numactl-devel 2.0.12-13   numactl-libs 2.0.12-13   ocaml-srpm-macros 5-4   ofed-scripts 4.9-OFED.4.9.7.1   ogdi 4.1.0-1   openblas 0.3.15-6   openblas-devel 0.3.15-6   openblas-openmp 0.3.15-6   openblas-openmp64 0.3.15-6   openblas-openmp64_ 0.3.15-6   openblas-serial64 0.3.15-6   openblas-serial64_ 0.3.15-6   openblas-srpm-macros 2-2   openblas-static 0.3.15-6   openblas-threads 0.3.15-6   openblas-threads64 0.3.15-6   openblas-threads64_ 0.3.15-6   openjpeg2 2.4.0-5   openldap 2.4.46-18   openldap-clients 2.4.46-18   openldap-devel 2.4.46-18   openmpi 4.0.3rc4-1   opensm 5.7.2.MLNX20201014.9378048-0.1   opensm-devel 5.7.2.MLNX20201014.9378048-0.1   opensm-libs 5.7.2.MLNX20201014.9378048-0.1   opensm-static 5.7.2.MLNX20201014.9378048-0.1   openssh 8.0p1-19   openssh-clients 8.0p1-19   openssh-server 8.0p1-19   openssl 1:1.1.1k-9   openssl-devel 1:1.1.1k-9   openssl-libs 1:1.1.1k-9   openssl-pkcs11 0.4.10-3   opus 1.3-0.4.beta   orc 0.4.28-3   os-prober 1.74-9   osinfo-db 20220727-2   osinfo-db-tools 1.9.0-1   ostree-libs 2022.2-6   p11-kit 0.23.22-1   p11-kit-server 0.23.22-1   p11-kit-trust 0.23.22-1   p7zip 16.02-20   p7zip-plugins 16.02-20   pam 1.3.1-25   pam_duo 1.12.1-5   pango 1.42.4-8   pango-devel 1.42.4-8   parted 3.2-39   passwd 0.80-4   patch 2.7.6-11   pciutils 3.7.0-3   pciutils-libs 3.7.0-3   pcre 8.42-6   pcre-cpp 8.42-6   pcre-devel 8.42-6   pcre-utf16 8.42-6   pcre-utf32 8.42-6   pcre2 10.32-3   pcre2-devel 10.32-3   pcre2-utf16 10.32-3   pcre2-utf32 10.32-3   pdsh 2.34-5   pdsh-rcmd-ssh 2.34-5   perf 4.18.0-477.21.1   perftest 4.5.0.mlnxlibs-0.3.g1121951.49417   perl 4:5.26.3-422   perl-Algorithm-Diff 1.1903-9   perl-Archive-Tar 2.30-1   perl-Archive-Zip 1.60-3   perl-Attribute-Handlers 0.99-422   perl-B-Debug 1.26-2   perl-CPAN 2.18-397   perl-CPAN-Meta 2.150010-396   perl-CPAN-Meta-Requirements 2.140-396   perl-CPAN-Meta-YAML 0.018-397   perl-Carp 1.42-396   perl-Compress-Bzip2 2.26-6   perl-Compress-Raw-Bzip2 2.081-1   perl-Compress-Raw-Zlib 2.081-1   perl-Config-Perl-V 0.30-1   perl-DB_File 1.842-1   perl-Data-Dumper 2.167-399   perl-Data-OptList 0.110-6   perl-Data-Section 0.200007-3   perl-Devel-PPPort 3.36-5   perl-Devel-Peek 1.26-422   perl-Devel-SelfStubber 1.06-422   perl-Devel-Size 0.81-2   perl-Digest 1.17-395   perl-Digest-MD5 2.55-396   perl-Digest-SHA 1:6.02-1   perl-Encode 4:2.97-3   perl-Encode-Locale 1.05-10.module+el8.6   perl-Encode-devel 4:2.97-3   perl-Env 1.04-395   perl-Errno 1.28-422   perl-Error 1:0.17025-2   perl-Exporter 5.72-396   perl-ExtUtils-CBuilder 1:0.280230-2   perl-ExtUtils-Command 1:7.34-1   perl-ExtUtils-Embed 1.34-422   perl-ExtUtils-Install 2.14-4   perl-ExtUtils-MM-Utils 1:7.34-1   perl-ExtUtils-MakeMaker 1:7.34-1   perl-ExtUtils-Manifest 1.70-395   perl-ExtUtils-Miniperl 1.06-422   perl-ExtUtils-ParseXS 1:3.35-2   perl-File-Fetch 0.56-2   perl-File-HomeDir 1.002-4   perl-File-Path 2.15-2   perl-File-Temp 0.230.600-1   perl-File-Which 1.22-2   perl-Filter 2:1.58-2   perl-Filter-Simple 0.94-2   perl-Getopt-Long 1:2.50-4   perl-Git 2.39.3-1   perl-HTTP-Tiny 0.074-1   perl-IO 1.38-422   perl-IO-Compress 2.081-1   perl-IO-Socket-IP 0.39-5   perl-IO-Socket-SSL 2.066-4.module+el8.6   perl-IO-Tty 1.12-11   perl-IO-Zlib 1:1.10-422   perl-IPC-Cmd 2:1.02-1   perl-IPC-Run 0.99-1   perl-IPC-SysV 2.07-397   perl-IPC-System-Simple 1.25-17   perl-JSON-PP 1:2.97.001-3   perl-Locale-Codes 3.57-1   perl-Locale-Maketext 1.28-396   perl-Locale-Maketext-Simple 1:0.21-422   perl-MIME-Base64 3.15-396   perl-MRO-Compat 0.13-4   perl-Math-BigInt 1:1.9998.11-7   perl-Math-BigInt-FastCalc 0.500.600-6   perl-Math-BigRat 0.2614-1   perl-Math-Complex 1.59-422   perl-Memoize 1.03-422   perl-Module-Build 2:0.42.24-5   perl-Module-CoreList 1:5.20181130-1   perl-Module-CoreList-tools 1:5.20181130-1   perl-Module-Load 1:0.32-395   perl-Module-Load-Conditional 0.68-395   perl-Module-Loaded 1:0.08-422   perl-Module-Metadata 1.000033-395   perl-Mozilla-CA 20160104-7.module+el8.6   perl-Net-Ping 2.55-422   perl-Net-SSLeay 1.88-2.module+el8.6   perl-Package-Generator 1.106-11   perl-Params-Check 1:0.38-395   perl-Params-Util 1.07-22   perl-PathTools 3.74-1   perl-Perl-OSType 1.010-396   perl-PerlIO-via-QuotedPrint 0.08-395   perl-Pod-Checker 4:1.73-395   perl-Pod-Escapes 1:1.07-395   perl-Pod-Html 1.22.02-422   perl-Pod-Parser 1.63-396   perl-Pod-Perldoc 3.28-396   perl-Pod-Simple 1:3.35-395   perl-Pod-Usage 4:1.69-395   perl-Scalar-List-Utils 3:1.49-2   perl-SelfLoader 1.23-422   perl-Socket 4:2.027-3   perl-Software-License 0.103013-2   perl-Storable 1:3.11-3   perl-Sub-Exporter 0.987-15   perl-Sub-Install 0.928-14   perl-Sys-Syslog 0.35-397   perl-Term-ANSIColor 4.06-396   perl-Term-Cap 1.17-395   perl-TermReadKey 2.37-7   perl-Test 1.30-422   perl-Test-Harness 1:3.42-1   perl-Test-Simple 1:1.302135-1   perl-Text-Balanced 2.03-395   perl-Text-Diff 1.45-2   perl-Text-Glob 0.11-4   perl-Text-ParseWords 3.30-395   perl-Text-Tabs+Wrap 2013.0523-395   perl-Text-Template 1.51-1   perl-Text-Unidecode 1.30-5   perl-Thread-Queue 3.13-1   perl-Time-Duration 1.21-3   perl-Time-HiRes 4:1.9758-2   perl-Time-Local 1:1.280-1   perl-Time-Piece 1.31-422   perl-TimeDate 1:2.30-15.module+el8.6   perl-URI 1.73-3   perl-Unicode-Collate 1.25-2   perl-Unicode-Normalize 1.25-396   perl-XML-Parser 2.44-11   perl-XML-XPath 1.42-3   perl-autodie 2.29-396   perl-bignum 0.49-2   perl-constant 1.33-396   perl-devel 4:5.26.3-422   perl-encoding 4:2.22-3   perl-experimental 0.019-2   perl-inc-latest 2:0.500-9   perl-interpreter 4:5.26.3-422   perl-libnet 3.11-3   perl-libnetcfg 4:5.26.3-422   perl-libs 4:5.26.3-422   perl-local-lib 2.000024-2   perl-macros 4:5.26.3-422   perl-open 1.11-422   perl-parent 1:0.237-1   perl-perlfaq 5.20180605-1   perl-podlators 4.11-1   perl-srpm-macros 1-25   perl-threads 1:2.21-2   perl-threads-shared 1.58-2   perl-utils 5.26.3-422   perl-version 6:0.99.24-1   pigz 2.4-4   pinentry 1.1.0-2   pinentry-gtk 1.1.0-2   pipewire 0.3.6-1   pipewire-libs 0.3.6-1   pixman 0.38.4-2   pixman-devel 0.38.4-2   pkgconf 1.4.2-1   pkgconf-m4 1.4.2-1   pkgconf-pkg-config 1.4.2-1   platform-python 3.6.8-51.el8_8.1.rocky   platform-python-devel 3.6.8-51.el8_8.1.rocky   platform-python-pip 9.0.3-22.el8.rocky   platform-python-setuptools 39.2.0-7   plymouth 0.9.4-11.20200615git1e36e30   plymouth-core-libs 0.9.4-11.20200615git1e36e30   plymouth-scripts 0.9.4-11.20200615git1e36e30   policycoreutils 2.9-24   policycoreutils-python-utils 2.9-24   polkit 0.115-15   polkit-libs 0.115-15   polkit-pkla-compat 0.1-12   poppler 20.11.0-6   poppler-data 0.4.9-1   poppler-glib 20.11.0-6   popt 1.18-1   prefixdevname 0.1.0-6   procps-ng 3.3.15-13   proj 6.3.2-4   proj-datumgrid 1.8-6.3.2.4   proj-devel 6.3.2-4   protobuf 3.5.0-15   protobuf-c 1.3.0-6   protobuf-compiler 3.5.0-15   protobuf-devel 3.5.0-15   protobuf-lite 3.5.0-15   protobuf-lite-devel 3.5.0-15   psmisc 23.1-5   publicsuffix-list-dafsa 20180723-1   pulseaudio 14.0-4   pulseaudio-libs 14.0-4   pulseaudio-libs-glib2 14.0-4   pulseaudio-module-bluetooth 14.0-4   python-rpm-macros 3-45   python-srpm-macros 3-45   python2 2.7.18-13.module+el8.8.0+1314+be03569e.1.rocky.0   python2-libs 2.7.18-13.module+el8.8.0+1314+be03569e.1.rocky.0   python2-pip 9.0.3-19.module+el8.6   python2-pip-wheel 9.0.3-19.module+el8.6   python2-setuptools 39.0.1-13.module+el8.4   python2-setuptools-wheel 39.0.1-13.module+el8.4   python3-audit 3.0.7-4   python3-bind 32:9.11.36-8.el8_8   python3-cairo 1.16.3-6   python3-chardet 3.0.4-7   python3-cups 1.9.72-21.el8   python3-dateutil 1:2.6.1-6   python3-dbus 1.2.4-15   python3-decorator 4.2.1-2   python3-dnf 4.7.0-16   python3-dnf-plugins-core 4.0.21-19   python3-firewall 0.9.3-13   python3-gobject 3.28.3-2   python3-gobject-base 3.28.3-2   python3-gpg 1.13.1-11   python3-hawkey 0.63.0-14   python3-idna 2.5-5   python3-libcomps 0.1.18-1   python3-libdnf 0.63.0-14   python3-libs 3.6.8-51.el8_8.1.rocky   python3-libselinux 2.9-8   python3-libsemanage 2.9-9   python3-linux-procfs 0.7.1-1   python3-nftables 1:0.9.3-26   python3-perf 4.18.0-477.21.1   python3-pip 9.0.3-22.el8.rocky   python3-pip-wheel 9.0.3-22.el8.rocky   python3-ply 3.9-9   python3-policycoreutils 2.9-24   python3-pycurl 7.43.0.2-4   python3-pyparsing 2.1.10-7   python3-pysocks 1.6.8-3   python3-pyudev 0.21.0-7   python3-pyyaml 3.12-12   python3-requests 2.20.0-2.1   python3-rpm 4.14.3-26   python3-rpm-generators 5-8   python3-rpm-macros 3-45   python3-setools 4.3.0-3   python3-setuptools 39.2.0-7   python3-setuptools-wheel 39.2.0-7   python3-six 1.11.0-8   python3-slip 0.6.4-13   python3-slip-dbus 0.6.4-13   python3-sssdconfig 2.8.2-3   python3-syspurpose 1.28.36-2   python3-systemd 234-8   python3-unbound 1.16.2-5   python3-urllib3 1.24.2-5   python36 3.6.8-38.module+el8.5   python36-devel 3.6.8-38.module+el8.5   qperf 0.4.11-1   qt5-qtbase 5.15.3-1   qt5-qtbase-common 5.15.3-1   qt5-qtbase-gui 5.15.3-1   qt5-qtsvg 5.15.3-1   qt5-srpm-macros 5.15.3-1   quota 1:4.04-14   quota-nls 1:4.04-14   readline 7.0-10   readline-devel 7.0-10   redhat-rpm-config 131-1   rest 0.8.1-2   rocky-backgrounds 86.3-1   rocky-gpg-keys 8.8-1.8   rocky-logos 86.3-1   rocky-release 8.8-1.8   rocky-repos 8.8-1.8   rootfiles 8.1-22   rpcbind 1.2.5-10   rpm 4.14.3-26   rpm-build 4.14.3-26   rpm-build-libs 4.14.3-26   rpm-libs 4.14.3-26   rpm-plugin-selinux 4.14.3-26   rpm-plugin-systemd-inhibit 4.14.3-26   rsync 3.1.3-19.el8_7   rsyslog 8.2102.0-13   rtkit 0.11-19   rust-srpm-macros 5-2   samba-client-libs 4.17.5-3   samba-common 4.17.5-3   samba-common-libs 4.17.5-3   sbc 1.3-9   scl-utils 1:2.0.2-16   screen 4.6.2-12   sed 4.5-5   selinux-policy 3.14.3-117.el8_8   selinux-policy-targeted 3.14.3-117.el8_8   setup 2.12.2-9   sg3_utils 1.44-6   sg3_utils-libs 1.44-6   shadow-utils 2:4.6-17   shapelib 1.5.0-12   shared-mime-info 1.9-3   sharp 2.1.2.MLNX20200428.ddda184-1   slang 2.3.2-3   smem 1.5-6   snappy 1.1.8-3   sound-theme-freedesktop 0.8-9   soundtouch 2.0.0-3   source-highlight 3.1.8-17   speex 1.2.0-1   speexdsp 1.2-0.13.rc3   sqlite 3.26.0-18   sqlite-devel 3.26.0-18   sqlite-libs 3.26.0-18   squashfs-tools 4.3-20   srp 4.9-OFED.4.9.7.1.0.1.kver.4.18.0_477.21.1.el8_8   srptools 41mlnx1-5   sssd 2.8.2-3   sssd-ad 2.8.2-3   sssd-client 2.8.2-3   sssd-common 2.8.2-3   sssd-common-pac 2.8.2-3   sssd-ipa 2.8.2-3   sssd-kcm 2.8.2-3   sssd-krb5 2.8.2-3   sssd-krb5-common 2.8.2-3   sssd-ldap 2.8.2-3   sssd-nfs-idmap 2.8.2-3   sssd-proxy 2.8.2-3   startup-notification 0.12-15   subversion 1.10.2-5.module+el8.7   subversion-libs 1.10.2-5.module+el8.7   sudo 1.8.29-10   switcheroo-control 1.1-5   system-config-printer-libs 1.5.11-13   systemd 239-74.el8_8   systemd-libs 239-74.el8_8   systemd-pam 239-74.el8_8   systemd-udev 239-74.el8_8   systemtap-sdt-devel 4.8-2   taglib 1.11.1-8   tar 2:1.30-9   tbb 2018.2-9   tcl 1:8.6.8-2   tcl-devel 1:8.6.8-2   tcsh 6.20.00-15   teamd 1.31-4   texlive-base 7:20180414-29   texlive-kpathsea 7:20180414-29   texlive-lib 7:20180414-29   texlive-tetex 7:20180414-29   texlive-texlive 7:20180414-29   tigervnc-license 1.12.0-15   tigervnc-selinux 1.12.0-15   tigervnc-server 1.12.0-15   tigervnc-server-minimal 1.12.0-15   time 1.9-3   timedatex 0.5-3   tk 1:8.6.8-1   tk-devel 1:8.6.8-1   totem-pl-parser 3.26.1-2   tpm2-tss 2.3.2-4   tracker 2.1.5-2   tracker-miners 2.1.5-1   tree 1.7.0-15   trousers 0.3.15-1   trousers-lib 0.3.15-1   tuned 2.20.0-1   twolame-libs 0.3.13-12   tzdata 2023c-1   ucx 1.8.0-1   ucx-cma 1.8.0-1   ucx-devel 1.8.0-1   ucx-ib 1.8.0-1   ucx-ib-cm 1.8.0-1   ucx-knem 1.8.0-1   ucx-rdmacm 1.8.0-1   udisks2 2.9.0-13   udunits2 2.2.26-5   udunits2-devel 2.2.26-5   unbound-libs 1.16.2-5   unixODBC 2.3.7-1   unzip 6.0-46   upower 0.99.7-4   uriparser 0.9.7-1   urw-base35-bookman-fonts 20170801-10   urw-base35-c059-fonts 20170801-10   urw-base35-d050000l-fonts 20170801-10   urw-base35-fonts 20170801-10   urw-base35-fonts-common 20170801-10   urw-base35-gothic-fonts 20170801-10   urw-base35-nimbus-mono-ps-fonts 20170801-10   urw-base35-nimbus-roman-fonts 20170801-10   urw-base35-nimbus-sans-fonts 20170801-10   urw-base35-p052-fonts 20170801-10   urw-base35-standard-symbols-ps-fonts 20170801-10   urw-base35-z003-fonts 20170801-10   usbutils 010-3   utf8proc 2.6.1-3.module+el8.7   util-linux 2.32.1-42   uuid 1.6.2-43   vim-common 2:8.0.1763-19.el8_6   vim-enhanced 2:8.0.1763-19.el8_6   vim-filesystem 2:8.0.1763-19.el8_6   vim-minimal 2:8.0.1763-19.el8_6   vino 3.22.0-11   virt-what 1.25-3   volume_key-libs 0.3.11-5   vte-profile 0.52.4-2   vte291 0.52.4-2   wavpack 5.1.0-16   webkit2gtk3 2.38.5-1.el8_8   webkit2gtk3-jsc 2.38.5-1.el8_8   webrtc-audio-processing 0.3-10   wget 1.19.5-11   which 2.21-18   woff2 1.0.2-5   wpa_supplicant 1:2.10-1   wpebackend-fdo 1.10.0-3   xcb-util 0.4.0-10   xcb-util-image 0.4.0-9   xcb-util-keysyms 0.4.0-7   xcb-util-renderutil 0.3.9-10   xcb-util-wm 0.4.1-12   xdg-desktop-portal 1.8.1-1   xdg-desktop-portal-gtk 1.8.0-1   xdg-utils 1.1.2-5   xerces-c 3.2.3-5   xfsprogs 5.0.0-11   xkeyboard-config 2.28-1   xml-common 0.6.3-50   xorg-x11-apps 7.7-21   xorg-x11-drv-fbdev 0.5.0-2   xorg-x11-drv-libinput 0.29.0-1   xorg-x11-drv-vesa 2.4.0-3   xorg-x11-font-utils 1:7.5-41   xorg-x11-fonts-ISO8859-1-100dpi 7.5-19   xorg-x11-fonts-misc 7.5-19   xorg-x11-proto-devel 2020.1-3   xorg-x11-server-Xorg 1.20.11-15   xorg-x11-server-Xvfb 1.20.11-15   xorg-x11-server-Xwayland 21.1.3-10   xorg-x11-server-common 1.20.11-15   xorg-x11-server-utils 7.7-27   xorg-x11-xauth 1:1.0.9-12   xorg-x11-xbitmaps 1.1.1-13   xorg-x11-xinit 1.3.4-18   xorg-x11-xkb-utils 7.7-28   xterm 331-1.el8_3   xterm-resize 331-1.el8_3   xz 5.2.4-4   xz-devel 5.2.4-4   xz-libs 5.2.4-4   yasm 1.3.0-7   yum 4.7.0-16   zenity 3.28.1-2   zip 3.0-23   zlib 1.2.11-21   zlib-devel 1.2.11-21   zsh 5.5.1-10   zstd 1.4.4-1    ","excerpt":"The following packages are provided as part of the CentOS 8 operating …","ref":"/about/software/system/","title":"System Software"},{"body":"Summary report  Click image to enlarge!\n   -- ","excerpt":"Summary report  Click image to enlarge!\n   -- ","ref":"/about/overview/activity/","title":"Activity Report"},{"body":"The following commercial software is available only for UCR accounts on the cluster.\n   Package Name Version     Gaussian 16   Mathematica 12.3.1   MATLAB R2021b    Please contact us at support@hpcc.ucr.edu to be added to the corresponding group in order to gain access to the software via our module system.\n","excerpt":"The following commercial software is available only for UCR accounts …","ref":"/about/software/commercial/","title":"Commercial Software"},{"body":"Overview In Linux (and Unix systems in general), access to files and directories is controlled by a system of owners, groups, and permission bits. Changing these settings is necessary to control access by other users. The permission system also affects what files can be executed.\nOwnership Levels  user (u) - User ownership of a file/directory. This user has the special right to change the permission bits and group ownership. group (g) - Group ownership of a file/directory. Members of this group may be assigned greater access rights than non-members. other (o) - Everyone else that isn’t the owning user or from the owning group.  Permission Bits The elemental permissions in Linux/Unix are read, write, and execute. Users and groups can have one many, or none of these rights. Their meanings are as follows:\n    Letter Number File Directory     Read r 4 View the contents View the listings   Write w 2 Modify the contents Create a new file, or rename or delete existing files   Execute x 1 Execute a program/script Traversal rights    Checking Permissions Annotated output for ls -la:\n---------- File type (d = directory, - = regular file, l = symlink) |--------- User permission triplet || ------ Group permission triplet || | --- Other permission triplet || | | || | | [user] [group] drwx-----x 61 username groupname 4096 Feb 24 16:39 ./ drwxr-xr-x 688 root root 262144 Feb 24 11:05 ../ drwx------ 2 username groupname 4096 Feb 2 22:45 .ssh/ drwxr-xr-x 5 username groupname 4096 Dec 12 15:57 Downloads/ drwxr-xr-x 2 username groupname 4096 Jan 9 16:29 bin/ -rw------- 1 username groupname 7960 Feb 23 18:37 .bash_history -rw-r--r-- 1 username groupname 306 Nov 3 15:08 .bashrc -rw-r--r-- 1 username groupname 677 Apr 8 2013 .profile -rw-r--r-- 1 username groupname 128 Nov 30 12:38 .tmux.conf -rw-r--r-- 1 username groupname 12126 Nov 2 13:14 .vimrc lrwxrwxrwx 1 username groupname 23 Sep 12 10:49 bigdata -\u003e /bigdata/groupname/username/ -rw-r--r-- 1 username groupname 5657 Sep 19 11:31 bookmarks.html lrwxrwxrwx 1 username groupname 23 Sep 12 10:49 shared -\u003e /bigdata/groupname/shared/  Assign write and execute permissions to user and group\nchmod ug+rx my_file\nTo remove all permissions from all three user groups\nchmod ugo-rwx my_file # '+' causes the permissions selected to be added # '-' causes them to be removed # '=' causes them to be the only permissions that the file has. chmod +rx public_html/ or $ chmod 755 public_html/ # Example for number system:  Change ownership chown \u003cuser\u003e \u003cfile or dir\u003e # changes user ownership chgrp \u003cgroup\u003e \u003cfile or dir\u003e # changes group ownership chown \u003cuser\u003e:\u003cgroup\u003e \u003cfile or dir\u003e # changes user \u0026 group ownership  ","excerpt":"Overview In Linux (and Unix systems in general), access to files and …","ref":"/manuals/linux_basics/permissions/","title":"Permissions and Ownership"},{"body":"Start Times Start times are a great way to track your jobs:\nsqueue -u $USER --start  Start times are rough estimates based on the current state of the queue.\nPartition Quotas Each partition has a specific usecase. Below outlines each partition, it’s usecase, as well as any job/user/group limits that are in place. Empty boxes imply no limit, but is still limited by the next higher limit. Job limits are capped by user limits, and user limits are capped by group limits.\n   Partition Name Usecase Per-User Limit Per-Job Limit Max Job Time     epyc (2021 CPU) CPU Intensive Workloads, Multithreaded, MPI, OpenMP 384 Cores, 1TB memory 64GB memory per Core 1,2 30 Days   intel (2016 CPU) CPU Intensive Workloads, Multithreaded, MPI, OpenMP 384 Cores, 1TB memory 64GB memory per Core 1,3 30 Days   batch (2012 CPU) CPU Intensive Workloads, Multithreaded, MPI, OpenMP 384 Cores, 1TB memory 64GB memory per Core 1,4 30 Days   short Short CPU Intensive Workloads, Multithreaded, MPI, OpenMP 384 Cores, 1TB memory 64GB memory per Core, 2-hour time limit 2 Hours   highmem Memory Intensive Workloads 32 Cores, 1TB memory  30 Days   gpu GPU-Enabled Workloads 4 GPUs5,48 Cores, 512GB memory 16 Cores, 256GB memory 1,6 7 Days    In addition to the above limits, there is:\n A 768 core group limit that spans across all users in a group across all partitions. A 8 GPU group limit that spans across all users in a group across all GPU-enabled partitions.  Attempting to allocate more member than a node can support, eg 500GB on an Intel node, will cause the job to immediately fail. Limits are for actively running jobs, and any newly queued job that exceeds a limit will be queued until resources become available. If you require additional resourced beyond the listed limits, please see the “Additional Resource Request” section below.\nPartition quotas can also be viewed on the cluster using the slurm_limits command.\nAdditionally, users can have up to 5000 jobs in queue/running at the same time. Attempting to queue more than 5000 jobs will cause jobs submissions to fail with the reason “MaxSubmitJobLimit”.\nExternal Labs Labs external to UCR will have reduced resource limits as follows:\n Labs will have a CPU quota of 256 cores across all lab users across all partitions Per user CPU quotas on epyc, intel, batch, and short will be 128 cores Per user CPU quotas on highmem will be 16 GPU quotas on the gpu partition will be 4 per-lab, and 2 per-user CPU quotas on the gpu partition will be 24 per-user and 8 per-job  Private Node Ownership Labs have the ability to purchase nodes and connect them to the cluster for increased quotas. More information can be found in the Ownership Model section of our Access page.\nAdditional Resource Request Sometimes, whether it be due to deadlines or technical limitations, more resources might be needed than are supplied by default. If you require a temporary increase in quotas, please reach out to support@hpcc.ucr.edu with a short justification as to why additional resources are needed. The following are typical circumstances that could justify increased quotas:\n Urgent Deadlines: ie. Grant submissions, conference presentations, paper deadlines Special Technical Needs: The limits do not meet the technical requirements for the program(s) that are trying to be ran.  The amount of additional resources, the length of time that the resources are needed, and the frequency of the requests are all factors that determine whether your request will be accepted. It also must be within the capacity of the HPCC’s infrastructure while also ensuring minimal disruption to other users. The final decision of approving exception requests, and how many extra resources to provide, will be decided by the HPCC Staff, the Director, and in exceptional cases the HPCC Oversight Committee.\nRequests limited by unoptimized code/datasets or strictly for the sake of convenience will be denied.\nAdditionally at this time we are unable to grant additional resource requests for external labs due to how our cluster is partially subsidized by our campus. We appologize for this, and suggest looking into national computing facilities or cloud offerings to fill the gap in compute.\nExample Scenarios Per-Job Limit A job is submitted on the gpu partition. The job requests 32 cores.\n This job will not be able to be submitted, as 32 cores is above the partition’s 16 core per-job limit.\n Per-User Limit You submit a job the highmem partition, requesting 32 cores.\n This job will start successfully, as it is within the partition’s core limit.\n You submit a second job while the first job is still running. The new job is requesting 32 cores.\n Because you are at your per-user core limit on the highmem partition, the second job will be queued until the first job finishes.\n Per-Lab Limit User A submits a job requesting 384 cores. User B submits a job requesting 384 cores.\n Because each user is within their per-user limits and the lab is within their limit, the jobs will run in parallel.\n User C submits a job, requesting 16 cores.\n Because User A and User B are using all 768 cores within the lab, User C’s job will be queued until either User A’s or User B’s jobs finishes.\n Changing Partitions In srun commands and sbatch scripts, the -p or --partition flag controls which partition/queue a job will run on. For example, using -p epyc will have your job queued and ran on the epyc partition. For more examples and information on running jobs, see the Managing Jobs page of our documentation.\nFair-Share Users that have not submitted any jobs in a long time usually have a higher priority over others that have ran jobs recently. Thus the estimated start times can be extended to allow everyone their fair share of the system. This prevents a few large groups from dominating the queuing system for long periods of time.\nYou can see with the sqmore command what priority your job has (list is sorted from lowest to highest priority). You can also check to see how your group’s priority is compared to other groups on the cluster with the “sshare” command.\nFor example:\nsshare  It may also be useful to see your entire group’s fairshare score and who has used the most shares:\nsshare -A $GROUP --all  Lastley, if you only want to see your own fairshare score:\nsshare -A $GROUP -u $USER  The fairshare score is a number between 0 and 1. The best score being 1, and the worst being 0. The fairshare score approches zero the more resource you (or your group) consume. Your individual consumption of resources (usage) does affect your entire group’s fiarshare score. The affects of your running/completed jobs on your fairshare score are halved each day (half-life). Thus, after waiting several days without running any jobs, you should see an improvment in your fairshare score.\nHere is a very good explaination of fairshare.\nPriority The fairshare score and jobs queue wait time is used to calculate your job’s priority. You can use the sprio command to check the priority of your jobs:\nsprio -u $USER  Even if your group has a lower fairshare score, your job may still have a very high priority. This would be likely due to the job’s queue wait time, and it should start as soon as possible regardless of fairshare score. You can use the sqmore command to see a list of all jobs sorted by priority.\nBackfill Some small jobs may start before yours, only if they can complete before yours starts and thus not negatively affecting your start time.\nPriority Partition Some groups on our system have purchased additional hardware. These nodes will not be affected by the fairshare score. This is because jobs submitted to the group’s partition will be evaluated first before any other jobs that have been submitted to those nodes from a different partition.\nUsing the Preempt Partitions NOTE The full release of the preempt partition is planned for future release and is not yet available!\nThis guide assumes that you know how to run Interactive and Batch jobs through Slurm. If you do not, then please see the Managing Jobs page of our documentation.\nThere are two partitions that will have preemption enabled: “preempt” for CPU jobs, and “preempt_gpu” for GPU jobs.\nTo fully take advantage of preemption, your jobs must be be able to tolerate being cancelled at a random time and restarted at some later point in the future. When your job is preempted, it will be cancelled and requeued. When the job is elegible to start again, it will start from the beginning of the sbatch script as if it were newly run.\nYour job is only guaranteed 5 minutes of runtime when it starts before it is elegible to be preempted.\nJob Limitations Time As mentioned above, jobs can be killed at any time after the 5 minute grace period. Jobs should be set up such that any initialization steps that cannot tolerate being randomly killed happen within those first 5 minutes. The max walltime of a job is currently set to 1 day (24 hours).\nResources Currently, users are allowed to use an equal number of CPU cores as their current per-partition CPU limit. If you’re currently allowed to use 384 cores on the epyc partition, then you can use 384 cores on the preempt partition. The same applies to memory. For the GPU partition, users are currently allowed to use 1 GPU on the “preempt_gpu” partition.\nStarting a job Similar to other partitions, you must specifically queue jobs to the preempt partition. One special thing that is required is to also specify the preempt account using -A preempt. Jobs started on the preempt partition do not count against your lab’s CPU quota.\nInteractive Example To start a CPU preemptable interactive job, you can build off of the following command:\nsrun -A preempt -p preempt -c 8 --mem 8GB --pty bash -l  This will start a job with 8 cores and 8GB of memory on the preempt partition under the preempt account. Jobs that do not explicitly state -A preempt will fail to start. Note that because this is a preemptable job, your session can be terminated at any moment without notice after the 5 minute grace period.\nTo start a GPU preemptable interactive job, you can build off of the following command:\nsrun -A preempt -p preempt_gpu --gres=gpu:1 -c 8 --mem 8GB --pty bash -l  Non-interactive (batch) Example As with all preemptable jobs, batch jobs can be cancelled at any time without notice and the programs must be able to tolerate this. Jobs that have been preempted will automatically be requeued to resume running at a later time when resources become available. The $SLURM_RESTART_COUNT environment variable can be used to check if the job has been preempted and restarted to allow you to recover and resume running.\nTo start a batch job, you can build off of the following sbatch file:\n#!/bin/bash -l #SBATCH -A preempt #SBATCH -p preempt #SBATCH -c 8 #SBATCH --mem 8GB #SBATCH --time 1-00:00:00 # Check if this is the first run or a resumed job if [ \"$SLURM_RESTART_COUNT\" -eq 0 ]; then echo \"This is the first time running the job\" # Put the code for the first run here # Example: initializing data or setting up environment # Remember that a job only has 5 minutes of guaranteed runtime. Keep # any initialization/recovery short otherwise it might be interrupted else echo \"The job is being resumed after a preemption\" # Put the code for a resumed job here # Example: resuming from a checkpoint or continuing work # Remember that a job only has 5 minutes of guaranteed runtime. Keep # any initialization/recovery short otherwise it might be interrupted fi # Common job code that runs regardless of first run or resume echo \"Running main job tasks...\" # Put your main job code here  Jobs that do not explicitly state #SBATCH -A preempt will fail to start. Note that because this is a preemptable job, your job can be cancelled at any moment without notice.\nSelecting Resources Similar to the Short partition, the Preempt partition is a union of all public and private machines, excluding specialty machines like highmem and GPU. This means that if you do not specify any restrictions, your job can run on nodes in the batch, intel, or epyc partition. If a certain architecture is required for your job, then you can use the --constraint flag.\nFor example, if you want your job to run on an Intel machine, you can include #SBATCH --constraint=intel in your sbatch script, or --constraint=intel in your srun command. If you want either an Intel or Epyc Rome machine, then you could use #SBATCH --constraint=intel|rome in your sbatch script, or constraint=intel|rome in your srun command. More information on constraints is available in the Slurm Documentation.\nTo view which nodes contain which features, see the Feature Constraints listed on the Feature Constraints page\n  A 64GB-per-core limit is placed to prevent over allocating memory compared to CPUs. If more than a 64GB-per-core ratio is requested, the core count will be increased to match. ↩︎\n Allocatable memory per-node in the epyc partition is limited to ~950GB to allow for system overhead. ↩︎\n Allocatable memory per-node in the intel partition is limited to ~450GB to allow for system overhead. ↩︎\n Allocatable memory per-node in the batch partition is limited to ~500GB to allow for system overhead. ↩︎\n If a user needs more than 4 GPUs, please contact support@hpcc.ucr.edu with a short justification for a temporary increase. ↩︎\n Allocatable memory per-node in the gpu partition is dependent on the node. 115GB for gpu[01-02], 500GB for gpu[03-04], 200GB for gpu05, 922GB for gpu06, 950GB for gpu[07-08] ↩︎\n   ","excerpt":"Start Times Start times are a great way to track your jobs:\nsqueue -u …","ref":"/manuals/hpc_cluster/queue/","title":"Queue Policies"},{"body":"SSH Keys on Linux How to create SSH keys on LInux OSs or the command-line in general is described on the general login page here.\n","excerpt":"SSH Keys on Linux How to create SSH keys on LInux OSs or the …","ref":"/manuals/hpc_cluster/sshkeys/sshkeys_linux/","title":"SSH Keys Linux"},{"body":"Find Files find ~ -name \"*pattern*\" # Searches for *pattern* in and below your home directory find ~ -iname \"*pattern*\" # Same as above, but case insensitive find ~ -type f -mtime -2 # Searches for files you have modified in the last two days  Useful find arguments:\n -user \u003cuserName\u003e -group \u003cgroupName\u003e -ctime \u003cnumber of days ago changed\u003e -exec \u003ccommand to run on each file\u003e {} \\;  Find Text grep \"pattern\" \u003cFILENAME\u003e # Provides lines in a file where \"pattern\" appears grep -H \"pattern\" # -H prints out file name in front of pattern find ~ -name \"*.txt\" -exec grep -H \"pattern\" {} \\; # Search lines where \"pattern\" appears in files with names that end with \".txt\"  Find Applications which \u003cAPPLICATION_NAME\u003e # Location of application whereis \u003cAPPLICATION_NAME\u003e # Searches for executables in set of directories rpm -qa | grep \"pattern\" # List all RPM packages and filter based on \"pattern\"  ","excerpt":"Find Files find ~ -name \"*pattern*\" # Searches for *pattern* in and …","ref":"/manuals/linux_basics/finding_things/","title":"Finding Things"},{"body":"Python The scope of this manual is a brief introduction on how to manage Python packages.\nPython Versions Different Python versions do not play nice with each other. It is best to only load one Python module at any given time. The miniconda3 module for Python is the default version. This will enable users to leverage the conda installer, but with as few Python packages pre-installed as possible. This is to avoid conflicts with future needs of individuals.\nConda We have several Conda software modules:\n miniconda3 - Basic Python 3 install (Default) anaconda - Full Python 3 install For more information regarding our module system please refer to Environment Modules.  The miniconda modules are very basic installs, however users can choose to unload this basic install for a fuller one (anaconda), like so:\nmodule load miniconda3  After loading anaconda, you will see that there are many more Python packages installed (ie. numpy, scipy, pandas, jupyter, etc…). For a list of installed Python packages try the following:\npip list  Virtual Environments Sometimes it is best to create your own environment in which you have full control over package installs. Conda allows you to do this through virtual environments.\nInitialize Conda will now auto initialize when you load the corresponding module. No need to run the conda init or make any modifications to your ~/.bashrc file.\nConfigure Installing many packages can consume a large (ie. \u003e20GB) amount of disk space, thus it is recommended to store conda environments under your bigdata space. If you have bigdata, create the .condarc file (otherwise conda environments will be created under your home directory).\nCreate the file .condarc in your home, with the following content:\nchannels: - defaults pkgs_dirs: - ~/bigdata/.conda/pkgs envs_dirs: - ~/bigdata/.conda/envs auto_activate_base: false   After changing the configuration, environments can be moved to the new bigdata location using conda rename -n NAME NAME_tmp, then conda rename -n NAME_tmp NAME to return it to it’s original name. Replacing NAME with the name of the environment you wish to move. If you receive an error while trying to rename, try activting the base conda environment using conda activate base and running the conda rename commands again.\n Create a Python 3.10 conda environment, like so:\nmodule load miniconda3 # Should already be auto-loaded during login conda create -n NameForNewEnv python=3.10 # Many Python versions are available  Activating Once your virtual environment has been created, you need to activate it before you can use it:\nconda activate NameForNewEnv  With more modules being added as conda environments, it’s sometimes requried to “stack” user environments on top of module-provided environments. Running conda activate will deactivate the current environment before activating the new environment.. To counter this, the --stack flag can be used to effectively “combine” environments. For example conda activate --stack NameForNewEnv. Please see the conda page on Nested Activation for more details.\nDeactivating In order to exit from your virtual environment, do the following:\nconda deactivate  Installing packages Before installing your packages, make sure you are on a computer node. This ensures your downloads to be done quickly and with less chance of running out of memory. This can be done using the following command:\nsrun -p short -c 4 --mem=10g --pty bash -l # Adjust the resource request as needed  Here is a simple example for installing packages under your Python virtual environment via conda:\nconda install -n NameForNewEnv PackageName  You may need to enable an additional channel to install the package (refer to your package’s documentation):\nconda install -n NameForNewEnv -c ChannelName PackageName  Cloning It is possible for you to copy an existing environment into a new environment:\nconda create --name AnotherNameForNewEnv --clone NameForNewEnv  Listing Environments Run the following to get a list of currently installed conda evironments:\nconda env list  Removing If you wish to remove a conda environment run the following:\nconda env remove --name myenv  More Info For more information regarding conda please visit Conda Docs.\nJupyter You can run jupyter as an interactive job or you can use the web instance, see Jupyter Usage for details.\nVirtual Environments (Kernels) In order to use a custom Python/Conda virtual environment within Jupyter, it must be configured as a kernel. You will need to do the following:\n# Create a virtual environment named \"ipykernel_py3\", if you don't already have one # It can be named whatever you like, \"ipykernel_py3\" is just an example. # You can also indicate a more specific version of Python here. Otherwise you'll get # the latest version provided by Anaconda. conda create -n ipykernel_py3 python=3 ipykernel # Load the new environment conda activate ipykernel_py3 # Install kernel # --name is used to define the internal name used by Jupyter, and should not contain spaces. # --display-name is the name you will see in the Jupyter web interface, should be descriptive. python -m ipykernel install --user --name ipykernel_py3 --display-name \"IPyKernel (Python 3)\"  Now when you visit Jupyter-Hub you should see the option “JupyterPy3” when you click the “New” dropdown menu in the upper left corner of the home page.\nTo remove an unwanted kernel, use the following commands:\njupyter kernelspec list # List available kernels jupyter kernelspec uninstall UNWANTEDKERNEL   Replace UNWANTEDKERNEL with the name of the kernel you wish to remove\n Further reading: Installing the IPython kernel\nR For instructions on how to configure your R environment please visit IRkernel. Since we should already have IRkernel install in the latest version of R, you would only need to do the following within R:\nIRkernel::installspec(name = 'ir44', displayname = 'R 4.0.1')  R This section is regarding how to manage R packages.\nCurrent R Version  NOTE: Please be aware that this version of R is built with GCC/8.3.0, which means that previously compiled modules may be incompatible.\n Currently the default version of R is R/4.3.0 and is loaded automatically for you.\nWhen a new release of R is available, you should reinstall any local R packages, however keep in mind of the following:\n Remove redundantly installed local R packages with the RdupCheck command. Newer version of R packages are not backward compatible, once installed they only work for that specific version of R.  Older R Versions You can load other versions of R with the following:\nmodule unload R module avail R module load R/VERSION  Installing R Packages The default version of R has many of the most popular R packages already installed and available. It is also possible for you to install additional R packages in your local environment.\nOnly install packages if they are not already available, this will minimize issues later. You can check the current version of R from the command line, like so:\nRscript -e \"library('some-package-name')\"  Or you can check from within R, like so:\nlibrary('some-package-name')  If the package is not available, then proceed with installation.\nBioconductor Packages To install from Bioconductor you can use the following method:\nBiocManager::install(c(\"package-to-install\", \"another-packages-to-install\")) Update all/some/none? [a/s/n]: n  For more information please visit Bioconductor Install Page.\nGitHub Packages # Load devtools library(devtools) # Replace name with the GitHub account/repo install_github(\"duncantl/RGoogleDocs\")  Local Packages # Replace URL with your URL or local path to your .tar.gz file install.packages(\"http://hartleys.github.io/QoRTs/QoRTs_LATEST.tar.gz\",repos=NULL,type=\"source\")  ","excerpt":"Python The scope of this manual is a brief introduction on how to …","ref":"/manuals/hpc_cluster/package_manage/","title":"Package Management"},{"body":"Facility description  Facility description (e.g. for grant applications)  Recharging rates  Recharging rates: 2024/2025 Recharging rates: 2023/2024 Recharging rates: 2022/2023 Recharging rates: 2021/2022 Recharging rates: 2020/2021 Recharging rates: 2019/2020 Recharging rates: 2018/2019 Recharging rates: 2017/2018 Recharging rates: 2016/2017  Purchased Disk Information To achieve our desired balance of performance and density in our storage systems we must place some restrictions on the lifetime of purchased storage. Details can be found in our Owned Storage Information document\nOwnership Models For more information relating to owned storage, see the Ownership models section of the Access page.\nExternal user accounts Accounts for external customers can only be granted if a lab has a strong affiliation with UC Riverside, such as a research collaboration with UCR researchers. Both the corresponding UCR PI and external collaborator need to maintain an HPCC subscription. External accounts are subject to an annual review and approval process. To be approved, the external and internal PIs have to complete this External Usage Justification.\n","excerpt":"Facility description  Facility description (e.g. for grant …","ref":"/about/overview/rates/","title":"Rates"},{"body":"Introduction This page will talk about AWS costs and billing along with setting some controls for them.\nCosts AWS Costs come from each and every type of resources you consume on AWS.\nSome Examples:\n Node Utilization (How many computers and of what type you use for how long) Storage Backend services such as network traffic, security groups, and API transactions. (These costs are relatively very small)  Node Utilization and Storage are the two biggest contributors to cost. So when controlling costs focus here. This can be done by only uploading and downloading the data you need, and deleting your cluster when your not using it. There are also some saving to be realized by choosing the best type of compute node for the type of work you plan on doing. It is easy to change the compute type and HPCC can help you to make the best decision for your work. You can also simply use HPCCs default configuration.\nAs a practical example: (~$0.43 cents per hour to run in default configuration)\n Using the most common cluster configuration - One headnode and one 8 core compute node with 16G Ram. Without using spot pricing. Assuming 25GB of input and/or output data.  HPCC can help with cost projection if you need it.\nAWS Costs Calculator Here is a link to the AWS Costs Calculator .\nThis calculator is very comprehensive and can be difficult to navigate at first. (If you have questions you can ask HPCC)\nBilling Billing is done my Amazon on a monthly basis and is calculated from the previous months usage. This billing is done via PO associated with a FAU.\nAWS Billing \u0026 Cost Management Dashboard is a important webpage to have bookmarked. It will be your main interface for the billing aspects AWS.\nBilling access is allowed for the main AWS account only. If you have sub accounts for lab members all their activity is accumulated and reported back through the main aws account.\nBudgets Controls/Alerts It is possable to configure budget alerts. This allows you to get notified if you are spending or are projected to spend more that you would like.\nThere are many different options to customize.\nBasic and practical control example: This will alert you if you are projected to spend more than $50 in a givien month so you can take action on it.\nBilling Dashboard  Navigate to the AWS Billing \u0026 Cost Management Dashboard All aspects of billing are controlled from here  Click Budget from the left  Budgets  This is the budget control page  Click Create Budget  Define budget and alerts  This is the budget definition page  Fill out the Name field (what ever you would like to call it) Fill out the Period (Monthly is preffered) Fill out the Budgeted Amount (Limit we are interested in, in this example its $50) The “Refine your Budget” section can be left as is Fill out the “Notify me when” fields as shown Fill out your main email address in the “Email contacts” section. Click Create  Budget created  We are back to the budget page  Notice the new budget defined You will now be notified it your speding is projected to exceed your budget Complete  ","excerpt":"Introduction This page will talk about AWS costs and billing along …","ref":"/manuals/ext_cloud/aws/billing/","title":"Cost Control and Billing"},{"body":" The below links to usage manuals of selected research software requiring more complex usage instructions.\n ","excerpt":" The below links to usage manuals of selected research software …","ref":"/manuals/hpc_cluster/selected_software/","title":"Selected Research Software Usage"},{"body":"Text Viewing Here are a few commands that are used to just display the text within a file:\nmore \u003cFILENAME\u003e # Views text, use space bar to browse, 'q' to quit less \u003cFILENAME\u003e # Also views text, uses arrow keys to browse, 'q' to quit cat \u003cFILENAME\u003e # Concatenates files and prints content to screen  Text Editors  Nano  A simple terminal-based editor.   Neovim  Non-graphical (terminal-based) editor. Neovim is an improved version of vim.   Vim Gvim  Non-graphical (vim) or window-based editor (gvim). Vim is the improved version of vi.   Emacs  Non-graphical or window-based editor.   Visual Studio Code  Graphical editor that runs on your local machine that supports different plugins.    Nano The nano editor is the simplest to use and can be good for beginners:\nnano \u003cFILENAME\u003e # Open file if it exists, or create it  Navigation in nano uses the arrow keys, and all other commands are noted at the bottom of the screen. The CTRL key is used in combination with other keys to execute commands in nano.\nFor example, at the bottom of the nano screen it is noted that ^X is used to exit. This means you will need to hold the CTRL key and then press x in order to quit. After that, just follow the on screen prompts at the bottom.\nFor more nano commands, please visit Overview of nano shortcuts.\nNeovim / Vim / GVim / VI All of these editors follow the same principals.\nnvim \u003cFILENAME\u003e # Open file if it exists, or create it vim \u003cFILENAME\u003e # Open file if it exists, or create it gvim \u003cFILENAME\u003e # Open file if it exists, or create it (must have XForwarding or VNC) vi \u003cFILENAME\u003e # Open file if it exists, or create it  For more information please visit Vim Manual.\nEmacs Navigation in emacs also uses the arrow keys. It is similar to nano, in that, CTRL is combined with other keys to execute commands.\nFor example, to open a file, simply run the command with a file name:\nemacs \u003cFILENAME\u003e # Open file if it exists, or create it  Then, after you have made some changes, exit by holding the CTRL key and then pressing c, releasing and then holding the CTRL key once more and pressing c again. After that, just follow the on screen prompts at the bottom.\nFor more commands in emacs please visit GNU Emacs Reference Card\nVisual Studio Code Install This editor should be installed on your local machine (ie. workstation, laptop). Please visit Visual Studio Code for software download.\nRemote Editing To setup Visual Studio Code to remotely edit files on the cluster, please refer to our guide to VSCode Usage on HPCC.\nRStudio Server  The shared web instances of RStudio will soon be deprecated and replaced with Open OnDemand. Please start transitioning there to receive the best performance from RStudio.\n Two options exist to access the RStudio Server IDE:\n Shared Web Instance Compute Node Instance  While the Shared Web Instance is easier for less experienced users, it does not allow to load a specific R version nor access more extensive computing resources. Thus, experienced users may prefer the Compute Node Instance as it does not share these limitations.\n1. Shared Web Instance R users can log in to their HPCC accounts via an RStudio Server instance. To do so, visit the HPCC RStudio Server. Next provide your HPCC login credentials and click the Sign In button.\n2. Compute Node Instance a. Interactive Alternatively, an RStudio Server instances can be started directly on a compute node and accessed via an SSH tunnel. This involves the following steps.\n  SSH into the cluster as outlined here.\n  Log in to a compute node interactively via srun, where the proper parameters need to be specified by the user. These include partition name, RAM, number of CPU cores, wall time limit, etc. Additional details on using srun are available here.\n  srun --partition=short --mem=8gb --cpus-per-task=2 --ntasks=1 --time=2:00:00 --pty bash -l  Load the latest versions of R and RStudio Server using the module system:  module unload R module load R/4.3.0 # Or latest version (previously it was R/4.2.2) module load rstudio-server/2023.09.1-494 # Or latest version (previously it was rstudio-server/2022.02.0-443)  Start RStudio Server:  start-rserver.sh  Next follow the instructions printed to the terminal after running start-rserver.sh. The command-lines given in these instructions need be executed in a terminal of a user’s local system, and not on the remote system where start-rserver.sh was exectuted.  b. Non-Interactive The steps for launching an interactive job can be integrated into a script and submitted non-interactvely for a quicker deployment of a RStudio Server instance on a compute node. Instructions outling how to do this are located here.\nJupyter Server  The shared web instance of Jupyter will soon be deprecated and replaced with Open OnDemand. Please start transitioning there to receive the best performance from Jupyter.\n Two options exist to access JupyterLab:\n Web Instance Compute Node Instance  1. Jupyter Web Instance Users can log into their HPCC accounts via the JupyterHub server instance. To do so, visit the HPCC JupyterHub server. Next provide your HPCC login credentials and click the Sign In button.\nAccount changes can sometimes lead to users needing to restart active cluster sessions, and Jupyter is no different. To restart your Jupyter session, from the “File” tab click “Hub Control Panel”. From the new screen click “Stop My Server”, then “Start My Server”. After a few seconds your session will be restarted.\nChoosing A Profile There are a handful of profiles that can be selected from, below is a chart displaying the existing profiles.\n   Profile Name Partition Resources Time Limit Notes     Immediate Server - Shared None This will run Jupyter on a shared machine with all other users who choose this option. Choose this if you want a server with no time limit at the expense of worse performance.   Quick Server Short 2 cores, 8GB memory 2 Hours This is for a quick session lasting under 2 hours. Good for testing or debugging work quickly.   Compute Intel Intel 8 cores, 32GB memory 24 hours This will run on an Intel machine with a 24 hour time limit.   Compute Batch Batch 8 cores, 32GB memory 24 hours This will run on an Batch (AMD) machine with a 24 hour time limit.   Compute Epyc Epyc 8 cores, 32GB memory 24 hours This will run on an AMD machine with a 24 hour time limit.   Highmem Highmem 8 cores, 128GB memory 24 hours This will run on the highmem partition, good for jobs that require a lot of memory.    NOTE that when your job reaches the specified time limit the job will be terminated and jupyter session closed. Jupyter autosaves every 2 minutes, but any new work will be lost and any running jobs cancelled once the time limit is reached.\nIf your job requires more flexible resources, please refer to the below section for running Jupyter directly on a compute node.\n2. Jupyter Compute Node Instance a. Steps First, start an interactive session on a compute node\nsrun -p epyc -t 4:00:00 -c 4 --mem=10GB --pty bash -l # Customize as needed  After your job has been scheduled, activate the jupyterlab module and start the server.\nmodule load jupyterlab/ start-jupyter.sh  A fed seconds after running start-jupyter.sh it will prompt you for a password. Enter a password that you would like to use to access the notebook. NOTE: Text will not show up when you type your password, this is expected.\nAfter entering a password, it will print some text guiding you on creating a tunnel.\nNOTE: The port and node will likely be different than pictured. This is expected, and unique to each session.\nAs the text suggests, enter the ssh -NL command in your terminal or setup MobaXTerm with the supplied details. If using the ssh method, the terminal is expected to hang after logging in and no further output should be generated.\nAfter a few seconds the server will start. At this point you can navigate to “http://127.0.0.1:PORT/lab” on your local machine, **replacing PORT with the port assigned when running the start-jupyter.sh command (9345 in the example above).\nb. Logging In After navigating to the login page, you will be prompted for the password you originally gave the notebook.\nAfter logging in, you can use the notebook as you would on our hosted JupyterLab server.\nc. Shutting Down When you are finished with your session, you can stop the Jupyter server from running by going to “File \u003e Shut Down” in the notebook, or by entering Ctrl+C in the terminal window.\n","excerpt":"Text Viewing Here are a few commands that are used to just display the …","ref":"/manuals/linux_basics/text/","title":"Text Editors"},{"body":"Support For cluster support, see the Help and Contacts section of our Introduction.\nFacility Please do not contact the System Administrators directly for general cluster/software support. Our support email (support [at] hpcc.ucr.edu) will make sure the most eyes see the ticket and prevent it from getting lost in cluttered email inboxes.\n Austin Leong, Sr. HPC Systems Administrator Emerson Jacobson, HPC Systems Administrator Thomas Girke, Director of HPC Center  Advisory Board (executive committee) The responsibilities of the Advisory Board are outlined here.\n  Faculty members\n Jason E Stajich (Microbiology \u0026 Plant Pathology) Wenxiu Ma (Statistics) Stefano Lonardi (CSE) Mark Alber (Mathematics) Adam Godzik (Biomedical Sciences) Laura Sales (Physics) Ahmed Eldawy (CSE) Xinping Cui (Statistics) Leonard Mueller (Chemistry)    HPC expert staff members from UCR\n Keith Richards-Dinger (Earth Sciences) Victor Hill (CS) Bill Strossman (C\u0026C)    External members from academia and industry\n One of each to be added here.    Office location and mailing address 1208/1207 Genomics Building (Google Map) 3401 Watkins Drive University of California Riverside, CA 92521\nServer rooms Genomics HPCC’s main server room is in the Genomics Building, Rm 1120A.\nCoLo server room School of Medicine CoLo\nHelp For questions or requesting new user accounts please email support@hpcc.ucr.edu.\n","excerpt":"Support For cluster support, see the Help and Contacts section of our …","ref":"/about/overview/contact/","title":"Contact"},{"body":"Dashboard HPCC cluster users are able to check on their home and bigdata storage usage from the Dashboard Portal.\nHome Home directories are where you start each session on the HPC cluster and where your jobs start when running on the cluster. This is usually where you place the scripts and various things you are working on. This space is very limited. Please remember that the home storage space quota per user account is 20 GB.\n   Path /rhome/username     User Availability All Users   Node Availability All Nodes   Quota Responsibility User    Bigdata Bigdata is an area where large amounts of storage can be made available to users. A lab purchases bigdata space separately from access to the cluster. This space is then made available to the lab via a shared directory and individual directories for each user.\nLab Shared Space This directory can be accessed by the lab as a whole.\n   Path /bigdata/labname/shared     User Availability Labs that have purchased space.   Node Availability All Nodes   Quota Responsibility Lab    Individual User Space This directory can be accessed by specific lab members.\n   Path /bigdata/labname/username     User Availability Labs that have purchased space.   Node Availability All Nodes   Quota Responsibility Lab    Non-Persistent Space Frequently, there is a need for faster temporary storage. For example activities like the following would fall under this category:\n Output a significant amount of intermediate data during a job Access a dataset from a faster medium than bigdata or the home directories Write out lock files  These types of activities are well suited to the use of fast non-persistent spaces. Below are the filesystems available on the HPC cluster that would best suited for these actions.\nSSD Backed Scratch Space This space is much faster than the persistent space (/rhome,/bigdata), but slower than using RAM based storage. The scratch space should be used with the $SCRATCH environment variable which is automatically set for each job. This location is local to the node it is ran on and is automatically deleted once a job has finished.\n   Path /scratch     User Availability All Users   Node Availability All Nodes   Quota Responsibility N/A    Temporary Space This is a standard space available on all Linux systems. Please be aware that it is limited to the amount of free disk space on the node you are running on.\n   Path /tmp     User Availability All Users   Node Availability All Nodes   Quota Responsibility N/A    RAM Space This type of space takes away from physical memory but allows extremely fast access to the files located on it. When submitting a job you will need to factor in the space your job is using in RAM as well. For example, if you have a dataset that is 1G in size and use this space, it will take at least 1G of RAM.\n   Path /dev/shm     User Availability All Users   Node Availability All Nodes   Quota Responsibility N/A    Usage and Quotas To quickly check your usage and quota limits:\ncheck_quota home check_quota bigdata  To get the usage of your current directory, run the following command:\ndu -sh .  To calculate the sizes of each separate sub directory, run:\ndu -sch * du -sch .[!.]* * | sort -h # includes hidden files and directories  This may take some time to complete, please be patient.\nFor more information on your home directory, please see the Linux Basics Orientation.\nAutomatic Backups The cluster does create backups however it is still advantageous for users to periodically make copies of their critical data to a separate storage device. The cluster is a production system for research computations with a very expensive high-performance storage infrastructure. It is not a data archiving system.\nHome backups are created daily and kept for one week. Bigdata backups are created weekly and kept for one month.\nHome and bigdata backups are located under the following respective directories:\n/rhome/.snapshots/ /bigdata/.snapshots/  The individual snapshot directories have names with numerical values in epoch time format. The higher the value the more recent the snapshot.\nTo view the exact time of when each snapshot was taken execute the following commands:\nmmlssnapshot home mmlssnapshot bigdata  ","excerpt":"Dashboard HPCC cluster users are able to check on their home and …","ref":"/manuals/hpc_cluster/storage/","title":"Data Storage"},{"body":"Streams On the command line, or terminal, there are three very important lanes where information can be sent, we call these streams. A single command can take information in from STDIN and then send information out on both STDOUT and STDERR simultaneously.\nSTDIN For example, we can send the contents of a file as a STDIN steam to the wc command in order to count the lines:\nwc -l \u003c file.txt  STDOUT The STDOUT steam is probably the most often used, since this is how commands send information to the screen. However, if we do not want the information printed to the screen, we can send it into a file for later review:\nls \u003e output.txt # Overwrite contents in output file with `ls` results  You can also append to the same file, if more information is to be saved:\nls \u003e\u003e output.txt # Append results from `ls` to the bottom of the file  STDERR The error stream is very useful to separate error messages (or warnings) from real output (your results). Since there is no -e flag for the ls command this will generate an error. We can then store this error in a by redirecting the error stream with 2\u003e.\nls -e 2\u003e errors.txt  Tips Combined streams If you want to combined your STDOUT with your STDERR stream and store it into a file, you can do this with \u0026\u003e, like so:\ncommand \u0026\u003e output_and_errors.txt  Trash Streams If you want to ignore all information from STDOUT and STDERR you can send both of these streams to the trash (/dev/null):\ncommand \u0026\u003e /dev/null  This can be useful when you are only interested in the resulting file that your command will create.\n","excerpt":"Streams On the command line, or terminal, there are three very …","ref":"/manuals/linux_basics/streams/","title":"Streams"},{"body":"Acknowledgement in Publications We appreciate that you have chosen our facility to support your research and would like to remind you to add the following statement to acknowledge the High-Performance Computing Center in your publications and presentations.\nComputations were performed using the computer clusters and data storage resources of the HPCC, which were funded by grants from NSF (MRI-2215705, MRI-1429826) and NIH (1S10OD016290-01A1).\n Your success is important to us and we would appreciate if you could share with us the references or URLs to any publications or presentations that used our facilty.\nGrants  NSF MRI-2215705 NSF MRI-1429826 NIH 1S10OD016290-01A1  ","excerpt":"Acknowledgement in Publications We appreciate that you have chosen our …","ref":"/about/overview/acknowledgement/","title":"Acknowledgement of Facility"},{"body":"Piping One of the the most powerful things you can do in Linux is piping. This allows chaining of commands so that the output (STDOUT) of one command is the input (STDIN) for another. This is done by placing a | (pipe) character between the commands. Please note that not all commands support this, for example if your command is not taking input from STDIN.\nAs an example, let’s collect all the lines where pattern is found in a file, then count how many lines were found:\ngrep 'pattern' filename | wc -l  You can pipe as many commands together as you like, not just two. For example, you can combined two CSV files and extract the first column, then filter for only unique values:\ncat filename1.csv filename2.csv | cut -f 1 | sort | uniq  For a few more simple examples, please visit here Pipe, Grep and Sort Command in Linux/Unix with Examples. Or you can try searching Google for even more complex examples, the possibilities are endless.\n","excerpt":"Piping One of the the most powerful things you can do in Linux is …","ref":"/manuals/linux_basics/pipes/","title":"Piping"},{"body":"Permissions It is useful to share data and results with other users on the cluster, and we encourage collaboration. The easiest way to share a file is to place it in a location that both users can access. Then the second user can simply copy it to a location of their choice. However, this requires that the file permissions permit the second user to read the file. Basic file permissions on Linux and other Unix like systems are composed of three groups: owner, group, and other. Each one of these represents the permissions for different groups of people: the user who owns the file, all the group members of the group owner, and everyone else, respectively Each group has 3 permissions: read, write, and execute, represented as r,w, and x. For example the following file is owned by the user username (with read, write, and execute), owned by the group groupname (with read and execute), and everyone else cannot access it.\nusername@pigeon:~$ ls -l myFile -rwxr-x--- 1 username groupname 1.6K Nov 19 12:32 myFile  If you wanted to share this file with someone outside the groupname group, read permissions must be added to the file for ‘other’:\nusername@pigeon:~$ chmod o+r myFile  To learn more about ownership, permissions, and groups please visit Linux Basics Permissions.\nSet Default Permissions In Linux, it is possible to set the default file permission for new files. This is useful if you are collaborating on a project, or frequently share files and you do not want to be constantly adjusting permissions The command responsible for this is called ‘umask’. You should first check what your default permissions currently are by running ‘umask -S’.\nusername@pigeon:~$ umask -S u=rwx,g=rx,o=rx  To set your default permissions, simply run umask with the correct options. Please note, that this does not change permissions on any existing files, only new files created after you update the default permissions. For instance, if you wanted to set your default permissions to you having full control, your group being able to read and execute your files, and no one else to have access, you would run:\nusername@pigeon:~$ umask u=rwx,g=rx,o=  It is also important to note that these settings only affect your current session. If you log out and log back in, these settings will be reset. To make your changes permanent you need to add them to your .bashrc file, which is a hidden file in your home directory (if you do not have a .bashrc file, you will need to create an empty file called .bashrc in your home directory). Adding umask to your .bashrc file is as simple as adding your umask command (such as umask u=rwx,g=rx,o=r) to the end of the file. Then simply log out and back in for the changes to take affect. You can double check that the settings have taken affect by running umask -S.\nTo learn more about umask please visit What is Umask and How To Setup Default umask Under Linux?.\nFile Transfers For file transfers and data sharing, both command-line and GUI applications can be used. For beginners we recommend the FileZilla GUI application (download/install from here) since it is available for most OSs including macOS, Windows, Linux and ChromeOS. A basic user manual for FileZilla is here and a video tutorial is here. Alternative user-friendly SCP/SFTP GUI applications include Cyberduck and WinSCP for Mac and Windows OSs, respectively.\nFileZilla Usage FileZilla supports both Password+DUO and SSH Key based authentication methods. Both options are described below.\nAuthentication with Password+DUO When using FileZilla a new site can be created by selecting File -\u003e Site Manager. In the subsequent window, New Site should be selected. Next the following information should be provided in the right pane of the General tab.\nProtocol: SFTP Host: cluster.hpcc.ucr.edu Logon Type: Interactive User: \u003cusername\u003e  Under \u003cusername\u003e the actual username of an HPCC account should be provided. The Logon Type can be Interactive or Key File for Password+DUO or SSH Keys authentication, respectively. When choosing Password+DUO authentication, the max connections should be configured. To do so, navigate to the Transfer Settings tab and make the following settings.\n Limit Number of simultaneous connections: checked Maximum number of connections: 1  By clicking “OK” the new site will be saved. Subsequently, one can select the new site from the main window by clicking the arrow next to the site list, or just reopen the Site Manager and clicking the “connect” button from the new site window.\nAuthentication with SSH Key For ssh key based access, users want to make the selections shown in the Figure below. For this access method it is important to choose the Site Manager option as FileZilla’s Quick Access method will not work here.\n FileZilla settings with an SSH key. For generating SSH keys see here. Command-line SCP Advantages of this method include: batch up/downloads and ease of automation. A detailed manual is available here.\n  To copy files To the server run the following on your workstation or laptop:\nscp -r \u003cpath_to_directory\u003e \u003cyour_username\u003e@\u003chost_name\u003e:\n  To copy files From the server run the following on your workstation or laptop:\nscp -r \u003cyour_username\u003e@\u003chost_name\u003e:\u003cpath_to_directory\u003e .\n  Copying bigdata Rsync can:\n Copy (transfer) directories between different locations Perform transfers over the network via SSH Compare large data sets (-n, --dry-run option) Resume interrupted transfers  Rsync Notes:\n Rsync can be used on Windows, but you must install Cygwin. Most Mac and Linux systems already have rsync install by default. Always put the / after both folder names, e.g: FOLDER_A/ Failing to do so will result in the nesting folders every time you try to resume. If you don’t put / you will get a second folder_B inside folder_B FOLDER_A/FOLDER_A/ Rsync only copies by default. Once the rsync command is done, run it again. The second run will be shorter and can be used as a double check. If there was no output from the second run then nothing changed. To learn more try man rsync  If you are transfering to, or from, your laptop/workstation it is required that you run the rsync command locally from your laptop/workstation.\nTo transfer to the cluster:\nrsync -av --progress FOLDER_A/ cluster.hpcc.ucr.edu:FOLDER_A/  To transfer from the cluster:\nrsync -av --progress cluster.hpcc.ucr.edu:FOLDER_A/ FOLDER_A/  Rsync will use SSH and will ask you for your cluster password, the same way SSH or SCP does.\nIf your rsync transer was interrupted, rsync can continue where it left off. Simply run the same command again to resume.\nCopying large folders on the cluster between Directories If you want to syncronize the contents from one directory to another, then use the following:\nrsync -av --progress PATH_A/FOLDER_A/ PATH_B/FOLDER_A/  Rsync does not move but only copies. Thus you would need to delete the original once you confirm that everything has been transfered.\nCopying large folders between the cluster and other servers If you want to copy data from the cluster to your own server, or another remote system, use the following:\nrsync -ai FOLDER_A/ sever2.xyz.edu:FOLDER_A/  The sever2.xyz.edu machine must be a server that accepts Rsync connections via SSH.\nSharing Files on the Web  Note: This is not intended to be used as a long-term solution or referenced in publications. It should be used for internal project purposes only. A long-term solution is required, please use a web or cloud-based installation.\n Simply create a symbolic link or move the files into your html directory when you want to share them. For exmaple, log into the HPC cluster and run the following:\n# Make sure you have an html directory mkdir ~/.html #Make sure permissions are set correctly chmod a+x ~/ chmod a+rx ~/.html # Make a new web project directory mkdir www-project chmod a+rx www-project # Create a default test file echo '\u003ch1\u003eHello!\u003c/h1\u003e' \u003e ~/www-project/index.html # Create shortcut/link for new web project in html directory ln -s ~/www-project ~/.html/  Now, test it out by pointing your web-browser to https://cluster.hpcc.ucr.edu/~username/www-project/ Be sure to replace username with your actual user name. The forward slash at the end is important.\nCommon Problems “403 Forbidden” / You don’t have permissions If using a symbolic link to data stored elsewhere on the cluster, every folder in the tree leading up to the shared folder must, at a minimum, have the execute permission (chmod a+x folder_name). For example, if you have a symbolic link to /bigdata/mylab/myuser/data/web-content, then myuser, data, and web-content must all have the execute permission (bigdata and mylab should already have them).\nPassword Protect Web Pages Files in web directories can be password protected. First create a password file and then create a new user:\ntouch ~/.html/.htpasswd htpasswd ~/.html/.htpasswd newwebuser  This will prompt you to enter a password for the new user ‘newwebuser’. Create a new directory, or go to an existing directory, that you want to password protect:\nmkdir ~/.html/locked_dir cd ~/.html/locked_dir  For the above commands you can choose any directory name you want.\nThen place the following content within a file called .htaccess:\nAuthName 'Please login' AuthType Basic AuthUserFile /rhome/username/.html/.htpasswd require user newwebuser  Now, test it out by pointing your web-browser to http://cluster.hpcc.ucr.edu/~username/locked_dir Be sure to replace username with your actual user name for the above code and URL.\nGoogle Drive There are several tools used to transfer files from Google Drive to the cluster, however RClone may be the easiest to setup.\n  Create an SSH tunnel to the cluster, (MS Windows users should use MobaXterm):\nssh -L 53682:localhost:53682 username@cluster.hpcc.ucr.edu    Once you have logged into the cluster with the above command, then load rclone via the module system and run it, like so:\nmodule load rclone rclone config    After that, follow this RClone Walkthrough to complete your setup.\n  Globus See Globus page here.\n","excerpt":"Permissions It is useful to share data and results with other users on …","ref":"/manuals/hpc_cluster/sharing/","title":"Sharing Data"},{"body":"Protection Levels and Classification UCR protection levels, and data classifications are outlined by UCOP as a UC wide policy: UCOP Institutional Information and IT Resource Classification According to the above documentation, there are 4 levels of protection for 4 classifications of data:\n   Protection Level Policy Examples     P1 - Minimal IS-1 Internet facing websites, press releases, anything intended for public use   P2 - Low IS-2 Unpublished research work, intellectual property NOT classified as P3 or P4   P3 - Moderate IS-3 Research information classified by an Institutional Review Board as P3 (ie. dbGaP from NIH)   P4 - High IS-4 Protected Health Information (PHI/HIPAA), patient records, sensitive identifiable human subject research data, Social Security Numbers    The HPC cluster could be compliant with with other security polices (ie. NIH), however the policy must be reviewed by our security team.\nAt this time the HPC cluster is not a IS-4 (P4) compliant cluster. If you have needs for very sensitive data, it may be best to work with UCSD and their Sherlock service. Our cluster is IS-3 compliant, however there are several responsibilities that users will need to adhere to.\nGeneral Guidelines First, please contact us (support@hpcc.ucr.edu) before transferring any data to the cluster. After we have reviewed your needs, data classification and appropriate protection level, then it may be possible to proceed to use the HPCC.\nHere are a few basic rules to keep in mind:\n Always be aware of access control methods (Unix permissions and ACLs), do not allow others to view the data (ie. chmod 400 filename) Do not make unnecessary copies of the data Do not transfer the data to insecure locations Encrypt data when/where possible Delete all data when it is no longer needed  Access Controls When sharing files with others, it is imperative that proper permission are used. However, basic Unix permissions (user,group,other) may not be adequate. It is better to use ACLs in order to allow fine grained access to sensitive files.\nGPFS ACLs GPFS is used for most of our filesystems (/rhome and /bigdata) and it uses nfsv4 style ACLs. Users are able to explicitly allow many individuals, or groups, access to specific files or directories.\n# Get current permissions and store in acls file mmgetacl /path/to/file \u003e ~/acls.txt # Edit acls file containing permissions vim ~/acls.txt # Apply new permissions to file mmputacl -i ~/acls.txt /path/to/file # Delete acls file rm ~/acls.txt  For more information regarding GPFS ACLs refer to the following: GPFS ACLs. An example for granting another user access to a file is given here.\nXFS ACLs The XFS filesystem is used for the CentOS operating system and typical unix locations (/,/var,/tmp,etc). For more information on how to use ACLs under XFS, please refer to the following: CentOS 7 XFS\n Note: ACLs are not applicable to gocryptfs, which is a FUSE filesystem, not GPFS nor XFS.\n Encryption Under the IS-3 policy, P3 data encryption is mandatory. It is best if you get into the habit of doing encryption in transit, as well as encryption at rest. This means, when you move the data (transit) or when the data is not in use (rest), it should be encrypted.\nIn Transit When transferring files make sure that files are encrypted in flight with one of the following transfer protocols:\n SCP SFTP RSYNC (via SSH)  The destination for sensitive data on the cluster must also be encrypted at rest under one of the follow secure locations:\n /dev/shm/ - This location is in RAM, so it does not exist at rest (ensure proper ACLs) /run/user/$EUID/unencrypted - This location is manually managed, and should be created for access to unencrypted files.  It is also possible to encrypt your files with GPG (GPG Example), before they are transferred. Thus, during transfer they will be GPG encrypted. However, decryption must occur in one of the secure locations mentioned above.\n Note: Never store passphrases/passwords/masterkeys in an unsecure location (ie. a plain text script under /rhome).\n At Rest There are 3 methods available on the cluster for encryption at rest:\n GPG encryption of files via the command line GPG Example, however you must ensure proper ACLs and decryption must occur in a secure location. Create your own location with gocryptfs.  GocryptfsMgr You can use gocryptfs directly or use the gocryptfsmgr, which automates a few steps in order to simplify things.\nHere are the basics when using gocryptfsmgr:\n# Load the gocryptfs module. Not strictly required, but sets a handful of useful environment variables module load gocryptfs # Create new encrypted data directory gocryptfsmgr create bigdata privatedata1 # List all encrypted and unencrypted (access point) directories gocryptfsmgr list # Unencrypted privatedata1 (create access point) gocryptfsmgr open bigdata privatedata1 rw # Transfer files (ie. SCP,SFTP,RSYNC) scp user@remote-server:sensitive_file.txt $UNENCRYPTED/privatedata1/sensitive_file.txt # Remove access point (re-encrypt) privatedata1 gocryptfsmgr close privatedata1 # Remove all access points (re-encrypt all) gocryptfsmgr quit  For subsequent access to the encrypted space, (ie. computation or analysis) the follow procedure is recommended:\n# Request a 2hr interactive job on an exclusive node, resources can be adjusted as needed srun -p short --exclusive=user --pty bash -l # Unencrypted privatedata1 in read-only mode (create access point) gocryptfsmgr open bigdata privatedata1 ro # Read file contents from privatedata1 (simulating work or analysis) cat $UNENCRYPTED/privatedata1/sensitive_file.txt # List all encrypted and unencrypted (access points) directories gocryptfsmgr list # Make sure we re-encrypt (close access point) for privatedata1 gocryptfsmgr close privatedata1 # Exit from interactive job exit  With the above methods you can create multiple encrypted directories and access points and move between them.\nGocryptfs When using the gocryptfs directly, you will need to know a bit more details on how it works. The gocryptfs module on the HPCC cluster uses these predefined variables:\n HOME_ENCRYPTED = /rhome/$USER/encrypted - Very small encrypted space, not recommended to use BIGDATA_ENCRYPTED = /rhome/$USER/bigdata/encrypted - Best encrypted space for private data sets SHARED_ENCRYPTED = /rhome/$USER/shared/encrypted - Encrypted space when intending to share data sets with group UNENCRYPTED = /run/user/$UID/unencrypted - Access directory where encrypted data will be viewed as unencrypted  Here is an example how to create an encrypted directory under the BIGDATA_ENCRYPTED location using gocryptfs:\n# Load gocyptfs software module load gocryptfs # Create empty data directory mkdir -p $BIGDATA_ENCRYPTED/privatedata1 # Then intialize empty directory and encrypt it gocryptfs -aessiv -init $BIGDATA_ENCRYPTED/privatedata1 # Create access point directory where encrypted files will be viewed as unencrypted mkdir -p $UNENCRYPTED/privatedata1 # After that mount the encrypted directory on the access point and open a new shell within it gocryptfssh $BIGDATA_ENCRYPTED/privatedata1 # Transfer files (ie. SCP,SFTP,RSYNC) scp user@remote-server:sensitive_file.txt $UNENCRYPTED/sensitive_file.txt # Exiting this shell will automatically unmount the unencrypted directory exit  For subsequent access to the encrypted space, (ie. computation or analysis) the follow procedure is recommended:\n# Request a 2hr interactive job on an exclusive node, resources can be adjusted as needed srun -p short --exclusive=user --pty bash -l # Load cyptfs software module load gocryptfs # Create unencrypted directory mkdir -p $UNENCRYPTED/privatedata1 # Mount encrypted filesystem as read-only and unmount idling for 1 hour gocryptfs -ro -i 1h -sharedstorage $BIGDATA_ENCRYPTED/privatedata1 $UNENCRYPTED/privatedata1 # Read file contents (simulating work or analysis) cat $UNENCRYPTED/privatedata1/sensitive_file.txt # Manually close access point when analysis has completed fusermount -u $UNENCRYPTED/privatedata1 # Delete old empty access point rmdir $UNENCRYPTED/privatedata1   WARNING: Avoid writing to the same file at the same time from different nodes. The encrypted file system cannot handle simultaneous writes and will corrupt the file. If simultaneous jobs are necessary then using write mode from a head node and read-only mode from compute nodes may be the best solution here. Also, be mindful of reamaining job time and make sure that you have unmounted the unencrypted directories before your job ends.\n For another example on how to use gocrypfs on an HPC cluster: Luxembourg HPC gocryptfs Example\nDeletion To ensure the complete removal of data, it is best to shred files instead of removing them with rm. The shred program will overwrite the contents of a file with randomized data such that recovery of this file will be very difficult, if not impossible.\nInstead of using the common rm command to delete something, please use the shred command, like so:\nshred -u somefile  The above command will overwrite the file with random data, and then remove (unlink) it.\nIf we want to be even more secure, we can pass over the file seven times to ensure that reconstruction is nearly impossible, then remove it:\nshred -v -n 6 -z -u somefile  ","excerpt":"Protection Levels and Classification UCR protection levels, and data …","ref":"/manuals/hpc_cluster/security/","title":"Security"},{"body":"Variables The HPCC cluster uses bash as the default shell environment. Within this environment, variables can be set and reused.\nFor example:\nMYVAR=’Something’ export MYVAR=’Something’ echo $MYVAR  Default Variables Some softwares utilize this feature and require that specific environment variables be set. For example, every time you login, the following variables are set by default:\necho $HOME #Contains your home path echo $USER #Contains your username echo $PATH #Contains paths of executables echo $LD_LIBRARY_PATH #Contains paths of library dependencies  Finding Variables To see a list of all variables currently set in your shell, use the env command. You can also grep through this list to find variables, like so:\nenv | grep -i home  Or if you are in a Slurm job, you can find all related Slurm variables:\nenv | grep -i slurm  Setting variables Try to choose unique names when setting variables. It is best to not overwrite a variable that is already set, unless on purpose.\nTo set a variable in your current shell, you can do so like this:\nMYVAR='Something Important'   Notice that there is no spaces around the = sign.\n If you would like to set a variable that is carried over to all other commands or sub-shells, then it must be exported:\nexport MYVAR='Something Important'  ","excerpt":"Variables The HPCC cluster uses bash as the default shell environment. …","ref":"/manuals/linux_basics/variables/","title":"Variables"},{"body":"Communicating with others The cluster is a shared resource, and communicating with other users can help to schedule large computations.\nLooking-Up Specific Users\nA convenient overview of all users and their lab affiliations can be retrieved with the following command:\nuser_details.sh  You can search for specific users by running:\nMATCH1='username1' # Searches by real name, and username, and email address and PI name MATCH2='username2' user_details.sh | grep -P \"$MATCH1|$MATCH2\"  Listing Users with Active Jobs on the Cluster To get a list of usernames:\nsqueue --format '%u' | sort | uniq  To get the list of real names:\ngrep \u003c(user_details.sh | awk '{print $2,$3,$4}') -f \u003c(squeue --format '%u' --noheader | sort | uniq) | awk '{print $1,$2}'  To get the list of emails:\ngrep \u003c(user_details.sh | awk '{print $4,$5}') -f \u003c(squeue --format '%u' --noheader | sort | uniq) | awk '{print $2}'  ","excerpt":"Communicating with others The cluster is a shared resource, and …","ref":"/manuals/hpc_cluster/users/","title":"Communicating"},{"body":"Scripting Converting code into a script is useful, and almost necessary when running jobs on the cluster.\nThere are many benifits of doing this:\n Easy to run - blackbox Easy to maintain - consolidated code Easy to distribute - capsulated code Easy to automate (crontab?) - does not require interaction  Breakdown There are four basic parts that are needed to convert your commands into a script:\n  First save all your commands into a file and call it myscript.sh, you can do this with a Text Editor or Transferring it from your computer.\n  Add the #! (SheBang) as the first line in file, this defines the interpreter. In this example, we are using bash as the interpreter, which will run all subsequent lines in this file:\n  #!/bin/bash  Add the proper permissions to the script, allow user (or group) execution:  chmod u+x myscript.sh  OR\nchmod g+x myscript.sh  You can pass arguments via command line into a script, this step is optional, but important to note.  For example if I want to call my script, like so:\n/where/my/script/lives/myscript.sh username number  Then inside my script I can capture the command line arguments into variables, like this:\nusername=$1 number=$2  Lastly adding the path to a script to the PATH environment variable, allows us to call the script without a prefixed path:  export PATH=/where/my/script/lives/:$PATH # Can be added to .bashrc for convenience  After we have exported PATH with the new path of our script, we call it like so:\nmyscript.sh username number  Walkthrough My bash commands:\nsacct -n -p -u jhayes -S 2020-01-01 -l \u003e myjobs.txt cut -d'|' -f4 myjobs.txt \u003e partitions.txt wc -l partitions.txt \u003e count.txt  Convert the above commands into a script named myscript.sh, with the following contents:\n#!/bin/bash # Gather Slurm job information sacct -n -p -u jhayes -S 2020-01-01 -l \u003e myjobs.txt # Filter on parittion column cut -d'|' -f4 myjobs.txt \u003e partitions.txt # Count how many records per partition cat partitions.txt | sort | uniq -c \u003e count.txt  Optional, we can alter the above commands by adding some pipes, as well as adding some variables to make this script count records for only a given partition:\n#!/bin/bash -l # Gather Slurm job information # filter on partition column # count how many records for given partition sacct -n -p -u $1 -S 2020-01-01 -l | cut -d'|' -f4 | grep $2 | wc -l \u003e count.txt  Add correct permissions:\nchmod u+x myscript.sh  Add to my PATH:\nmkdir -p ~/bin mv myscript.sh ~/bin export PATH=~/bin:$PATH  Now run my new script:\nmyscript.sh  Or, if we did the optional step of adding variables, we can do this:\nmyscript.sh johndoe001 intel # Arguments are \u003cUSERNAME\u003e and \u003cPARTITION\u003e  ","excerpt":"Scripting Converting code into a script is useful, and almost …","ref":"/manuals/linux_basics/scripting/","title":"Scripting"},{"body":"Process Management Basic Linux process management commands only apply to processes that are running on the current machine you are logged into. This means that you cannot use these commands to manage jobs. Jobs on the cluster are managed through Slurm, see Cluster Jobs for more details. However, these commands are still useful for pausing, backgrounding, killing processes on a login node directly. This commands could also be useful when running an interactive session on a compute node.\nUser Management top # view top consumers of memory and CPU (press 1 to see per-CPU statistics) who # Shows who is logged into system w # Shows which users are logged into system and what they are doing  Process Management Processes ps # Shows processes running by user ps -e # Shows all processes on system; try also '-a' and '-x' arguments ps ux -u \u003cUSERNAME\u003e # Shows all processes owned by user ps axjf # Shows the child-parent hierarchy of all processes ps -o %t -p \u003cPID\u003e # Shows how long a particular process was running. # (E.g. 6-04:30:50 means 6 days 4 hours ...)  Here are two common utilities for displaying processes, sorting, and even killing them:\ntop # Basic text based interface for exploring and managing processes htop # Text based interface for exploring and managing processes   Note q to quit and ? to see help\n Background Resume Cancel CTRL+z ENTER # Suspend a process in the background fg # Resume a suspended process and brings it into foreground bg # Resume a suspended process but keeps it running in the background CTRL+c # Cancel the process that is currently running in the foreground  PID echo $! # Get PID of last executed command  Killing kill -l # List all of the signals that can be sent to a process kill \u003cPID\u003e # Kill a specific process with process ID using SIGTERM kill -9 \u003cPID\u003e # Violently kill process with process ID using SIGKILL, may corrupt files  More on Terminating Processes DigitalOcean - How To Use ps, kill, and nice to Manage Processes in Linux\n","excerpt":"Process Management Basic Linux process management commands only apply …","ref":"/manuals/linux_basics/processes/","title":"Process Management"},{"body":"Terminal IDEs This page introduces tmux and Neovim as terminal-based working environments for working efficiently on remote systems like HPC clusters or cloud systems. They can be used independently or in combination, and provide many useful functionalities for working in local or remote terminal environments. Users who prefer a more graphical environment, VSCode might be a good alternative.\nTmux: virtual terminal multiplexer Tmux is a virtual terminal multiplexer providing persistent terminal sessions that are de- and re-attachable. It is an incredible useful tool for terminal-based work on remote systems. Major advantages of tmux are:\n Work on remote system cannot get lost due to disconnects. One can always re-attach to a session from the same or different computers. Many useful functionalities ‘power charge’ terminals.  Screen is a related virtual terminal multiplexer tool that can be used as an alternative (not covered here). It has similar functionalities as tmux.\nTmux can be downloaded and installed from here. A custom tmux (and nvim) environment with extensions can be installed by HPCC users with a single command (here Install_Nvim-R_Tmux). The same script also installs several useful Nvim plugins (see below). Alternatively, the install script can be downloaded from here. After installing the provided tmux environment in a user account, one needs to log out and in again to activate the environment. Note, installing the custom environment is optional and not required for any of the following examples. Users also need to be aware that the install script will make changes to their .bashrc and.tmux.conf` files. If this is not desirable, then one can install the components stepwise, or run the install, and then undo any configuration changes by following the instructions printed to the screen during the install.\n Tmux: Window Split into Several Panes Important considerations for virtual tmux sessions  Both tmux and screen sessions run on the system, where they were initialized. To reattach to a specific session on a remote system, like the HPCC cluster, one needs to first log in to the same node (here headnode) and then re-attach to the corresponding tmux session. It is important not to run tmux (or screen) sessions on computer nodes since tmux sessions are persistent. Instead tmux sessions should be run on a headnode. From an open tmux session one can then log in to a computer node via srun, or just submit jobs from a tmux session with sbatch.  Start Tmux  module load tmux: only required on systems that use environment modules, and the tmux load command is not specified in a user’s .bashrc file tmux: starts a new tmux session tmux a: attaches to an existing session, or a default session of a system, e.g. specified under ~/.tmux.conf tmux attach -t \u003cid\u003e: attaches to a running session selected under \u003cid\u003e tmux ls: lists existing tmux sessions  Prefix The prefix for controlling tmux depends on a user’s settings in their ~/.tmux.conf file.\n Ctrl-b: default is hard to type, and thus often not preferred Ctrl-a: more commonly used, also on HPCC  The prefix can be changed by placing the following lines into ~/.tmux.conf.\nunbind C-b set -g prefix C-a  Mouse Support Mouse support in tmux can be enabled with the following command.\n Ctrl-a : set -g mouse on  To turn mouse support on by default, include on a separate line of ~/.tmux.conf this command: set -g mouse on\nImportant keybindings for tmux Tmux sessions are organized in panes, windows and sessions themselves, where a window can have a single or several panes, and a session a single or several windows. The following commands for controlling tmux are organized by pane-, window- and session-level commands.\nPane-level commands\n Ctrl-a %: splits pane vertically Ctrl-a “: splits pane horizontally Ctrl-a o or Ctrl-a \u003carrow keys\u003e: jumps cursor to next pane Ctrl-a Ctrl-o: swaps panes Ctrl-a \u003cspace bar\u003e: rotates pane arrangement Ctrl-a Alt \u003cleft or right\u003e: resizes to left or right Ctrl-a Esc \u003cup or down\u003e: resizes to left or right Ctrl-a z: zoom into split pane (full window view); press again to zoom out  Window-level commands\n Ctrl-a n: switches to next tmux window Ctrl-a Ctrl-a: switches to previous tmux window Ctrl-a c: creates a new tmux window; any tmux window can be closed by typing exit on the command prompt Ctrl-a 1: switches to specific tmux window selected by number  Session-level commands\n Ctrl-a d: detaches from current session Ctrl-a s: switch between available tmux sessions $ tmux new -s \u003cname\u003e: starts new session with a specific name $ tmux ls: lists available tmux session(s) $ tmux attach -t \u003cid\u003e: attaches to specific tmux session $ tmux attach: reattaches to session $ tmux kill-session -t \u003cid\u003e: kills a specific tmux session Ctrl-a : kill-session: kills a session from tmux command mode that can be initiated with Ctrl-a :  Vim/Nvim overview Vim is a widely used, extremely powerful and versatile text editor for coding that is usually available on most Linux, Unix and macOS systems by default, and also can be installed on Windows. The newer version is called Neovim or Nvim. The main advantages of Nvim compared to Vim are its better performance and its built-in terminal emulator facilitating the communication among Nvim and interactive programming environments, such as command-lines, octave, R, etc. Since Vim and Nvim are managed independently, one can easily install and use them in parallel on the same system without interfering with each other. The usage of Nvim is almost identical to Vim. Emacs is a powerful alternative that can be used as an alternative to Nvim.\n Neovim Example with Autocompletion Nvim introduction The following opens a file (here myfile) with nvim (or vim). If nvim is not found then one might need to load it with module load neovim first. A custom nvim/tmux environment with extensions can be installed by HPCC users with the Install_Nvim-R_Tmux command. For details about this install script, see the corresponding tmux section above.\nOpen file with Nvim nvim myfile.txt # for neovim (or 'vim myfile.txt' for vim)  Tip: to always load Nvim with the standard vim command, one can add alias vim=nvim to ~/.bashrc.\nThree main modes Within Vim/Nvim, there are three main modes: normal, insert and command mode. The most important commands for navigating between the three modes are:\n i: The i key switches from the normal mode to the insert mode. The latter is used for typing. Esc: The Esc key switches from the insert mode back to the normal mode. :: The : key starts the command mode at the bottom of the screen.  Most important modifier keys The arrow keys can be used to move the cursor in the text. Using Fn Up/Down key allows to page through the text quicker (more on this below). In the following command overview, all commands starting with : need to be typed in the command mode. All other commands are typed in the normal mode after pressing the Esc key.\n :w: save changes to file. If you are in editing mode you have to hit Esc first. :q: quit file that has not been changed :wq: save and quit file :!q: quit file without saving any changes  Mouse support When enabled, one can position the cursor anywhere with the mouse as well as resize Nvim split windows, and switch the scope from one window split to another.\n :set mouse=n # enables mouse support, also try the a option :set mouse-=n # disables mouse support  To enable mouse support by default, add set mouse=n to Nvim’s config file located in a user’s home under ~/.config/nvim/init.vim. The corresponding config file for the older Vim version is ~/.vimrc.\nMoving around  arrow_keys: move cursor in the text Fn Up/Down: faster scrolling via paging. $ or 0: jump to back or beginning of line G or gg: jump to end of document and back to beginning w or b: move forward and backward by word ) or (: move forward and backward by sentence  Important keybindings  :split or :vsplit: splits viewport (similar to pane split in tmux) gz: maximizes size of viewport in normal mode (similar to Tmux’s Ctrl-a z zoom utility) Ctrl-w w: jumps cursor to other viewport and back Ctrl-w r: swaps viewports Ctrl-w =: resizes splits to equal size :resize \u003c+5 or -5\u003e: resizes height by specified value :vertical resize \u003c+5 or -5\u003e: resizes width by specified value Ctrl-w H or Ctrl-w K: toggles between horizontal/vertical splits :vsplit term://bash or :terminal: opens terminal in split mode or in a separate window, respectively. Ctrl-s and Ctrl-x: freezes/unfreezes vim (some systems)  Powerful features of command mode For example, search and replace with regular expression support. A detailed overview for using regular expressions in vim is here.\n / or ?: search in text forward and backward :%s/search_pattern/replace_pattern/cg: replacement syntax  Set command The set command typed in the command mode provides access to a large number of additional functions. Only a small number of examples is given here. For a more complete listing type :set all or consult the vim help with :help.\n :set wrap or :set nowrap: toggle for turning line wrapping on/off :set number or :set nonumber: toggle for turning line nubers on/off :set syntax=bash: toggle syntax highlighting for different languages (e.g. python, perl, bash, etc) or turn off with set syntax=off  Visual mode  Initialized from normal mode with v, V or Ctrl + v. Delete and copy selected text with d and y, respectively. For paste use p from normal mode. The copied (yanked) text is stored in a separate vim clipboard.  Copy and delete lines  yy: copies line where cursor is or those that are selected via visual mode. Paste works with p as above. dd: deletes line where cursor is or those that are selected via visual mode.  Indentation guides Vertical indentation lines (guides) are useful for tracking context in code. To enable indentation lines in nvim, one can use the indent-blankline.nvim plugin. Installation and configuration instructions for this plugin are here.\n Indentation Guides with `indent-blankline.nvim` Plugin Help Vim has a comprehensive built-in help system. To access and navigate it, here are some important commands. For a more detailed overview, visit this Built-in Vim Help page.\n :help: opens vim help system (:q closes it) Ctrl-] or Ctrl-[: use in help to jump to tagged topic :help helphelp: opens help as a file :help quickhelp or :help index: short help overview  File browser built into vim: NERDtree NERDtree provides file browser functionality for Vim. To enable it, the NERDtree plugin needs to be installed. It is included in the account configuration with Install_Nvim-R_Tmux mentioned above. To use NERDtree, open a file with vim/nvim and then type in normal mode zz. The same command closes NERDtree. Note the default for opening NERDtree is :NERDtree which has been remapped here to zz for quicker access. The basic NERDtree usage is explained here.\n NERDtree in action Useful resources for learning vim/nvim  Interactive Vim Tutorial Official Vim Documentation HPCC Linux Manual  Nvim for R users with nvim-R plugin Basics The Nvim-R plugin provides a powerful command-line working environment for R where users can send code from an R/Rmd script opened in Nvim to the R console. Essentially, this provides an RStudio like working environment within a terminal, which is often more flexible when working on remote systems than a GUI solution. It also can be combined with tmux to support ‘persistent’ R sessions that can be de- and re-attached (see tmux session above).\n Nvim-R IDE for R Quick configuration in user accounts The following steps 1-3 can be skipped if Nvim, Tmux and nvimR are already configured on a user’s system or account. One can also follow the detailed instructions for installing Nvim-R-Tmux from scratch.\n Log in to your user account on HPCC and execute Install_Nvim-R_Tmux (old: install_nvimRtmux). Additional details on this install are given in the tmux section above. Alternatively, one can use the step-by-step install here. To enable the nvim-R-tmux environment, log out and in again. Follow usage instructions of next section.  Basic usage of Nvim-R-Tmux The official and much more detailed user manual for Nvim-R is available here. The following gives a short introduction into the basic usage of Nvim-R-Tmux:\n1. Start tmux session (optional)\nNote, running Nvim from within a tmux session is optional. Skip this step if tmux functionality is not required (e.g. re-attaching to sessions on remote systems).\ntmux # starts a new tmux session tmux a # attaches to an existing session  2. Open nvim-connected R session\nOpen a *.R or *.Rmd file with nvim and intialize a connected R session with \\rf. This command can be remapped to other key combinations, e.g. uncommenting lines 10-12 in .config/nvim/init.vim will remap it to the F2 key. Note, the resulting split window between Nvim and R behaves like a split viewport in nvim meaning the usage of Ctrl-w w followed by i and Esc is important for navigation (for details see vim usage above).\nnvim myscript.R # or *.Rmd file  3. Send R code from nvim to the R pane\nSingle lines of code can be sent from nvim to the R console by pressing the space bar. To send several lines at once, one can select them in nvim’s visual mode and then press the space bar. Please note, the default command for sending code lines in the nvim-r-plugin is \\l. This key binding has been remapped in the provided .config/nvim/init.vim file to the space bar. Most other key bindings (shortcuts) still start with the \\ as LocalLeader, e.g. \\rh opens the help for a function/object where the curser is located in nvim. More details on this are given below.\nImportant keybindings for tmux See corresponding tmux section above.\nNvim-R-like solutions for Bash, Python and other languages Basics For languages other than R one can use the vimcmdline plugin for nvim (or vim). Supported languages include Bash, Python, Golang, Haskell, JavaScript, Julia, Jupyter, Lisp, Macaulay2, Matlab, Prolog, Ruby, and Sage. The nvim terminal also colorizes the output, as in the screenshot below, where different colors are used for general output, positive and negative numbers, and the prompt line.\n vimcmdline Install To install it, one needs to copy from the vimcmdline resository the directories ftplugin, plugin and syntax and their files to ~/.config/nvim/. For user accounts of UCR’s HPCC, the above install script Install_Nvim-R_Tmux (old: install_nvimRtmux) includes the install of vimcmdline (since 09-Jun-18).\nUsage The usage of vimcmdline is very similar to nvim-R. To start a connected terminal session, one opens with nvim a code file with the extension of a given language (e.g. *.sh for Bash or *.py for Python), while the corresponding interactive interpreter session is initiated by pressing the key sequence \\s (corresponds to \\rf under nvim-R). Subsequently, code lines can be sent with the space bar. More details are available here.\n","excerpt":"Terminal IDEs This page introduces tmux and Neovim as terminal-based …","ref":"/manuals/hpc_cluster/terminalide/","title":"Terminal-based Working Environments"},{"body":"Overview R provides a variety of packages for parallel computations. One of the most comprehensive parallel computing environments for R is batchtools (formerly BatchJobs). It supports both multi-core and multi-node computations with and without schedulers. By making use of cluster template files, most schedulers and queueing systems are also supported (e.g. Torque, Sun Grid Engine, Slurm).\nR code of this section To simplify the evaluation of the R code of this page, the corresponding text version is available for download from here.\nParallelization with batchtools The following introduces the usage of batchtools for a computer cluster using SLURM as scheduler (workload manager).\nSet up working directory for SLURM First login to your cluster account, open R and execute the following lines. This will create a test directory (here mytestdir), redirect R into this directory and then download the required files:\n slurm.tmpl .batchtools.conf.R  dir.create(\"mytestdir\") setwd(\"mytestdir\") download.file(\"https://bit.ly/3Oh9dRO\", \"slurm.tmpl\") download.file(\"https://bit.ly/3KPBwou\", \".batchtools.conf.R\")  Load package and define some custom function This is the test function (here toy example) that will be run on the cluster for demonstration purposes. It subsets the iris data frame by rows, and appends the host name and R version of each node where the function was executed. The R version to be used on each node can be specified in the slurm.tmpl file (under module load).\nlibrary('RenvModule') module('load','slurm') # Loads slurm among other modules library(batchtools) myFct \u003c- function(x) { result \u003c- cbind(iris[x, 1:4,], Node=system(\"hostname\", intern=TRUE), Rversion=paste(R.Version()[6:7], collapse=\".\")) }  Submit jobs from R to cluster The following creates a batchtools registry, defines the number of jobs and resource requests, and then submits the jobs to the cluster via SLURM.\nreg \u003c- makeRegistry(file.dir=\"myregdir\", conf.file=\".batchtools.conf.R\") Njobs \u003c- 1:4 # Define number of jobs (here 4) ids \u003c- batchMap(fun=myFct, x=Njobs) done \u003c- submitJobs(ids, reg=reg, resources=list(partition=\"short\", walltime=60, ntasks=1, ncpus=1, memory=1024)) waitForJobs() # Wait until jobs are completed  Summarize job status After the jobs are completed one instect their status as follows.\ngetStatus() # Summarize job status showLog(Njobs[1]) # killJobs(Njobs) # # Possible from within R or outside with scancel  Access/assemble results The results are stored as .rds files in the registry directory (here myregdir). One can access them manually via readRDS or use various convenience utilities provided by the batchtools package.\nreadRDS(\"myregdir/results/1.rds\") # reads from rds file first result chunk loadResult(1) lapply(Njobs, loadResult) reduceResults(rbind) # Assemble result chunks in single data.frame do.call(\"rbind\", lapply(Njobs, loadResult))  Remove registry directory from file system By default existing registries will not be overwritten. If required one can exlicitly clean and delete them with the following functions.\nclearRegistry() # Clear registry in R session removeRegistry(wait=0, reg=reg) # Delete registry directory # unlink(\"myregdir\", recursive=TRUE) # Same as previous line  Load registry into R Loading a registry can be useful when accessing the results at a later state or after moving them to a local system.\nfrom_file \u003c- loadRegistry(\"myregdir\", conf.file=\".batchtools.conf.R\") reduceResults(rbind)  ","excerpt":"Overview R provides a variety of packages for parallel computations. …","ref":"/manuals/hpc_cluster/parallelr/","title":"Parallel Evaluations in R"},{"body":"The Unix Shell When you log into UNIX/LINUX system, then is starts a program called the Shell. It provides you with a working environment and interface to the operating system. Usually there are several different shell programs installed. The shell program bash is one of the most common ones.\nfinger \u003cuser_name\u003e # shows which shell you are using chsh -l # gives list of shell programs available on your system (does not work on all UNIX variants) \u003cshell_name\u003e # switches to different shell  STDIN, STDOUT, STDERR, Redirections, and Wildcards See LINUX HOWTOs\nBy default, UNIX commands read from standard input (STDIN) and send their output to standard out (STDOUT).\nYou can redirect them by using the following commands:\n\u003cbeginning-of-filename\u003e* # * is wildcard to specify many files ls \u003e file # prints ls output into specified file command \u003c my_file # uses file after '\u003c' as STDIN command \u003e\u003e my_file # appends output of one command to file command | tee my_file # writes STDOUT to file and prints it to screen command \u003e my_file; cat my_file # writes STDOUT to file and prints it to screen command \u003e /dev/null # turns off progress info of applications by redirecting # their output to /dev/null grep my_pattern my_file | wc # Pipes (|) output of 'grep' into 'wc' grep my_pattern my_non_existing_file 2 \u003e my_stderr # prints STDERR to file  Useful shell commands cat \u003cfile1\u003e \u003cfile2\u003e \u003e \u003ccat.out\u003e # concatenate files in output file 'cat.out' paste \u003cfile1\u003e \u003cfile2\u003e \u003e \u003cpaste.out\u003e # merges lines of files and separates them by tabs (useful for tables) cmp \u003cfile1\u003e \u003cfile2\u003e # tells you whether two files are identical diff \u003cfileA\u003e \u003cfileB\u003e # finds differences between two files head -\u003cnumber\u003e \u003cfile\u003e # prints first lines of a file tail -\u003cnumber\u003e \u003cfile\u003e # prints last lines of a file split -l \u003cnumber\u003e \u003cfile\u003e # splits lines of file into many smaller ones csplit -f out fasta_batch \"%^\u003e%\" \"/^\u003e/\" \"{*}\" # splits fasta batch file into many files # at '\u003e' sort \u003cfile\u003e # sorts single file, many files and can merge (-m) # them, -b ignores leading white space, ... sort -k 2,2 -k 3,3n input_file \u003e output_file # sorts in table column 2 alphabetically and # column 3 numerically, '-k' for column, '-n' for # numeric sort input_file | uniq \u003e output_file # uniq command removes duplicates and creates file/table # with unique lines/fields join -1 1 -2 1 \u003ctable1\u003e \u003ctable2\u003e # joins two tables based on specified column numbers # (-1 file1, 1: col1; -2: file2, col2). It assumes # that join fields are sorted. If that is not the case, # use the next command: sort table1 \u003e table1a; sort table2 \u003e table2a; join -a 1 -t \"$(echo -e '\\t')\" table1a table2a \u003e table3 # '-a \u003ctable\u003e' prints all lines of specified table! # Default prints only all lines the two tables have in # common. '-t \"$(echo -e '\\t')\" -\u003e' forces join to # use tabs as field separator in its output. Default is # space(s)!!! cat my_table | cut -d , -f1-3 # cut command prints only specified sections of a table, # -d specifies here comma as column separator (tab is # default), -f specifies column numbers. grep # see chapter 4 egrep # see chapter 4  Screen Screen references\n Screen Turorial Screen Cheat Sheet  Starting a New Screen Session screen # Start a new session screen -S \u003csome-name\u003e # Start a new session and gives it a name  Commands to Control Screen\nCtrl-a d # Detach from the screen session Ctrl-a c # Create a new window inside the screen session Ctrl-a Space # Switch to the next window Ctrl-a a # Switch to the window that you were previously on Ctrl-a \" # List all open windows. Double-quotes \" are typed with the Shift key Ctrl-d or type exit # Exit out of the current window. Exiting form the last window will end the screen session Ctrl-a [ # Enters the scrolling mode. Use Page Up and Page Down keys to scroll through the window. Hit the Enter key twice to return to normal mode.  Attaching to Screen Sessions From any computer, you can attach to a screen session after SSH-ing into a server.\nscreen -r # Attaches to an existing session, if there is only one screen -r # Lists available sessions and their names, if there are more then one session running screen -r \u003csome-name\u003e # Attaches to a specific session screen -r \u003cfirst-few-letters-of-name\u003e # Type just the first few letters of the name # and you will be attached to the session you need  Destroying Screen Sessions  Terminate all programs that are running in the screen session. The standard way to do that is: Ctrl-c Exit out of your shell: exit Repeat steps 1 and 2 until you see the message: [screen is terminating]  There may be programs running in different windows of the same screen session. That’s why you may need to terminate programs and exit shells multiple time.\nTabs and a Reasonably Large History Buffer For a better experience with screen, run\ncp ~/.screenrc ~/.screenrc.backup 2\u003e /dev/null echo 'startup_message off defscrollback 10240 caption always \"%{=b dy}{ %{= dm}%H %{=b dy}}%={ %?%{= dc}%-Lw%?%{+b dy}(%{-b r}%n:%t%{+b dy})%?(%u)%?%{-dc}%?%{= dc}%+Lw%? %{=b dy}}\" ' \u003e ~/.screenrc  Simple One-Liner Shell Scripts Web page for script download.\nRenames many files *.old to *.new. To test things first, replace ‘do mv’ with ‘do echo mv’:\nfor i in *.input; do mv $i ${i/\\.old/\\.new}; done for i in *\\ *; do mv \"$i\" \"${i// /_}\"; done # Replaces spaces in files by underscores  Run an application in loops on many input files:\nfor i in *.input; do ./application $i; done  Run fastacmd from BLAST program in loops on many *.input files and create corresponding *.out files:\nfor i in *.input; do fastacmd -d /data/../database_name -i $i \u003e $i.out; done  Run SAM’s target99 on many input files:\nfor i in *.pep; do target99 -db /usr/../database_name -seed $i -out $i; done Search in many files for a pattern and print occurrences together with file names. for j in 0 1 2 3 4 5 6 7 8 9; do grep -iH \u003cmy_pattern\u003e *$j.seq; done  Example of how to run an interactive application (tmpred) that asks for file name input/output:\nfor i in *.pep; do echo -e \"$i\\n\\n17\\n33\\n\\n\\n\" | ./tmpred $i \u003e $i.out; done  Run BLAST2 for all .fasa1/.fasta2 file pairs in the order specified by file names and write results into one file:\nfor i in *.fasta1; do blast2 -p blastp -i $i -j ${i/_*fasta1/_*fasta2} \u003e\u003e my_out_file; done  This example uses two variables in a for loop. The content of the second variable gets specified in each loop by a replace function.  Runs BLAST2 in all-against-all mode and writes results into one file ('-F F' turns low-complexity filter off):\nfor i in *.fasta; do for j in *.fasta; do blast2 -p blastp -F F -i $i -j $j \u003e\u003e my_out_file; done; done;  How to write a real shell script   Create file which contains an interpreter as the first line:\n#!/bin/bash    Place shell commands in file below the interpreter line using a text editor.\n  Make file executable:\nchmod +x my_shell_script    Run shell script like this:\n./my_shell_script    Place it into your /rhome//bin directory\nmkdir -p ~/bin mv my_shell_script ~/bin/    Add the bin path to your shell permanently:\necho 'export PATH=~/bin:$PATH' \u003e\u003e ~/.bashrc source ~/.bashrc    Simple One-Liner Perl Scripts Small collection of useful one-liners:\nperl -p -i -w -e 's/pattern1/pattern2/g' my_input_file # Replaces a pattern in a file by a another pattern using regular expressions. # $1 or \\1: back-references to pattern placed in parentheses # -p: lets perl know to write program # -i.bak: creates backup file *.bak, only -i doesn't # -w: turns on warnings # -e: executable code follows  Parse lines based on patterns:\nperl -ne 'print if (/my_pattern1/ ? ($c=1) : (--$c \u003e 0)); print if (/my_pattern2/ ? ($d = 1) : (--$d \u003e 0))' my_infile \u003e my_outfile # Parses lines that contain pattern1 and pattern2. # The following lines after the pattern can be specified in '$c=1' and '$d=1'. # For logical OR use this syntax: '/(pattern1|pattern2)/'.  Remote Copy: wget, scp, ncftp Wget Use wget to download a file from the web:\nwget ftp://ftp.ncbi.nih.... # file download from www; add option '-r' to download entire directories  SCP Use scp to copy files between machines (ie. laptop to server):\nscp source target # Use form 'userid@machine_name' if your local and remote user ids are different. # If they are the same you can use only 'machine_name'.  Here are more scp examples:\nscp user@remote_host:file.name . # Copies file from server to local machine (type from local # machine prompt). The '.' copies to pwd, you can specify # here any directory, use wildcards to copy many files. scp file.name user@remote_host:~/dir/newfile.name # Copies file from local machine to server. scp -r user@remote_host:directory/ ~/dir # Copies entire directory from server to local machine.  Nice FTP From the linux command line run ncftp and use it to get files:\nncftp ncftp\u003e open ftp.ncbi.nih.gov ncftp\u003e cd /blast/executables ncftp\u003e get blast.linux.tar.Z (skip extension: @) ncftp\u003e bye  Archiving and Compressing Creating Archives tar -cvf my_file.tar mydir/ # Builds tar archive of files or directories. For directories, execute command in parent directory. Don't use absolute path. tar -czvf my_file.tgz mydir/ # Builds tar archive with compression of files or directories. For # directories, execute command in parent directory. Don't use absolute path. zip -r mydir.zip mydir/ # Command to archive a directory (here mydir) with zip. tar -jcvf mydir.tar.bz2 mydir/ # Creates *.tar.bz2 archive  Viewing Archives tar -tvf my_file.tar tar -tzvf my_file.tgz  Extracting Archives tar -xvf my_file.tar tar -xzvf my_file.tgz gunzip my_file.tar.gz # or unzip my_file.zip, uncompress my_file.Z, # or bunzip2 for file.tar.bz2 find -name '*.zip' | xargs -n 1 unzip # this command usually works for unzipping # many files that were compressed under Windows tar -jxvf mydir.tar.bz2 # Extracts *.tar.bz2 archive  Try also:\ntar zxf blast.linux.tar.Z tar xvzf file.tgz  Important options:\nf: use archive file p: preserve permissions v: list files processed x: exclude files listed in FILE z: filter the archive through gzip  Simple Installs Systems-wide installations Applications in user accounts Installation of RPMs Environment Variables xhost user@host # adds X permissions for user on server. echo $DISPLAY # shows current display settings export DISPLAY=\u003clocal_IP\u003e:0 # change environment variable unsetenv DISPLAY # removes display variable env # prints all environment variables  List of directories that the shell will search when you type a command:\necho $PATH  You can edit your default DISPLAY setting for your account by adding it to file .bash_profile\nExercises Exercise 1   Download proteome of Halobacterium spec. with wget and look at it:\nmodule load ncbi-blast/2.2.26 # Loads legacy blastall wget ftp://ftp.ncbi.nlm.nih.gov/genomes/genbank/archaea/Halobacterium_salinarum/representative/GCA_000069025.1_ASM6902v1/GCA_000069025.1_ASM6902v1_protein.faa.gz gunzip GCA_000069025.1_ASM6902v1_protein.faa.gz mv GCA_000069025.1_ASM6902v1_protein.faa AE004437.faa less AE004437.faa # press q to quit    Simple Analysis:\na. How many predicted proteins are there?\ngrep '^\u003e' AE004437.faa --count  b. How many proteins contain the pattern “WxHxxH” or “WxHxxHH”?\negrep 'W.H..H{1,2}' AE004437.faa --count  c. Use the find function (/) in ‘less’ to fish out the protein IDs containing the pattern or more elegantly do it with awk:\nawk --posix -v RS='\u003e' '/W.H..(H){1,2}/ { print \"\u003e\" $0;}' AE004437.faa | less # press q to quit    Create a BLASTable database with formatdb:\nls # before formatdb -i AE004437.faa -p T -o T ls # after '-p F' for nucleotide and '-p T' for protein database; '-o T' parse SeqId and create indexes    Generate myseq.fasta\na. Generate list of sequence IDs for the above pattern match result (i.e. retrieve my_IDs from step 2c). Alternatively, download the pre-generated file with wget.\nb. Retrieve the corresponding sequences for these IDs with the fastacmd command from the blastable database:\nwget https://cluster.hpcc.ucr.edu/~tgirke/Documents/UNIX/my_IDs fastacmd -d AE004437.faa -i my_IDs \u003e myseq.fasta less myseq.fasta # press q to quit    (Optional) Looking at several different patterns:\na. Generate several lists of sequence IDs from various pattern match results (i.e. retrieve a.my_ids, b.my_ids, and c.my_ids from step 2c).\nb. Retrieve the sequences in one step using the fastacmd in a for-loop:\nfor i in *.my_ids; do fastacmd -d AE004437.faa -i $i \u003e $i.fasta; done    Run blastall with a few proteins in myseq.fasta against your newly created Halobacterium proteome database.\nCreate first a complete blast output file including alignments. In a second step use the ’m -8' option to obtain a tabular output (i.e. tab separated values):\nblastall -p blastp -i myseq.fasta -d AE004437.faa -o blastp.out -e 1e-6 -v 10 -b 10 blastall -p blastp -i myseq.fasta -d AE004437.faa -m 8 -e 1e-6 \u003e blastp.tab less blastp.out # press q to quit less -S blastp.tab # -S disables line wrapping, press q to quit  The filed descriptions of the Blast tabular output (from the “-m 8” option) are available here:\n1 Query (The query sequence id) 2 Subject (The matching subject sequence id) 3 % id 4 alignment length 5 mismatches 6 gap openings 7 q.start 8 q.end 9 s.start 10 s.end 11 e-value 12 bit score    Is your blastp.out file equivalent to this one?\n  Parse blastall output into Excel spread sheet\na. Using biocore parser\nblastParse -i blastp.out -o blast.xls -c 5  b. Using BioPerl parser\nbioblastParse.pl blastp.out \u003e blastparse.txt    Exercise 2 Split sample fasta batch file with csplit (use sequence file myseq.fasta from Exercise 1).\ncsplit -z myseq.fasta '/\u003e/' '{*}'  Delete some of the files generated by csplit Concatenate single fasta files from (step 1) into to one file with cat (e.g. cat file1 file2 file3 \u003e bigfile). BLAST two related sequences, retrieve the result in tabular format and use comm to identify common hit IDs in the two tables.\nExercise 3 Run HMMPFAM search with proteins from Exercise 1 against Pfam database (will take ~3 minutes)\nhmmscan -E 0.1 --acc /srv/projects/db/pfam/2011-12-09-Pfam26.0/Pfam-A.hmm myseq.fasta \u003e output.pfam  Easier to parse/process tabular output\nhmmscan -E 0.1 --acc --tblout output.pfam /srv/projects/db/pfam/2011-12-09-Pfam26.0/Pfam-A.hmm myseq.fasta # also try --domtblout  Which query got the most hits? How many hits were found that query?\nExercise 4 Create multiple alignment with ClustalW (e.g. use sequences with ‘W.H..HH’ pattern)\nclustalw myseq.fasta mv myseq.aln myalign.aln  Exercise 5 Reformat alignment into PHYILIP format using ‘seqret’ from EMBOSS\nseqret clustal::myalign.aln phylip::myalign.phylip  Exercise 6 Create neighbor-joining tree with PHYLIP\ncp myalign.phylip infile protdist # creates distance matrix (you may need to press 'R' and then 'Y') cp outfile infile neighbor # use default settings (press 'Y') cp outtree intree  retree # displays tree and can use midpoint method for defining root of tree, my typical command sequence is: ‘N’ (until you see PHYLIP) ‘Y’ ‘M’ ‘W’ ‘R’ ‘R’ ‘X’\ncp outtree tree.dnd  View your tree in TreeBrowse or open it in TreeView\n","excerpt":"The Unix Shell When you log into UNIX/LINUX system, then is starts a …","ref":"/manuals/linux_basics/shell/","title":"Shell Bootcamp"},{"body":" The below links to detailed instructions. A shorter but more comprehensive summary for all major OSs is available here.\n ","excerpt":" The below links to detailed instructions. A shorter but more …","ref":"/manuals/hpc_cluster/sshkeys/","title":"SSH Keys"},{"body":"Compute Node We support running graphical programs on the cluster using VNC. For more information refer to Desktop Environments.\nGPU Workstation If a remote compute node does not fit your needs then we also have a GPU workstation specifically designed for rendering high resolution 3D graphics.\nHardware  Intel(R) Xeon(R) CPU E5-2620 v4 @ 2.10GHz DDR4 256GB @ 2400 MHz NVIDIA Corporation GM204GL [Quadro M5000] 1TB RAID 1 HDD  Software The GPU workstation is uniquely configured to be an extension of the HPCC cluster. Thus, all software available to the cluster is also available on the GPU workstation through Environment Modules.\nAccess The GPU workstation is currently located in the Genomics building room 1208. Please check ahead of time to make sure the machine is available support@hpcc.ucr.edu. Once you have access to the GPU workstation, login with your cluster credentials. If your username does not appear in the list, you may need to click Not listed? at the bottom of the screen so that you are able to type in your username.\nUsage There are 2 ways to use the GPU workstation:\n Local - Run processes directly on the GPU workstation hardware Remote - Run processes remotely on the GPU cluster hardware  Local\nLocal usage is very simple. Open a terminal and use the Environment Modules to load the desired software, then run your software from the terminal. For example:\nmodule load amira Amira  Remotely\nOpen a terminal and submit a job. This is to reserve the time on the remote GPU node. Then once your job has started connect to the remote GPU node via ssh and forward the graphics back to the GPU workstation. For example:\n  Submit a job for March 28th, 2018 at 9:30am for a duration of 24 hours, 4 cpus, 100GB memory:\nsbatch --begin=2018-03-28T09:30:00 --time=24:00:00 -p gpu --gres=gpu:1 --mem=100g --cpus-per-task=4 --wrap='echo ${CUDA_VISIBLE_DEVICES} \u003e ~/.CUDA_VISIBLE_DEVICES; sleep infinity'  Read about GPU jobs for more information regarding the above.\n  Run the VirtualGL client in order to receive 3D graphics from the remove GPU node:\nvglclient \u0026    Wait for the job to start, and then check where your job is running:\nGPU_NODE=$(squeue -h -p gpu -u $USER -o '%N'); echo $GPU_NODE    The above command should result in a GPU node name, which you then need to SSH directly into with the following:\nssh -XY $GPU_NODE    Once you have SSH’ed into the remote GPU node, run setup the environment and run your software:\nexport NO_AT_BRIDGE=1 module load amira vglrun -display :$(head -1 ~/.CUDA_VISIBLE_DEVICES) Amira    ","excerpt":"Compute Node We support running graphical programs on the cluster …","ref":"/manuals/hpc_cluster/visual/","title":"Visualization"},{"body":"What is Singularity In short, Singularity is a program that will allow a user to run code or command, within a customized environment. We will refer to this customized environment as a container. This type of container system is common, the more popular one being Docker. Since Docker requires root access and HPC users are not typically granted these permissions, we use Singularity instead. Docker containers can be used via Singularity, with varying compatibility.\nSingularity is forking into 2 branches:\n Singularity-CE - Community Edition from Sylabs.io Apptainer - Original Sinularity open source project  We will be using Apptainer when it is ready for production use. However, in the meantime, singularity-ce is currently availble on the cluster.\nLimitations Currently we are not supporting Slurm jobs being submitted from within a container. If you load the container centos/7.9 and try to submit a job from within it will fail. Please contact support in order to work around this issue.\nAdditionally, the building of Singularity contains on the cluster is not possible due to the steps requiring elevated permissions. If custom containers are required, you will need to build them on a machine that you have root/sudo access on (such as your local machine) or use a Remote Builder.\nHow to use Singularity You can use Singularity by running module load singularity. You can run singularity in an interactive mode by calling a shell, or you can run singularity in a non-interactive mode and just pass it a script/program to run. These 2 modes are very similar to job submission on the cluster; srun is used for interactive, while sbatch is used for non-interactive.\nPulling Container Images The first step in using Singularity is to get a container to run inside of. Containers can be custom built, pulled from Docker Hub or from SyLabs Container Library.\nFor example, if you wanted to run your program within an Ubuntu environment you could use the following command to pull the Ubuntu 22.04:\n# From Singularity Library: singularity pull library://library/default/ubuntu:22.04 # From Docker Hub: singularity pull docker://ubuntu:22.04  Note that the environment within these containers will be limited, mainly you lose the ability to use the module system. This is expected, as the environment (and the operating system) within the container will be different than the one we are running on our nodes. Even if you are able to get the modules mounted within the container, compatability can not be guatanteed as different libraries versions and packages might be present within the container that the modules were not compiled with.\n NOTE: If you get an error similar to “unexpected HTTP status: 401”, make sure your project on the Container Builder website is set to “Public”.\n HPCC Provided Images In an attempt to preserve some legacy software, we created a CentOS 7 image that integrates with the old CentOS 7 modules. Access to the CentOS 7 container can be granted by running module load centos/7.9. This will set the CENTOS7_SING environment variable, which is the location of the CentOS 7 container image. Usage examples of the CentOS 7 image are in the below sections.\nBuilding Container Images In order to build a custom image, you must use a machine you have sudo access on or use a Remote Builder.\nLocal/Sudo Machine Installing Singularity is outside of the scope for this tutorial. Please see the Installing SingularityCE steps.\nOnce Singularity is installed, you must create a definition (def) file. More details on creating a definition file can be found on the Singularity The Definition File documentation, but a simple definition file of a Debian container that installs Python3 is the following:\nBootStrap: docker From: debian:12 %post apt-get update -y apt-get install -y python3  If the above file was named “debian.def”, then an image could be build using singularity build debian.sif debian.def. This will create an image called debian.sif that can be ran using the sections below.\nRemote Builder After signing up for the remote builder , log in using the steps from singularity remote login.\nAfter logging in, you must create a definition file. We can use the same “debian.sif” file from the “Local/Sudo Machine” section. With the definition file, build the container image using singularity build --remote debian.sif debian.def. After the image has been built and downloaded, you can run it using the sections below.\nInteractive Singularity When running singularity you need to provide the path to a singularity image file. For example, this would be the most basic way to get a shell within your container:\nmodule load singularity singularity pull library://library/default/ubuntu:22.04 singularity shell ubuntu_22.04.sif cat /etc/os-release # Inside Container \u003e PRETTY_NAME=\"Ubuntu 22.04.4 LTS\" \u003e NAME=\"Ubuntu\" \u003e VERSION_ID=\"22.04\"  To run the CentOS 7 container:\nmodule load centos singularity shell $CENTOS7_SING  Additionally, there is a special shortcut for the centos module that allows us to run the above more simply, as:\nmodule load centos centos.sh  While running containers on a head node is technically possible, compute resources are still limited. You can use the following commands to run a job on a compute node:\nUbuntu:\nmodule load singularity singularity pull library://library/default/ubuntu:22.04 singularity shell ubuntu_22.04.sif srun -p epyc --mem=1g -c 4 --time=2:00:00 --pty singularity shell ubuntu_22.04.sif hostname # Inside container \u003e r21 cat /etc/os-release # Inside container \u003e PRETTY_NAME=\"Ubuntu 22.04.4 LTS\" \u003e NAME=\"Ubuntu\" \u003e VERSION_ID=\"22.04  CentOS 7:\nmodule load centos srun -p epyc --mem=1g -c 4 --time=2:00:00 --pty centos.sh cat /etc/os-release # Inside container \u003e PRETTY_NAME=\"CentOS Linux 7 (Core)\" \u003e NAME=\"CentOS Linux\" \u003e VERSION_ID=\"7\"  Non-Interactive Singularity When running singularity non-interactivly, the same basic rules apply. We need a path to our singularity image file as well as a command to run.\nBasics For example, here is the basic syntax:\nmodule load singularity singularity exec /path/to/singularity/image someCommand  Using ubuntu.sif as an example, you can execute an abitraty command like so:\nmodule load singularity singularity pull library://library/default/ubuntu:22.04 singularity exec ubuntu_22.04.sif cat /etc/os-release  And using our CentOS 7 image:\nmodule load singularity singularity exec $CENTOS7_SING cat /etc/redhat-release  Shortcuts Using the centos.sh shortcut that we provide for CentOS 7:\nmodule load centos centos.sh \"cat /etc/redhat-release\"  Here is a more complex example with modules:\nmodule load centos centos.sh \"module load samtools; samtools --help\"  Jobs Here is an example job submitted using an Ubuntu container:\nmodule load singularity singularity pull library://library/default/ubuntu:22.04 sbatch -p epyc --wrap=\"singularity exec ubuntu_22.04.sif cat /etc/os-release; whoami; date\"  Here is an example submitted as a job using the CentOS 7 container:\nmodule load centos sbatch -p epyc --wrap=\"centos.sh 'module load samtools; samtools --help'\"  Variables Here is an example with passing environment variables:\nexport SINGULARITYENV_SOMETHING='stuff' centos.sh 'echo $SOMETHING'   Notice: Just add the SINGULARITYENV_ prefix to pass any varibales to the centos container.\n Enable GPUs First review how to submit a GPU job from here. Then request an interactive GPU job, or embed one of the following within your submission script.\nIn order to enable GPUs within your container you need to add the --nv option to the singularity command:\nmodule load centos singularity exec -nv $CENTOS7_SING cat /etc/redhat-release  However, when using the centos shortcut it is easier to just set the following environment variable then run centos.sh as usual:\nmodule load centos export SINGULARITY_NV=1 centos.sh  Singularity Usecases In addition to using Singularity to run operating system containers (Debian, Ubuntu, CentOS, etc), it can also be used to run certain software on the cluster.\nThe most prominent example of this is AlphaFold. If you are interested in using AlphaFold on the cluster, see the AlphaFold Usage on HPCC page of our documentation. In addition to AlphaFold, we also offer freefem and prymetime through singularity, available by using module load freefem and module load prymetime respectively, and runnable with using singularity shell $FREEFEM_SING and singularity shell $PRYMETIME_SING.\n","excerpt":"What is Singularity In short, Singularity is a program that will allow …","ref":"/manuals/hpc_cluster/singularity/","title":"Singularity Jobs"},{"body":" These pages describe how to use common data transfer software on the UCR HPCC cluster.\n ","excerpt":" These pages describe how to use common data transfer software on the …","ref":"/manuals/hpc_cluster/data/","title":"Data Transfer"},{"body":"The HPC cluster is composed of various hardware.\n","excerpt":"The HPC cluster is composed of various hardware.\n","ref":"/about/hardware/","title":"Hardware"},{"body":"The HPC cluster continues to install various softwares.\n","excerpt":"The HPC cluster continues to install various softwares.\n","ref":"/about/software/","title":"Software"},{"body":"#— #type: docs #linkTitle: Overview slides #title: Overview slides #weight: 6 #—\n ","excerpt":"#— #type: docs #linkTitle: Overview slides #title: Overview slides …","ref":"/about/overview/slides_backup/","title":""},{"body":"","excerpt":"","ref":"/about/","title":"About"},{"body":"","excerpt":"","ref":"/manuals/ext_cloud/aws/","title":"AWS"},{"body":"CyVersae is a NFS funded web platorm that has many pre defined apps. Mainly of the apps are for life sicence and can be very useful.\n NOTE: Not suitable for jobs that require more than 12 CPU cores (ie. MPI).\n Account Go to CyVerse and click on Create Account.\nThere are a few steps, just fill in the fields accordingly and complete the forms.\nAfter you have completed the form and submitted it, CyVerse will send you an email. Within the email will contain a link to set your password.\nData Management There are a few ways to upload/download data, for example you can browse your files from the Discovery Environment. However here we will focus on the command line method, since that is directly supported on the HPC cluster. Please refer to here for additoinal methods.\nFirst you will need to load the icommands tools:\nmodule load icommands/4.1.10  Then you will need to initialize the connection to CyVerse:\niinit  When you run the above command it will ask a few questions about your connection:\n   host name port # username zone password     data.cyverse.org 1247 CyVerse UserID iplant CyVerse Password    Once the iinit command has completed you are now able to list, push, get files and folders on CyVerse directlry from the HPCC.\nUpload The basic format to push files to CyVerse is like so:\niput FileName CyVersePath  For example:\niput hg18.fasta .  Since you automatically start in your home directory from CyVerse, the . will just place the fasta file directly within your home.\nOnce that command completes, you can double check that the the does exist on CyVerse, by listing the files, like so:\nils  The ils and iput command will work with relative and absolute paths.\nDownload The download method is identical to the upload method, just repalce iget instead of iput:\niget hg18.fasta .  The above command will download the hg18.fasta file to your current directory on the cluster.\nJobs Jobs on CyVerse are deployed via apps that you launch through the GUI here. Here is s video explaining how to create a docker image on the CyVerse system as well as configure a custom app to use it.\nPlease contact support for help creating a custom app, or any other questions.\n","excerpt":"CyVersae is a NFS funded web platorm that has many pre defined apps. …","ref":"/manuals/ext_cloud/cyverse/","title":"CyVerse"},{"body":"    Join us in Slack Get help and useful tips!\nRead more …\n   Contributions welcome! We do a Pull Request contributions workflow on GitHub.\nRead more …\n   Follow us on Twitter! For announcement of latest features etc.\nRead more …\n    ","excerpt":"    Join us in Slack Get help and useful tips!\nRead more … …","ref":"/events/","title":"Events"},{"body":"{% include taglogic.html %}\n{% include links.html %}\n","excerpt":"{% include taglogic.html %}\n{% include links.html %}\n","ref":"/tag_events/","title":"Events"},{"body":"  #td-cover-block-0 { background-image: url(\"background.jpg\") }  Research Computing at UCR's HPCC Welcome to the website of the High-Performance Computing Center (HPCC) at UC Riverside.  About  Events  Manuals           HPCC Operation During COVID-19 Crisis  Since the research computing infrastructure of the HPCC is designed to be accessed remotely, we are currently not expecting any major downtimes or restrictions for users due to the campus access restrictions caused by the COVID-19 pandemic.     News \n  This year we were awarded an MRI equipment grant (#2215705) by NSF for the acquisition of a Big Data HPC Cluster in the total amount of $942,829. For details see here.      Mission and Services The High-Performance Computing Center (HPCC) provides state-of-the-art research computing infrastructure and training accessible to all UCR researchers and affiliates at low cost. The main advantage of a shared research computing environment is access to a much larger HPC infrastructure (with thousands of CPUs/GPUs and many PBs of directly attached storage) than what smaller clusters of individual research groups could afford, while also providing a long-term sustainability plan and professional systems administrative support.\n      Join us in Slack Get help and useful tips!\nRead more …\n   Contributions welcome! We do a Pull Request contributions workflow on GitHub.\nRead more …\n   Follow us on Twitter! For announcement of latest features etc.\nRead more …\n    ","excerpt":"  #td-cover-block-0 { background-image: url(\"background.jpg\") } …","ref":"/","title":"HPCC"},{"body":"Rollout of Rocky and DUO: Feb 18 through Mar 17, 2022 Also see email notification sent to users on 18-Feb-2022.  This is to inform users about several important system upgrades the HPCC will be implementing in the next 30 days (starting February 18th, 2022). Importantly, these changes are relevant to all users including those who are accessing the HPCC systems via JupyterHub, RStudio Server, SSH or sFTP/SCP. Thus, please read the instructions below carefully. If anything is unclear or there are questions, please email support@hpcc.ucr.edu for help.\nThe most important change will be the switch from the old CentOS/RHEL 7 platform to the new Rocky/RHEL 8. We anticipate to finalize this upgrade on March 17th, 2022. This gives users a 30-day transition period to log into the new Rocky/RHEL 8 platform and test whether the software and other resouces they are using for their research are properly working. It is important to understand the deployment of Rocky 8 is a major upgrade that requires the systems administrators recompiling most software from the old system onto the new system.\nTo avoid unnecessary extra downtimes, we are also elevating with this upgrade our security standards by adopting UCR’s DUO multi factor authentication system. This is important to prevent intrusions and comply with UC-wide IT standards.\nOperating System As mentioned above, the biggest change is that we are upgrading the OS from CentOS/RHEL 7 to Rocky/RHEL 8. Rocky Linux is the community equivalent and identical to RHEL (similar to how CentOS was). Currently, pigeon is the only head/login node that is linked to the new Rocky/RHEL 8 platform. To check which platform you are on, you can run the platform command.\nThe upgrade from RHEL 7 to RHEL 8 will result in the following user-facing changes.\nRocky Linux is an open-source enterprise operating system designed to be 100% bug-for-bug compatible with Red Hat Enterprise Linux -- Passwords User passwords will be expired on the current CentOS/RHEL 7 platform and users will need to reset their password during the next login. During this login, users need to provide their old password twice. The first time is to authenticate as cluster user (login password), and the second time to authorize the password reset (kerberos password). After this users will be prompted to provide their new password .\nWhen logging into the new Rocky/RHEL 8 platform as outlined here, users also need to configure DUO if using a password, or alternatively create an SSH key pair. If a user is new to DUO, the instructions from UCR’s ITS are here. Users accessing the cluster via SSH key pairs are strongly encouraged to also reset their password upon login by following the instructions here. Additional information about login related topics are provided on this manual page.\nExternal Users External users are unlikely to have a UCR NetID required for DUO. This includes users with HPCC cluster and restricted data transfer accounts. Those users want to access the cluster via SSH keys. This is both convenient (no need to type a password anymore) and secure. Please refer to our SSH keys manual for detailed instructions of configuring SSH key-based access. After creating an ssh key pair, users will need to email their public SSH key to support@hpcc.ucr.edu so that the systems admin can add their public SSH key to the corresponding user account.\nNote, the following instructions are only relevant for users who perform computations on our cluster(s). Users who are using our systems exclusively for data transfers can ignore them.\nSoftware All software installation requests will be restricted to the new cluster. New software may be installed under a different version, or may no longer be installed as a module, or may not be installed at all. Run the following command to list currently available modules:\nmodule avail  Or you can search for a specific software (ie. vim) like so:\nmodule avail vim  which vim  Since the new platform is built using GCC 8.5, then all previous compiled software must be re-compiled on the new platform. If you cannot find what you are looking for send an installation request to support@hpcc.ucr.edu.\nCompatibility mode (singularity) can be used to run the older CentOS 7 modules, however this may not work in all cases. Please refer to our Singularity Examples for more information.\nBash We officially support bash, even though other shells may work they have not been tested under the new Rocky/RHEL 8 platform.\nWhen logging in under a bash shell, some errors/warnings may be visible. The most common message being that a module cannot be loaded.\nCheck if the module (ie. vim) is available with the following:\nmodule avail vim  If there is no output, then the module is not currently available. Either remove the module load vim from your ~/.bashrc and/or ~/.bash_profile files, or request that it be installed.\nIf the software is available, ensure a proper check is in place around loading modules within your ~/.bashrc and/or ~/.bash_profile files.\nFor example:\nif [[ \"$(type -t module)\" == \"function\" ]]; then module load tmux module load neovim fi  It may also help to keep ~/.bashrc free of unnecessary bloat and only add customized changes to ~/.bash_profile. Also keep in mind that when running jobs with just /bin/bash the ~/.bashrc file is loaded. However, adding the lower case L to a job’s interpreter, as in /bin/bash -l this will load the ~/.bash_profile file. This can be useful since it provides flexibility to initialize a default job shell or a customized job shell.\nSlurm A newer version of Slurm is being used on the new Rocky/RHEL 8 platform, however very little is different from the previous version. All previous job submission scripts and commands/flags should still be compatible.\nDuring the transition period From the old CentOS/RHEL 7 platform any Slurm jobs scheduled to start after March 17th will never start. Please check your jobs and ensure that they run before this time frame. You can check your start times with the following command:\nsqueue --start -u $USER  Be sure to move to the newer Rocky/RHEL 8 platform as soon as possible.\nUser-facing Changes Implemented on 23-Aug-2019 Domain Names The old domain names biocluster.ucr.edu and bioinfo.ucr.edu have finally been discontinued. As a result, users need to use the new hpcc.ucr.edu name for the following services:\n ssh/scp/ftp/http: cluster.hpcc.ucr.edu instead of biocluster.ucr.edu RStudio Server: rstudio.hpcc.ucr.edu instead of rstudio.bioinfo.ucr.edu Jupyter: jupyter.hpcc.ucr.edu instead of jupyter.bioinfo.ucr.edu Email Support: support@hpcc.ucr.edu instead of support@biocluster.ucr.edu  In addition, URLs containing biocluster.ucr.edu need to be updated to cluster.hpcc.ucr.edu.\nPassword Reset After the upgrade on Aug 23, 2019, all HPCC users have been emailed a temporary password with instructions how to change it. This email was sent to the address we have on file for each user. In case you missed the corresponding email notification and/or you are not able to log into the cluster, please email us at support@hpcc.ucr.edu to receive a new password.\nUpdated OpenMPI If you have compiled or use software that was compiled with OpenMPI, then it will need to be recompiled. If you are running into any issues, please email us at support@hpcc.ucr.edu.\n","excerpt":"Rollout of Rocky and DUO: Feb 18 through Mar 17, 2022 Also see email …","ref":"/changes/","title":"Important Changes for HPCC Users"},{"body":"This page provides URLs to related resources  RED-UCR Data Science Center FIELDS Program Institute of Integrative Genome Biology  ","excerpt":"This page provides URLs to related resources  RED-UCR Data Science …","ref":"/links/","title":"Related Links"},{"body":"Requirements A user account is required to access HPCC’s research computing infrastructure. How to obtain a user account is described here. A new user’s login credentials (username and password) are emailed to the address provided in the corresponding account request. For external users it is important to know that the below Password+DUO multifactor authenication system is only available to UCR users. Thus, external users have to use the alternative SSH Key based authentication method, which is both secure and convenient to use.\nThe login instructions of this page are organized in three main sections:\n A. SSH Login via Terminal B. Web-based Access C. Data Sharing Access  A. SSH Login from Terminal Terminal-based login is the most feature-rich method for accessing HPC resources. Web-based alternatives via JupyterHub and RStudio Server are also provided. To access the HPCC cluster with the standard ssh protocol, users want to follow steps 1-3. Only step 1 is required after setting up ssh key based access.\n1. Type the following ssh login command from a terminal application, where \u003cusername\u003e needs to be replaced by the actual account name of a user. The \u003c\u003e characters indicate a placeholder and need to be removed. Next, press enter to execute the ssh command.\nssh -X \u003cusername\u003e@cluster.hpcc.ucr.edu  The -X argument enables X11 support, which is required for opening GUI applications on remote systems.\n2. Type your password and hit enter. Note, when typing the password the cursor will not move and nothing is printed to the screen. If ssh key access is enabled, both the password and Duo steps will be skipped automatically during the log in process.\n3. Follow the Duo multifactor authenication instructions printed to the screen. As external users do not have access to UCR’s Duo system, they can only log in via the alternative ssh key method. How to enable ssh keys is described here. Note, Duo will be bypassed if ssh key based login is enabled. This can be more conveniet than Duo when accessing the cluster frequently.\nIf the login is performed via a GUI application, which is an option in MobaXterm, then one can provide the same login information given under the above ssh commad in the corresponding fields of the login window as follows:\n Host name: cluster.hpcc.ucr.edu User name: … Password: …  Importantly, after the first login into a new account (or a password reset), users need to change their password with the passwd command and then follow the on-screen instructions. This requires to enter the current password once and the new password twice. New passwords need to be at least 8 characters long and meet at least 3 of the following requirments: lowercase character, uppercase character, number, and punctuation character.\nWhat to do if password/Duo is not working? If this happens then most often the login is blocked because a password was typed too many times incorrectly, or not changed after the first login (see above). To correct this, please request a password reset by emailing support@hpcc.ucr.edu. Remember, password/Duo based access is only possible if a user’s UCR NetID matches the corresponding HPCC username. If this is not the case then UCR users can request to change their HPCC user account name to their NetID or use the ssh key based access method.\nTerminal Options Various ssh terminal applications are available for all major operating systems. Examples include:\n macOS: built-in macOS Terminal or iTerm2 Windows: MobaXterm is a very feature rich terminal option for Windows users. Putty is an alternative, but outdated and not recommended anymore. Here are annimated usage introductions for MobaXterm. Linux: a wide range of Terminal applications is available for Linux. Usually, the default terminal available on a Linux distribution will be sufficient. ChromeOS: after enabling Linux apps on Chromebooks one can use the default terminal that is similar to those on Linux systems.  Remote Graphics Support X11 support is included in the terminal applications of most OSs. This includes MobaXterm on Windows, Linux and ChromeOS terminals. On macOS systems, users need to run XQuartz in the background to enable X11 graphics display support. XQuartz can be downloaded from here (also see this video here). Note, XQuartz is optional if remote graphics support is not needed.\nAdditional Authentication Details In early 2022 the HPCC adopted a more secure authentication method for logging into its clusters. Passwords alone will no longer be allowed for SSH or file tansfer protocols. Instead Password+DUO or SSH Keys will be required. Because Password+DUO authentication requires a UCR NetID, this access method is only available to UCR users for both ssh and file transfer protocols (e.g. sFTP or SCP). Thus, external users need to use the alternative ssh key method. To enable ssh key access, external users need to email their public key to support@hpcc.ucr.edu (see below for details).\nPassword+Duo Users familiar with UCR’s Duo system can log in to HPCC’s clusters by following the on screen instructions during the ssh login. For new users, instructions for UCR’s Duo Multifactor Authenication system are available in this PDF and on UCR’s MyAccount page. Importantly, the login via the Password+DUO method will only work if a user’s NetID matches the username of the corresponding HPCC account. If this is not the case then the HPCC username can be changed to a user’s NetID. This change can be initiated by emailing suppor@hpcc.ucr.edu. As mentioned above, external users will not have access to UCR’s Duo system, and thus have to use the alternative ssh key access method to log in to HPCC’s resources.\nSSH Keys SSH keys are an access credential used by the Secure Shell (SSH) protocol. For this a key pair is created comprised of a private key and a public key. The private key remains on a user’s system and should not be shared. The public key will be uploaded to the remote system, here ~/.ssh directory of a user’s account on the HPCC cluster. SSH key based access works analogous to how a key and a lock are used in the real world, where one uniquely fits into the other. Access can only be established if the private key on a user’s system fits the public key on the remote system.\nThe following introduces how to create an ssh key pair from the command-line in a terminal and upload the public key to the remote system. The latter upload will only work if a user can access the remote system, e.g. via temporary Password+DUO access. User without this option have to email their public ssh key to suppor@hpcc.ucr.edu so that the systems administrator can upload the public key for them. Additional details on ssh key generation and uploads are provided here. This includes GUI based based options. However, we highly recommend to use the command-line option on all OSs which are much more straigthforward to use, including MobaXterm on Windows.\n(a) SSH Key Creation An ssh key pair can be created with the following commands in a terminal application of all major operating systems, including Windows, macOS, Linux and ChromeOS. Note, the following commands need to be executed on a user’s local computer (not the cluster).\n1. Create SSH directory\nmkdir -p ~/.ssh  2. Create key pair (private and public)\nssh-keygen -t rsa -f ~/.ssh/id_rsa  Follow the prompts and complete the processes. You might be prompted to enter a passcode/passphrase for this key generation, enter one if you like it leave it blank for no passcode. This passcode is unrelated to your cluster password and HPCC system administrators have no control over this. If you forget this passcode then the SSH key must be renegerated, there’s no way to recover it. Once the command has completed, one will find two files in the ~/.ssh directory of a user account.\n3. Inspect ~/.ssh directory\nls ~/.ssh/ id_rsa id_rsa.pub  (b) SSH Key Upload The id_rsa and id_rsa.pub files are the private and public keys, respectively. The private key should never be shared with anyone. This means it should not be emailed or uploaded to a remote system. Only the public key will be uploaded to the remote system, here HPCC user account. Specifically, the public key will be stored in a file called authorized_keys under a directory called ~/.ssh. If not present yet both need to be created. Note, ~/ refers to the higest (root) level of a user account.\n1. Upload of first public ssh key\nIf the authorized_keys doesn’t exist yet, the following scp command can be run from a user’s system. This command will create the ~/.ssh/authorized_keys file and populate it with the public key.\nscp .ssh/id_rsa.pub username@cluster.hpcc.ucr.edu:.ssh/authorized_keys  2. Upload of subsequent public ssh keys\nIf the authorized_keys file already exists, one can append the new public key as follows.\nscp .ssh/id_rsa.pub username@cluster.hpcc.ucr.edu:tmpkey \u0026\u0026 ssh username@cluster.hpcc.ucr.edu \"cat tmpkey \u003e\u003e ~/.ssh/authorized_keys \u0026\u0026 rm tmpkey\"  3. Check ssh key based access\nTo test whether ssh key based access is functional, the following log in should work without asking for a password. However, it may ask for a passphrase if the ssh key pair was created this way.\nssh \u003cusername\u003e@cluster.hpcc.ucr.edu  (c) Additional Details on SSH Keys See here.\nB. Web-based Access Web-based HPCC cluster access is provided via OnDemand, allowing users to launch JupyterHub, RStudio, VSCode, Desktop Sessions, and more all from their web browser. All jobs will run on compute nodes, meaning that resources are dedicated to your instance and provide the best performance possible for these applications.\nAlternatively we host instances of RStudio Server and JupyterHub, though resources are shared for all users accessing them, so performance is likely to be worse than what you would get through OnDemand. For load balancing RStudio Server has two instances: RStudio Server 1 and RStudio Server 2. If one of these services is slow or not available, users want to choose the alternative instance. A much more efficient method for using RStudio Server are custom compute node instances using srun. This option is described here. Related options for Jupyter are outlined here.\nUsers with an HPCC cluster account can access them with the same login credential used for ssh access.\nC. Data Sharing Access Users accessing HPCC’s infrastructure mainly for file transfers and data sharing want to follow the corresponding instructions on the Data Sharing page. Users with accounts restricted to data sharing have to provide their public SSH Key when accesing the system for the first time by following the instructions in the SSH Key section above.\n","excerpt":"Requirements A user account is required to access HPCC’s research …","ref":"/manuals/access/login/","title":"Login"},{"body":"","excerpt":"","ref":"/manuals/","title":"Manuals"},{"body":"","excerpt":"","ref":"/news/","title":"News"},{"body":" CAVEATS:\nA fair amount of resources must be manually calculated from the currently available.\nAlso, it seems like a beta feature would repalce what I did here:\nhttps://kubernetes.io/blog/2021/04/19/introducing-indexed-jobs/\n The PRP (Pacific Research Platform) is a NSF funded Kubernetes infrastructure and therefore requires the use of a Kubernetes interface. The command line Kubernetes interface kubectl is what is used from the HPC cluster.\nAccount Follow the guides on the PRP website posted here. There is a link called Get access, depending on your role, you can request a admin account or a user account.\nInstall Once you have an account, and you have read all of the docs here, you can proceed to install. Submitting jobs, checking states, getting logs as well as interactive sessions can all be done from the HPCC.\nHowever, running these operations directly from your laptop/workstation can be faster. In order to run these actions locally you will need to install kubectl on your local laptop/workstation.\nYou can install kubectl via conda, like so:\nconda create -n kube -c anaconda-platform kubectl  If you do not yet have conda installed, follow these instructions.\nConfig For kubectl to function, it requires your config file provdied by PRP. In order to get the PRP Kubernetes config file, do the following:\n Visit Nautilus Portal Click on Login in upper right coner. Login using CILogon credentials (A.K.A UCR netID). Once authenticated, click on the Get config in the upper right conner. This takes a while to dynamically generate, just wait and eventually your browser will present you a download prompt. Place this file in your ~/.kube directory.  Next set the namespace, or else you will have to append the -n ucr-hpcc flag to every Kubernetes command. You may be under the ucr-hpcc namespace if you are testing, otherwise you should have your own namespace:\nkubectl config set-context nautilus --namespace=ucr-hpcc  Usage Here is an example of an array style job on utilizing redis to track job numbers, and the dockerfile stored within the PRP GitLab repository.\n  First copy scripts/* files from the repo to your code base. Make sure that your analysis workflow is started within the worker.py script.\n  Create redis service and deployment\n  kubectl create -f hpcc-redis.yml  Log into Redis pod and manually add items  # Get into pod kubectl exec -it redis-master -- /bin/bash # Add 10 items to list \"job2\" echo lpush job2 {1..10} | redis-cli -h redis --pipe # Print all items in \"job2\" redis-cli -h redis lrange job2 0 -1  Submit job  kubectl create -f hpcc-job.yml  Egress The PRP has fail2ban blocking rapid SSH connections, so copying files within a loop would fail. It is best to try and copy all needed files with a single rsync command, like so:\nnohup rsync -rvP --include='*/spades.log.gz' --include='*/scaffolds.fasta' --exclude='*/*' /output/ cluster.hpcc.ucr.edu:~/output \u0026\u003e rsync_spades.log  The above rsync command looks into the sub-directories within /output and will copy only the spades.log.gz and scaffolds.fasta files from each onto the HPCC cluster.\nTrouble From pod list, check log:\nkubectl logs hpcc-pod  Jobs and pods will expire after 1 week, however you can alter this with the following:\nttlSecondsAfterFinished=604800  Check on the pod details:\nkubectl describe pod hpcc-pod  Delete job if you want to rerun, this will also delete associated pods:\nkubectl delete pods hpcc-pod  For updating your repo to the lastest HPCC changes, you can sync like so:\n# Add upstream git remote add upstream git://github.com/ORIGINAL-DEV-USERNAME/REPO-YOU-FORKED-FROM.git # Get branchs git fetch upstream # Sync local files with master branch git pull upstream master  Links  Help - https://element.nrp-nautilus.io Resources - https://nautilus.optiputer.net/resources Monitoring - https://grafana.nautilus.optiputer.net  ","excerpt":" CAVEATS:\nA fair amount of resources must be manually calculated from …","ref":"/manuals/ext_cloud/prp/","title":"PRP"},{"body":"","excerpt":"","ref":"/search/","title":"Search Results"},{"body":" Note: Neovim has replaced vim on the cluster, however most (if not all) items here still apply. Load Neovim via the module system, like so module load neovim. It may also be useful to alias nvim to vim, like so alias vim=nvim. Thia alias can be added to your ~/.bashrc file for convenience.\n Vim Manual This is an extensive overview of vim features and operations.\nBasics vim \u003cmy_file_name\u003e # open/create file with vim\nOnce you are in Vim the most important commands are i , : and ESC. The i key brings you into the insert mode for typing. ESC brings you out of there. And the : key starts the command mode at the bottom of the screen. In the following text, all commands starting with : need to be typed in the command mode. All other commands are typed in the normal mode after hitting the ESC key.\nModifiers i # INSERT MODE ESC # NORMAL (NON-EDITING) MODE : # Commands start with ':' :w # Save command; if you are in editing mode you have to hit ESC first!! :q # Quit file, don't save :q! # Exits WITHOUT saving any changes you have made :wq # Save and quit R # Replace MODE r # Replace only one character under cursor q: # History of commands (from NORMAL MODE!), to reexecute one of them, select and hit enter! :w new_filename # Saves into new file :#,#w new_filename # Saves specific lines (#,#) to new file :# # Go to specified line number  Moving Around $ # Moves cursor to end of line A # Same as $, but switches to insert mode 0 # Zero moves cursor to beginning of line CTRL-g # Shows file name and current line you are on SHIFT-G # Brings you to bottom of file  Lines :set wrap # Wrap lines around the screen if too long :set nowrap # No line wrapping :set number # Shows line numbers :set nonumber # No line numbers  Multiple Files vim -o *.txt # Opens many files at once and displays them with horizontal # Split, '-O' does vertical split vim *.txt # Opens many files at once; ':n' switches between files  :wall or :qall # Write or quit all open files :args *.txt # Places all the relevant files in the argument list :all # Splits all files in the argument list (buffer) horizontally CTRL-w # Switch between windows :split # Shows same file in two windows :split \u003cfile-to-open\u003e # Opens second file in new window :vsplit # Splits windows vertically, very useful for tables, \":set scrollbind\" let's you scroll all open windows simultaneously :close # Closes current window :only # Closes all windows except current one  Spell Checking :set spell # Turns on spell checking :set nospell # Turns spell checking off :! dict \u003cword\u003e # Meaning of word :! wn 'word' -over # Synonyms of word  Syntax Highlighting :set filetype=perl # Turns on syntax coloring for a chosen programming language. :syn on # Turns syntax highlighting on :syn off # Turns syntax highlighting off  Undo and Redo u # Undo last command U # Undo all changes on current line CTRL-R # Redo one change which was undone  Deleting x # Deletes what is under cursor dw # Deletes from curser to end of word including the space de # Deletes from curser to end of word NOT including the space cw # Deletes rest of word and lets you then insert, hit ESC to continue with NORMAL mode c$ # Deletes rest of line and lets you then insert, hit ESC to continue with with NORMAL mode d$ # Deletes from cursor to the end of the line dd # Deletes entire line 2dd # Deletes next two lines, continues: 3dd, 4dd and so on.  Copy and Paste yy # Copies line, for copying several lines do 2yy, 3yy and so on p # Pastes clipboard behind cursor  Search /my_pattern # Searches for my_pattern downwards, type n for next match ?my_pattern # Searches for my_pattern upwards, type n for next match :set ic # Switches to ignore case search (case insensitive) :set hls # Switches to highlight search (highlights search hits)  Replacements Great intro: A Tao of Regular Expressions\nQuick reference to some replacement techniques:\n:s/old_pat/new_pat/ # Replaces first occurrence in a line :s/old_pat/new_pat/g # Replaces all occurrence in a line :s/old_pat/new_pat/gc # Add 'c' to ask for confirmation :#,#s/old_pat/new_pat/g # Replaces all occurrence between line numbers: #,# :%s/old_pat/new_pat/g # Replaces all occurrence in file :%s/\\(pattern1\\)\\(pattern2\\)/\\1test\\2/g # Regular expression to insert, you need here '\\' in front of parentheses (\u003c# Perl) :%s/\\(pattern.*\\)/\\1 my_tag/g # Appends something to line containing pattern (\u003c# .+ from Perl is .* in VIM) :%s/\\(pattern\\)\\(.*\\)/\\1/g # Removes everything in lines after pattern :%s/\\(At\\dg\\d\\d\\d\\d\\d\\.\\d\\)\\(.*\\)/\\1\\t\\2/g # Inserts tabs between At1g12345.1 and Description :%s/\\n/new_pattern/g # Replaces return signs :%s/pattern/\\r/g # Replace pattern with return signs!! :%s/\\(\\n\\)/\\1\\1/g # Insert additional return signs :%s/\\(^At\\dg\\d\\d\\d\\d\\d.\\d\\t.\\{-}\\t.\\{-}\\t.\\{-}\\t.\\{-}\\t\\).\\{-}\\t/\\1/g # Replaces content between 5th and 6th tab (5th column), '{-}' turns off 'greedy' behavior :#,#s/\\( \\{-} \\|\\.\\|\\n\\)/\\1/g # Performs simple word count in specified range of text :%s/\\(E\\{6,\\}\\)/\u003cfont color=\"green\"\u003e\\1\u003c\\/font\u003e/g # Highlight pattern in html colors, here highlighting of \u003e= 6 occurences of Es :%s/\\([A-Z]\\)/\\l\\1/g # Change uppercase to lowercase, '%s/\\([A-Z]\\)/\\u\\1/g' does the opposite  Uses ‘global’ command to apply replace function only on those lines that match a certain pattern. The ‘copy $’ command after the pipe ‘|’ prints all matching lines at the end of the file.\n:g/my_pattern/ s/\\([A-Z]\\)/\\l\\1/g | copy $  Command ‘args’ places all relevant files in the argument list (buffer); ‘all’ displays each file in separate split window; command ‘argdo’ applies replacement to all files in argument list (buffer); flag ‘e’ is necessary to avoid stop at error messages for files with no matches; command ‘update’ saves all changes to files that were updated.\n:args *.txt | all | argdo %s/\\old_pat/new_pat/ge | update  Utilities   Matching Parentheses\n Place cursor on (, [ or { and type % # cursor moves to matching parentheses    Printing and Inserting Files\n :ha # Prints entire file :#,#ha # Prints specified lines: #,# :r \u003cfilename\u003e # Inserts content of specified file after cursor    Convert Text File to HTML Format\n :runtime! syntax/2html.vim # Run this command with open file in Vim    Shell Commands in Vim\n :!\u003cSHELL_COMMAND\u003e \u003cENTER\u003e # Executes any shell command, hit \u003center\u003e to return :sh # Switches window to shell, 'exit' switches back to vim    Using Vim as Table Editor\n v starts visual mode for selecting characters V starts visual mode for selecting lines` CTRL-V starts visual mode for selecting blocks (use CTRL-q in gVim under Windows). This allows column-wise selections and operations like inserting and deleting columns. To restrict substitute commands to a column, one can select it and switch to the command-line by typing :. After this the substitution syntax for a selected block looks like this: '\u003c,'\u003es///. :set scrollbind starts simultaneous scrolling of ‘vsplitted’ files. To set to horizontal binding of files, use command :set scrollopt=hor (after first one). Run all these commands before the :split command. :AlignCtrl I= \\t then :%Align This allows to align tables by column separators (here ‘\\t’) when the Align utility from Charles Campbell’s is installed. To sort table rows by selected lines or block, perform the visual select and then hit F3 key. The rest is interactive. To enable this function, one has to include in the .vimrc file the Vim sort script from Gerald Lai.    Settings The default settings in Vim are controlled by the .vimrc file in your home directory.\n see last chapter of vimtutor (start from shell) useful .vimrc sample when vim starts to respond very slowly then one may need to delete the .viminf* files in home directory  Help  Online Help  Find help on the web. Google will find answers to most questions on vi and vim (try searching for both terms). Purdue University Vi Tutorial Animated Vim Tutorial: https://linuxconfig.org/vim-tutorial Useful list of vim commands:  Vim Commands Cheat Sheet VimCard      You can run a tutor from the command Line:\nvimtutor # Open vim tutorial from shell, \":q\" to quit  You can also get help from within Vim:\n:help # opens help within vim, hit :q to get back to your file :help \u003ctopic\u003e # opens help on specified topic :help_topic| CTRL-] # when you are in help this command opens help topic specified between |...|, # CTRL-t brings you back to last topic :help \u003ctopic\u003e CTRL-D # gives list of help topics that contain key word : \u003cup-down keys\u003e # like in shell you get recent commands!!!!  ","excerpt":" Note: Neovim has replaced vim on the cluster, however most (if not …","ref":"/manuals/linux_basics/vim/","title":"Linux Basics - Vim Manual"},{"body":"If you require large amounts of resources (ie. 1000s of CPUs, or 100s of GPUs) then access to the computing resources at the NSF funded XSEDE might be a good option.\nAccount To create an account, visit the XSEDE Portal.\nProposal As stated previously, writting a propoasl and then getting it approved is required to gain access to XSEDE. Instructions on how to do this are outlined here.\nData Management There are several methods used to transfer data to and from XSEDE resources, they are outlined here\nJobs For submitting jobs, XSEDE also supports Slurm, which is similar to what we already use on the HPC cluster.\nExample on how to submit Slurm style jobs are described here\nUCR Campus Champion You can contact Jordan Hayes for additional information regarding XSEDE and how to gain access.\n","excerpt":"If you require large amounts of resources (ie. 1000s of CPUs, or 100s …","ref":"/manuals/ext_cloud/xsede/","title":"XSEDE"}]