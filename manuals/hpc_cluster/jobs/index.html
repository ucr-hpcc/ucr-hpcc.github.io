<!doctype html><html lang=en class=no-js><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=generator content="Hugo 0.83.1"><meta name=ROBOTS content="INDEX, FOLLOW"><link rel="shortcut icon" href=../../../favicons/favicon.ico><link rel=apple-touch-icon href=../../../favicons/apple-touch-icon-180x180.png sizes=180x180><link rel=icon type=image/png href=../../../favicons/favicon-16x16.png sizes=16x16><link rel=icon type=image/png href=../../../favicons/favicon-32x32.png sizes=32x32><link rel=icon type=image/png href=../../../favicons/android-36x36.png sizes=36x36><link rel=icon type=image/png href=../../../favicons/android-48x48.png sizes=48x48><link rel=icon type=image/png href=../../../favicons/android-72x72.png sizes=72x72><link rel=icon type=image/png href=../../../favicons/android-96x96.png sizes=96x96><link rel=icon type=image/png href=../../../favicons/android-144x144.png sizes=144x144><link rel=icon type=image/png href=../../../favicons/android-192x192.png sizes=192x192><title>Managing Jobs | HPCC</title><meta property="og:title" content="Managing Jobs"><meta property="og:description" content="What is a Job? Submitting and managing jobs is at the heart of using the cluster. A &lsquo;job&rsquo; refers to the script, pipeline or experiment that you run on the nodes in the cluster.
Partitions Jobs are submitted to so-called partitions (or queues). Each partition is a group of nodes, often with similar hardware specifications (e.g. CPU or RAM configurations). The quota policies applying to each partitions are outlined on the Queue Policies page."><meta property="og:type" content="article"><meta property="og:url" content="https://hpcc.ucr.edu/manuals/hpc_cluster/jobs/"><meta property="article:section" content="Manuals"><meta property="article:modified_time" content="2025-06-17T18:51:20-07:00"><meta property="og:site_name" content="HPCC"><meta itemprop=name content="Managing Jobs"><meta itemprop=description content="What is a Job? Submitting and managing jobs is at the heart of using the cluster. A &lsquo;job&rsquo; refers to the script, pipeline or experiment that you run on the nodes in the cluster.
Partitions Jobs are submitted to so-called partitions (or queues). Each partition is a group of nodes, often with similar hardware specifications (e.g. CPU or RAM configurations). The quota policies applying to each partitions are outlined on the Queue Policies page."><meta itemprop=dateModified content="2025-06-17T18:51:20-07:00"><meta itemprop=wordCount content="3917"><meta itemprop=keywords content><meta name=twitter:card content="summary"><meta name=twitter:title content="Managing Jobs"><meta name=twitter:description content="What is a Job? Submitting and managing jobs is at the heart of using the cluster. A &lsquo;job&rsquo; refers to the script, pipeline or experiment that you run on the nodes in the cluster.
Partitions Jobs are submitted to so-called partitions (or queues). Each partition is a group of nodes, often with similar hardware specifications (e.g. CPU or RAM configurations). The quota policies applying to each partitions are outlined on the Queue Policies page."><link rel=preload href=../../../scss/main.min.398442d9d5d4e5f14be4a4e8e9b450b7e55d46418388cfab16ab414422fad5fa.css as=style><link href=../../../scss/main.min.398442d9d5d4e5f14be4a4e8e9b450b7e55d46418388cfab16ab414422fad5fa.css rel=stylesheet integrity><script src=https://code.jquery.com/jquery-3.5.1.min.js integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0=" crossorigin=anonymous></script><script src=https://unpkg.com/lunr@2.3.8/lunr.min.js integrity=sha384-vRQ9bDyE0Wnu+lMfm57BlYLO0/XauFuKpVsZPs7KEDwYKktWi5+Kz3MP8++DFlRY crossorigin=anonymous></script><link rel=stylesheet href=../../../css/prism.css><script src=../../../js/driver0.9.8.min.js></script><link rel=stylesheet href=../../../css/driver0.9.8.min.css><link rel=stylesheet href=../../../css/site.css><script src=../../../js/site.js></script></head><body class="td-page line-numbers"><header><nav class="js-navbar-scroll navbar navbar-expand navbar-dark flex-column flex-md-row td-navbar"><a class=navbar-brand href=../../../><span class=navbar-logo><svg height="16" id="Layer_1" style="enable-background:new 0 0 16 16" viewBox="0 0 16 16" width="16" sodipodi:docname="126572_home_house_icon.svg" inkscape:export-filename="/home/jhayes/126572_home_house_icon.png" inkscape:export-xdpi="96" inkscape:export-ydpi="96" inkscape:version="1.1 (c4e8f9ed74, 2021-05-24)" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape" xmlns:sodipodi="http://sodipodi.sourceforge.net/DTD/sodipodi-0.dtd" xmlns="http://www.w3.org/2000/svg" xmlns:svg="http://www.w3.org/2000/svg"><defs id="defs7"/><sodipodi:namedview id="namedview5" pagecolor="#ffffff" bordercolor="#666666" borderopacity="1" inkscape:pageshadow="2" inkscape:pageopacity="0" inkscape:pagecheckerboard="0" showgrid="false" inkscape:zoom="49.0625" inkscape:cx="3.5464968" inkscape:cy="7.9694268" inkscape:window-width="3440" inkscape:window-height="1391" inkscape:window-x="0" inkscape:window-y="24" inkscape:window-maximized="1" inkscape:current-layer="Layer_1"/><path style="display:inline;fill:#fff;fill-opacity:1;stroke-width:.0203822" d="m2.8659775 15.977961c-.3936273-.051-.7525483-.389889-.8358103-.789155-.015862-.07606-.022524-1.009027-.022524-3.15414V8.988535H1.4412249C.82810729 8.988535.73920188 8.977312.53578892 8.8742347.39201211 8.8013774.21162731 8.6243834.13540635 8.4813782.01718088 8.2595644-.01861304 7.986667.04066723 7.7590746c.03869392-.148556.08395752-.2367427.20614877-.4016385C.36550378 7.1972684 1.9544118 5.5981695 5.0547771 2.5186377L7.3783439.21068447 7.5602468.12062886 7.7421497.03057325h.2563137c.246189.0.2617641.0025817.3942926.06535805.075888.03594693.1905382.10669563.254777.15721933.064239.0505237.6212568.59095827 1.2378173 1.20096567l1.1210187 1.1091045.01139-.4000817c.01-.3510397.01694-.413483.05667-.5094082.08521-.2057341.220395-.3661097.415434-.4928535.209421-.1360904.280363-.1452781 1.065556-.1380008l.692994.00642.159642.076227c.292895.1398524.506525.4124209.562217.7173255.01479.080962.02209.7172189.02209 1.9247442v1.8038031l.83131.8370395c.970032.9767169 1.061962 1.0876674 1.135666 1.3706362.134824.5176265-.189416 1.0615314-.71432 1.1982515-.09368.024401-.243383.031209-.686238.031209h-.566418v3.0331994c0 2.090488-.0069 3.070788-.02209 3.15414-.07531.412343-.432865.748345-.854561.803061-.08611.01117-.794528.02008-1.62526.02044L10.017834 16 10.017431 13.498089C10.01718 11.939735 10.009334 10.942371 9.9966286 10.853503 9.9447492 10.490634 9.6889196 10.179327 9.3299104 10.042205 9.2140078 9.9979369 9.1996048 9.9974522 8 9.9974522s-1.2140078 4851e-7-1.3299104.044753c-.3590092.137122-.6148388.448429-.6667182.811298-.012705.08887-.02055 1.086232-.020802 2.644586L5.9821656 16 4.489172 15.997416c-.8211465-.0014-1.551584-.01018-1.6231945-.01945z" id="path58637" sodipodi:insensitive="true"/></svg></span><span class=font-weight-bold>HPCC</span></a>
<row class=header-switch><div class="custom-control custom-switch"><input type=checkbox class=custom-control-input id=sidebarSwitch checked>
<label class=custom-control-label for=sidebarSwitch>sidebar</label></div><div class="custom-control custom-switch"><input type=checkbox class=custom-control-input id=tocSwitch checked>
<label class=custom-control-label for=tocSwitch>toc</label></div></row><div class="td-navbar-nav-scroll ml-md-auto" id=main_navbar><ul class="navbar-nav mt-2 mt-lg-0"><li class="nav-item mr-4 mb-2 mb-lg-0" style=position:relative><i class="fas fa-info-circle btn-primary"></i><a class="nav-link dropdown-toggle" href=../../../about/ id=navbarDropdown role=button>
About</a><div class=dropdown-menu aria-labelledby=navbarDropdownMenuLink><a class=dropdown-item href=../../../about/overview/ style=padding-left:1rem;font-weight:calc(3/3*500)>Overview</a>
<a class=dropdown-item href=../../../about/overview/introduction/ style=padding-left:2rem;font-weight:calc(3/4*500)>Introduction</a>
<a class=dropdown-item href=../../../about/overview/access/ style=padding-left:2rem;font-weight:calc(3/4*500)>Access</a>
<a class=dropdown-item href=../../../about/overview/activity/ style=padding-left:2rem;font-weight:calc(3/4*500)>Activity Report</a>
<a class=dropdown-item href=../../../about/overview/rates/ style=padding-left:2rem;font-weight:calc(3/4*500)>Rates</a>
<a class=dropdown-item href=../../../about/overview/contact/ style=padding-left:2rem;font-weight:calc(3/4*500)>Contact</a>
<a class=dropdown-item href=../../../about/overview/acknowledgement/ style=padding-left:2rem;font-weight:calc(3/4*500)>Acknowledgement</a>
<a class=dropdown-item href=../../../about/hardware/ style=padding-left:1rem;font-weight:calc(3/3*500)>Hardware</a>
<a class=dropdown-item href=../../../about/hardware/overview style=padding-left:2rem;font-weight:calc(3/4*500)>Overview</a>
<a class=dropdown-item href=../../../about/hardware/details style=padding-left:2rem;font-weight:calc(3/4*500)>Details</a>
<a class=dropdown-item href=../../../about/software/ style=padding-left:1rem;font-weight:calc(3/3*500)>Software</a>
<a class=dropdown-item href=../../../about/software/conda_packages/ style=padding-left:2rem;font-weight:calc(3/4*500)>Conda Packages</a>
<a class=dropdown-item href=../../../about/software/installs/ style=padding-left:2rem;font-weight:calc(3/4*500)>Installs</a>
<a class=dropdown-item href=../../../about/software/modules/ style=padding-left:2rem;font-weight:calc(3/4*500)>Modules</a>
<a class=dropdown-item href=../../../about/software/system/ style=padding-left:2rem;font-weight:calc(3/4*500)>System</a>
<a class=dropdown-item href=../../../about/software/commercial style=padding-left:2rem;font-weight:calc(3/4*500)>Commercial</a></div></li><li class="nav-item mr-4 mb-2 mb-lg-0" style=position:relative><i class="fas fa-newspaper btn-primary"></i><a class="nav-link dropdown-toggle" href=../../../news id=navbarDropdown role=button>
News</a><div class=dropdown-menu aria-labelledby=navbarDropdownMenuLink><a class=dropdown-item href=../../../news/announce/ style=padding-left:1rem;font-weight:calc(3/3*500)>Announcements</a>
<a class=dropdown-item href=../../../news/alerts/ style=padding-left:1rem;font-weight:calc(3/3*500)>Alerts</a>
<a class=dropdown-item href=../../../changes/ style=padding-left:1rem;font-weight:calc(3/3*500)>Upgrades</a></div></li><li class="nav-item mr-4 mb-2 mb-lg-0" style=position:relative><i class="fas fa-users btn-primary"></i><a class="nav-link dropdown-toggle" href=../../../events id=navbarDropdown role=button>
Events</a><div class=dropdown-menu aria-labelledby=navbarDropdownMenuLink><a class=dropdown-item href=../../../events/small/ style=padding-left:1rem;font-weight:calc(3/3*500)>Workshops</a>
<a class=dropdown-item href=../../../events/related/ style=padding-left:1rem;font-weight:calc(3/3*500)>Related Events</a></div></li><li class="nav-item mr-4 mb-2 mb-lg-0" style=position:relative><i class="fas fa-book btn-primary"></i><a class="nav-link dropdown-toggle" href=../../../manuals id=navbarDropdown role=button>
Manuals</a><div class=dropdown-menu aria-labelledby=navbarDropdownMenuLink><a class=dropdown-item href=../../../manuals/access style=padding-left:1rem;font-weight:calc(3/3*500)>Access</a>
<a class=dropdown-item href=../../../about/facility/access/ style=padding-left:2rem;font-weight:calc(3/4*500)>How to Get Account?</a>
<a class=dropdown-item href=../../../manuals/access/login style=padding-left:2rem;font-weight:calc(3/4*500)>Login</a>
<a class=dropdown-item href=../../../manuals/linux_basics/ style=padding-left:1rem;font-weight:calc(3/3*500)>Linux Basics</a>
<a class=dropdown-item href=../../../manuals/linux_basics/cmdline_basics style=padding-left:2rem;font-weight:calc(3/4*500)>Command Line Basics</a>
<a class=dropdown-item href=../../../manuals/linux_basics/filesystems style=padding-left:2rem;font-weight:calc(3/4*500)>File Systems and Transfers</a>
<a class=dropdown-item href=../../../manuals/linux_basics/permissions style=padding-left:2rem;font-weight:calc(3/4*500)>Permissions and Ownership</a>
<a class=dropdown-item href=../../../manuals/linux_basics/finding_things style=padding-left:2rem;font-weight:calc(3/4*500)>Finding Things</a>
<a class=dropdown-item href=../../../manuals/linux_basics/text style=padding-left:2rem;font-weight:calc(3/4*500)>Text Editors</a>
<a class=dropdown-item href=../../../manuals/linux_basics/streams style=padding-left:2rem;font-weight:calc(3/4*500)>Streams</a>
<a class=dropdown-item href=../../../manuals/linux_basics/pipes style=padding-left:2rem;font-weight:calc(3/4*500)>Piping</a>
<a class=dropdown-item href=../../../manuals/linux_basics/variables style=padding-left:2rem;font-weight:calc(3/4*500)>Variables</a>
<a class=dropdown-item href=../../../manuals/linux_basics/scripting style=padding-left:2rem;font-weight:calc(3/4*500)>Scripting</a>
<a class=dropdown-item href=../../../manuals/linux_basics/processes style=padding-left:2rem;font-weight:calc(3/4*500)>Processe Management</a>
<a class=dropdown-item href=../../../manuals/linux_basics/shell style=padding-left:2rem;font-weight:calc(3/4*500)>Shell Bootcamp</a>
<a class=dropdown-item href=../../../manuals/hpc_cluster/ style=padding-left:1rem;font-weight:calc(3/3*500)>HPC Cluster</a>
<a class=dropdown-item href=../../../manuals/hpc_cluster/intro style=padding-left:2rem;font-weight:calc(3/4*500)>Introduction</a>
<a class=dropdown-item href=../../../manuals/hpc_cluster/start style=padding-left:2rem;font-weight:calc(3/4*500)>Getting Started</a>
<a class=dropdown-item href=../../../manuals/hpc_cluster/jobs style=padding-left:2rem;font-weight:calc(3/4*500)>Managing Jobs</a>
<a class=dropdown-item href=../../../manuals/hpc_cluster/queue style=padding-left:2rem;font-weight:calc(3/4*500)>Queue Policies</a>
<a class=dropdown-item href=../../../manuals/hpc_cluster/package_manage style=padding-left:2rem;font-weight:calc(3/4*500)>Package Management</a>
<a class=dropdown-item href=../../../manuals/hpc_cluster/selected_software style=padding-left:2rem;font-weight:calc(3/4*500)>Selected Software</a>
<a class=dropdown-item href=../../../manuals/hpc_cluster/storage style=padding-left:2rem;font-weight:calc(3/4*500)>Data Storage</a>
<a class=dropdown-item href=../../../manuals/hpc_cluster/sharing style=padding-left:2rem;font-weight:calc(3/4*500)>Sharing Data</a>
<a class=dropdown-item href=../../../manuals/hpc_cluster/security style=padding-left:2rem;font-weight:calc(3/4*500)>Data Security</a>
<a class=dropdown-item href=../../../manuals/hpc_cluster/users style=padding-left:2rem;font-weight:calc(3/4*500)>Communicating</a>
<a class=dropdown-item href=../../../manuals/hpc_cluster/terminalide style=padding-left:2rem;font-weight:calc(3/4*500)>Terminal IDEs</a>
<a class=dropdown-item href=../../../manuals/hpc_cluster/parallelr style=padding-left:2rem;font-weight:calc(3/4*500)>Parallel Evaluations in R</a>
<a class=dropdown-item href=../../../manuals/hpc_cluster/sshkeys style=padding-left:2rem;font-weight:calc(3/4*500)>SSH Keys</a>
<a class=dropdown-item href=../../../manuals/ext_cloud/ style=padding-left:1rem;font-weight:calc(3/3*500)>Cloud/External</a>
<a class=dropdown-item href=../../../manuals/hpc_cluster/visual style=padding-left:2rem;font-weight:calc(3/4*500)>Visualization</a>
<a class=dropdown-item href=../../../manuals/ext_cloud/aws/ style=padding-left:2rem;font-weight:calc(3/4*500)>AWS</a></div></li><li class="nav-item mr-4 mb-2 mb-lg-0" style=position:relative><i class="fas fa-share-alt btn-primary"></i><a class="nav-link dropdown-toggle" href=../../../links/ id=navbarDropdown role=button>
Links</a><div class=dropdown-menu aria-labelledby=navbarDropdownMenuLink><a class=dropdown-item href=https://research.ucr.edu style=padding-left:1rem;font-weight:calc(3/3*500)>RED-UCR</a>
<a class=dropdown-item href=http://datascience.ucr.edu style=padding-left:1rem;font-weight:calc(3/3*500)>Data Science Center</a>
<a class=dropdown-item href=http://bigdata.ucr.edu style=padding-left:1rem;font-weight:calc(3/3*500)>FIELDS Program</a>
<a class=dropdown-item href=http://genomics.ucr.edu style=padding-left:1rem;font-weight:calc(3/3*500)>Institute of Integrative Genome Biology</a></div></li></ul></div><div class="navbar-nav d-none d-lg-block"><input type=search class="form-control td-search-input" placeholder="&#xf002; Search this site…" aria-label="Search this site…" autocomplete=off data-offline-search-index-json-src=../../../offline-search-index.0db6e931b1e7a6a0aed085cc679f3a59.json data-offline-search-base-href=../../../ data-offline-search-max-results=10></div></nav></header><div class="container-fluid td-outer"><div class=td-main><div class="row flex-xl-nowrap"><div class="col-12 col-md-3 col-xl-2 td-sidebar d-print-none"><div id=td-sidebar-menu class=td-sidebar__inner><form class="td-sidebar__search d-flex align-items-center d-lg-none"><input type=search class="form-control td-search-input" placeholder="&#xf002; Search this site…" aria-label="Search this site…" autocomplete=off data-offline-search-index-json-src=../../../offline-search-index.0db6e931b1e7a6a0aed085cc679f3a59.json data-offline-search-base-href=../../../ data-offline-search-max-results=10>
<button class="btn btn-link td-sidebar__toggle d-md-none p-0 ml-3 fas fa-bars" type=button data-toggle=collapse data-target=#td-section-nav aria-controls=td-docs-nav aria-expanded=false aria-label="Toggle section navigation"></button></form><nav class="collapse td-sidebar-nav" id=td-section-nav><ul class="td-sidebar-nav__section pr-md-3"><li class=td-sidebar-nav__section-title><a href=../../../manuals/ class="align-left pl-0 pr-2 td-sidebar-link td-sidebar-link__section">Manuals</a></li><ul><li class="collapse show" id=manuals><ul class="td-sidebar-nav__section pr-md-3"><li class=td-sidebar-nav__section-title><a href=../../../manuals/access/ class="align-left pl-0 pr-2 collapsed td-sidebar-link td-sidebar-link__section">Access</a></li><ul><li class=collapse id=manualsaccess><a class="td-sidebar-link td-sidebar-link__page" id=m-manualsaccesslogin href=../../../manuals/access/login/>Login</a></li></ul></ul><ul class="td-sidebar-nav__section pr-md-3"><li class=td-sidebar-nav__section-title><a href=../../../manuals/linux_basics/ class="align-left pl-0 pr-2 collapsed td-sidebar-link td-sidebar-link__section">Linux Basics</a></li><ul><li class=collapse id=manualslinux_basics><a class="td-sidebar-link td-sidebar-link__page" id=m-manualslinux_basicscmdline_basics href=../../../manuals/linux_basics/cmdline_basics/>Command Line Basics</a>
<a class="td-sidebar-link td-sidebar-link__page" id=m-manualslinux_basicsfilesystems href=../../../manuals/linux_basics/filesystems/>File Systems and Transfers</a>
<a class="td-sidebar-link td-sidebar-link__page" id=m-manualslinux_basicspermissions href=../../../manuals/linux_basics/permissions/>Permissions and Ownership</a>
<a class="td-sidebar-link td-sidebar-link__page" id=m-manualslinux_basicsfinding_things href=../../../manuals/linux_basics/finding_things/>Finding Things</a>
<a class="td-sidebar-link td-sidebar-link__page" id=m-manualslinux_basicstext href=../../../manuals/linux_basics/text/>Text Editors</a>
<a class="td-sidebar-link td-sidebar-link__page" id=m-manualslinux_basicsstreams href=../../../manuals/linux_basics/streams/>Streams</a>
<a class="td-sidebar-link td-sidebar-link__page" id=m-manualslinux_basicspipes href=../../../manuals/linux_basics/pipes/>Piping</a>
<a class="td-sidebar-link td-sidebar-link__page" id=m-manualslinux_basicsvariables href=../../../manuals/linux_basics/variables/>Variables</a>
<a class="td-sidebar-link td-sidebar-link__page" id=m-manualslinux_basicsscripting href=../../../manuals/linux_basics/scripting/>Scripting</a>
<a class="td-sidebar-link td-sidebar-link__page" id=m-manualslinux_basicsprocesses href=../../../manuals/linux_basics/processes/>Process Management</a>
<a class="td-sidebar-link td-sidebar-link__page" id=m-manualslinux_basicsshell href=../../../manuals/linux_basics/shell/>Shell Bootcamp</a></li></ul></ul><ul class="td-sidebar-nav__section pr-md-3"><li class=td-sidebar-nav__section-title><a href=../../../manuals/hpc_cluster/ class="align-left pl-0 pr-2 active td-sidebar-link td-sidebar-link__section">HPC Cluster</a></li><ul><li class="collapse show" id=manualshpc_cluster><a class="td-sidebar-link td-sidebar-link__page" id=m-manualshpc_clusterintro href=../../../manuals/hpc_cluster/intro/>Introduction</a>
<a class="td-sidebar-link td-sidebar-link__page" id=m-manualshpc_clusterstart href=../../../manuals/hpc_cluster/start/>Getting Started</a>
<a class="td-sidebar-link td-sidebar-link__page active" id=m-manualshpc_clusterjobs href=../../../manuals/hpc_cluster/jobs/>Managing Jobs</a>
<a class="td-sidebar-link td-sidebar-link__page" id=m-manualshpc_clusterqueue href=../../../manuals/hpc_cluster/queue/>Queue Policies</a>
<a class="td-sidebar-link td-sidebar-link__page" id=m-manualshpc_clusterpackage_manage href=../../../manuals/hpc_cluster/package_manage/>Package Management</a><ul class="td-sidebar-nav__section pr-md-3"><li class=td-sidebar-nav__section-title><a href=../../../manuals/hpc_cluster/selected_software/ class="align-left pl-0 pr-2 collapsed td-sidebar-link td-sidebar-link__section">Selected Software</a></li><ul><li class=collapse id=manualshpc_clusterselected_software><a class="td-sidebar-link td-sidebar-link__page" id=m-manualshpc_clusterselected_softwarealphafold href=../../../manuals/hpc_cluster/selected_software/alphafold/>AlphaFold</a>
<a class="td-sidebar-link td-sidebar-link__page" id=m-manualshpc_clusterselected_softwareondemand href=../../../manuals/hpc_cluster/selected_software/ondemand/>Open OnDemand</a>
<a class="td-sidebar-link td-sidebar-link__page" id=m-manualshpc_clusterselected_softwarepytorch_in_jupyter href=../../../manuals/hpc_cluster/selected_software/pytorch_in_jupyter/>PyTorch In Jupyter</a>
<a class="td-sidebar-link td-sidebar-link__page" id=m-manualshpc_clusterselected_softwarevscode href=../../../manuals/hpc_cluster/selected_software/vscode/>VSCode</a></li></ul></ul><a class="td-sidebar-link td-sidebar-link__page" id=m-manualshpc_clusterstorage href=../../../manuals/hpc_cluster/storage/>Data Storage</a>
<a class="td-sidebar-link td-sidebar-link__page" id=m-manualshpc_clustersharing href=../../../manuals/hpc_cluster/sharing/>Sharing Data</a>
<a class="td-sidebar-link td-sidebar-link__page" id=m-manualshpc_clustersecurity href=../../../manuals/hpc_cluster/security/>Data Security</a>
<a class="td-sidebar-link td-sidebar-link__page" id=m-manualshpc_clusterusers href=../../../manuals/hpc_cluster/users/>Communicating</a>
<a class="td-sidebar-link td-sidebar-link__page" id=m-manualshpc_clusterterminalide href=../../../manuals/hpc_cluster/terminalide/>Terminal IDEs</a>
<a class="td-sidebar-link td-sidebar-link__page" id=m-manualshpc_clusterparallelr href=../../../manuals/hpc_cluster/parallelr/>Parallel Evaluations in R</a><ul class="td-sidebar-nav__section pr-md-3"><li class=td-sidebar-nav__section-title><a href=../../../manuals/hpc_cluster/sshkeys/ class="align-left pl-0 pr-2 collapsed td-sidebar-link td-sidebar-link__section">SSH Keys</a></li><ul><li class=collapse id=manualshpc_clustersshkeys><a class="td-sidebar-link td-sidebar-link__page" id=m-manualshpc_clustersshkeyssshkeys_simple href=../../../manuals/hpc_cluster/sshkeys/sshkeys_simple/>SSH Keys Summary</a>
<a class="td-sidebar-link td-sidebar-link__page" id=m-manualshpc_clustersshkeyssshkeys_macos href=../../../manuals/hpc_cluster/sshkeys/sshkeys_macos/>SSH Keys Apple macOS</a>
<a class="td-sidebar-link td-sidebar-link__page" id=m-manualshpc_clustersshkeyssshkeys_winos href=../../../manuals/hpc_cluster/sshkeys/sshkeys_winos/>SSH Keys Microsoft Windows</a>
<a class="td-sidebar-link td-sidebar-link__page" id=m-manualshpc_clustersshkeyssshkeys_linux href=../../../manuals/hpc_cluster/sshkeys/sshkeys_linux/>SSH Keys Linux</a></li></ul></ul><a class="td-sidebar-link td-sidebar-link__page" id=m-manualshpc_clustervisual href=../../../manuals/hpc_cluster/visual/>Visualization</a>
<a class="td-sidebar-link td-sidebar-link__page" id=m-manualshpc_clustersingularity href=../../../manuals/hpc_cluster/singularity/>Singularity Jobs</a><ul class="td-sidebar-nav__section pr-md-3"><li class=td-sidebar-nav__section-title><a href=../../../manuals/hpc_cluster/data/ class="align-left pl-0 pr-2 collapsed td-sidebar-link td-sidebar-link__section">Data Transfer</a></li><ul><li class=collapse id=manualshpc_clusterdata><a class="td-sidebar-link td-sidebar-link__page" id=m-manualshpc_clusterdataglobus href=../../../manuals/hpc_cluster/data/globus/>Globus Connect Personal</a></li></ul></ul></li></ul></ul><ul class="td-sidebar-nav__section pr-md-3"><li class=td-sidebar-nav__section-title><a href=../../../manuals/ext_cloud/ class="align-left pl-0 pr-2 collapsed td-sidebar-link td-sidebar-link__section">Cloud/External</a></li><ul><li class=collapse id=manualsext_cloud><ul class="td-sidebar-nav__section pr-md-3"><li class=td-sidebar-nav__section-title><a href=../../../manuals/ext_cloud/aws/ class="align-left pl-0 pr-2 collapsed td-sidebar-link td-sidebar-link__section">AWS</a></li><ul><li class=collapse id=manualsext_cloudaws><a class="td-sidebar-link td-sidebar-link__page" id=m-manualsext_cloudawsegress href=../../../manuals/ext_cloud/aws/egress/>Account Egress Waiver</a>
<a class="td-sidebar-link td-sidebar-link__page" id=m-manualsext_cloudawsbilling href=../../../manuals/ext_cloud/aws/billing/>Cost Control and Billing</a></li></ul></ul></li></ul></ul></li></ul></ul></nav></div></div><div class="d-none d-xl-block col-xl-2 td-toc d-print-none"><div class="td-page-meta ml-2 pb-1 pt-2 mb-0"><a href=https://raw.githubusercontent.com/ucr-hpcc/ucr-hpcc.github.io/master/content/en/Manuals/hpc_cluster/jobs.md target=_blank class=source-link><i class="fa fa-code fa-fw"></i> View source code</a>
<a href=https://github.com/ucr-hpcc/ucr-hpcc.github.io/edit/master/content/en/Manuals/hpc_cluster/jobs.md target=_blank><i class="fa fa-edit fa-fw"></i> Edit this page</a>
<a href="https://github.com/ucr-hpcc/ucr-hpcc.github.io/new/master/content/en/Manuals/hpc_cluster/jobs.md?filename=change-me.md&value=---%0Atitle%3A+%22Long+Page+Title%22%0AlinkTitle%3A+%22Short+Nav+Title%22%0Aweight%3A+100%0Adescription%3A+%3E-%0A+++++Page+description+for+heading+and+indexes.%0A---%0A%0A%23%23+Heading%0A%0AEdit+this+template+to+create+your+new+page.%0A%0A%2A+Give+it+a+good+name%2C+ending+in+%60.md%60+-+e.g.+%60getting-started.md%60%0A%2A+Edit+the+%22front+matter%22+section+at+the+top+of+the+page+%28weight+controls+how+its+ordered+amongst+other+pages+in+the+same+directory%3B+lowest+number+first%29.%0A%2A+Add+a+good+commit+message+at+the+bottom+of+the+page+%28%3C80+characters%3B+use+the+extended+description+field+for+more+detail%29.%0A%2A+Create+a+new+branch+so+you+can+preview+your+new+file+and+request+a+review+via+Pull+Request.%0A" target=_blank><i class="fa fa-edit fa-fw"></i> Create child page</a>
<a href="https://github.com/ucr-hpcc/ucr-hpcc.github.io/issues/new?title=Managing%20Jobs" target=_blank><i class="fab fa-github fa-fw"></i> Create documentation issue</a></div><nav id=TableOfContents><ul><li><a href=#what-is-a-job>What is a Job?</a></li><li><a href=#partitions>Partitions</a></li><li><a href=#slurm>Slurm</a><ul><li><a href=#resources-and-limits>Resources and Limits</a></li><li><a href=#submitting-jobs>Submitting Jobs</a><ul><li><a href=#non-interactive-submission>Non-interactive Submission</a></li><li><a href=#interactive-submission>Interactive Submission</a></li></ul></li><li><a href=#feature-constraints>Feature Constraints</a><ul><li><a href=#constraint-examples>Constraint Examples</a></li></ul></li><li><a href=#monitoring-jobs>Monitoring Jobs</a></li><li><a href=#canceling-jobs>Canceling Jobs</a></li><li><a href=#optimizing-jobs>Optimizing Jobs</a></li><li><a href=#slurm-job-reasonerror-codes>Slurm Job Reason/Error Codes</a></li><li><a href=#advanced-jobs>Advanced Jobs</a><ul><li><a href=#array-jobs>Array Jobs</a></li></ul></li><li><a href=#highmem-jobs>Highmem Jobs</a></li><li><a href=#gpu-jobs>GPU Jobs</a></li><li><a href=#web-browser-access>Web Browser Access</a><ul><li><a href=#ports>Ports</a></li><li><a href=#tunneling>Tunneling</a></li><li><a href=#examples>Examples</a></li></ul></li><li><a href=#desktop-environments>Desktop Environments</a><ul><li><a href=#vnc-server-cluster>VNC Server (cluster)</a></li><li><a href=#vnc-client-desktoplaptop>VNC Client (Desktop/Laptop)</a></li></ul></li></ul></li><li><a href=#parallelization>Parallelization</a><ul><li><a href=#parallel-methods>Parallel Methods</a><ul><li><a href=#mpi>MPI</a></li></ul></li></ul></li><li><a href=#more-examples>More examples</a></li></ul></nav></div><main class="col-12 col-md-9 col-xl-8 pl-md-5" role=main><nav aria-label=breadcrumb class="d-none d-md-block d-print-none"><ol class="breadcrumb spb-1"><li class=breadcrumb-item><a href=https://hpcc.ucr.edu/manuals/>Manuals</a></li><li class=breadcrumb-item><a href=https://hpcc.ucr.edu/manuals/hpc_cluster/>HPC Cluster</a></li><li class="breadcrumb-item active" aria-current=page><a href=https://hpcc.ucr.edu/manuals/hpc_cluster/jobs/>Managing Jobs</a></li></ol></nav><div class=td-content><h1>Managing Jobs</h1><p class=reading-time><i class="fa fa-clock" aria-hidden=true></i> 19 minute read</p><h2 id=what-is-a-job>What is a Job?</h2><p>Submitting and managing jobs is at the heart of using the cluster. A &lsquo;job&rsquo; refers to the script, pipeline or experiment that you run on the nodes in the cluster.</p><h2 id=partitions>Partitions</h2><p>Jobs are submitted to so-called partitions (or queues). Each partition is a group of nodes, often with similar hardware specifications (e.g. CPU or RAM configurations). The quota policies applying to each partitions are outlined on the <a href=https://hpcc.ucr.edu/manuals/hpc_cluster/queue/>Queue Policies</a> page.</p><ul><li>epyc<ul><li>Nodes: r21-r38</li><li>CPU: AMD</li><li>Supported Extensions<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup>: AVX, AVX2, SSE, SSE2, SSE4</li><li>RAM: 1 GB default</li><li>Time (walltime): 168 hours (7 days) default</li></ul></li><li>intel<ul><li>Default partition</li><li>Nodes: i01-02,i17-i40</li><li>CPU: Intel</li><li>Supported Extensions<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup>: AVX, AVX2, SSE, SSE2, SSE4</li><li>RAM: 1 GB default</li><li>Time (walltime): 168 hours (7 days) default</li></ul></li><li>batch<ul><li>Nodes: c01-c48</li><li>CPU: AMD</li><li>Supported Extensions<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup>: AVX, SSE, SSE2, SSE4</li><li>RAM: 1 GB default</li><li>Time (walltime): 168 hours (7 days) default</li></ul></li><li>highmem<ul><li>Nodes: h01-h06</li><li>CPU: Intel</li><li>Supported Extensions<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup>: AVX, SSE, SSE2, SSE4</li><li>RAM: 100 GB to 1000 GB</li><li>Time (walltime): 48 hours (2 days) default</li></ul></li><li>highclock<ul><li>Nodes: hz01-hz04</li><li>CPU: Intel</li><li>Supported Extensions<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup>: AVX, SSE, SSE2, SSE4</li><li>RAM: 1 GB default</li><li>Time (walltime): 168 hours (7 days) default</li></ul></li><li>gpu<ul><li>Nodes: gpu01-gpu06</li><li>CPU: AMD/Intel</li><li>GPUs: NVIDIA K80, A100, P100</li><li>RAM: 1 GB default</li><li>Time (walltime): 48 hours (2 days) default</li></ul></li><li>short<ul><li>Nodes: Mixed set of nodes from batch, intel, and group partitions</li><li>Cores: AMD/Intel</li><li>RAM: 1 GB default</li><li>Time (walltime): 2 hours Maximum</li></ul></li><li>Lab Partitions<ul><li>If your lab has purchased nodes then you will have a priority partition with the same name as your group (ie. girkelab).</li></ul></li></ul><p>In order to submit a job to different partitions add the optional &lsquo;-p&rsquo; parameter with the name of the partition you want to use:</p><pre><code class=language-bash>sbatch -p batch SBATCH_SCRIPT.sh
sbatch -p highmem SBATCH_SCRIPT.sh
sbatch -p epyc SBATCH_SCRIPT.sh
sbatch -p gpu SBATCH_SCRIPT.sh
sbatch -p intel SBATCH_SCRIPT.sh
sbatch -p highclock SBATCH_SCRIPT.sh
sbatch -p mygroup SBATCH_SCRIPT.sh
</code></pre><h2 id=slurm>Slurm</h2><p>Slurm is used as a queuing system across all head nodes. <a href=#getting-started>SSH directly into the cluster</a> and your connection will be automatically load balanced to a head node:</p><pre><code class=language-bash>ssh -XY cluster.hpcc.ucr.edu
</code></pre><h3 id=resources-and-limits>Resources and Limits</h3><p>To see your limits you can do the following:</p><pre><code class=language-bash>slurm_limits
</code></pre><p>Check total number of cores used by your group in the all partitions:</p><pre><code class=language-bash>group_cpus
</code></pre><p>However this does not tell you when your job will start, since it depends on the duration of each job.
The best way to do this is with the &ldquo;&ndash;start&rdquo; flag on the squeue command:</p><pre><code class=language-bash>squeue --start -u $USER
</code></pre><h3 id=submitting-jobs>Submitting Jobs</h3><p>There are 2 basic ways to submit jobs; non-interactive and interactive. Slurm will automatically start within the directory where you submitted the job from, so keep that in mind when you use relative file paths.</p><h4 id=non-interactive-submission>Non-interactive Submission</h4><p>Non-interactive jobs are submitted as SBATCH scripts, an example is as follows:</p><pre><code class=language-bash>sbatch SBATCH_SCRIPT.sh
</code></pre><p>Here is an example of an SBATCH script:</p><pre><code class=language-bash>#!/bin/bash -l

#SBATCH --nodes=1
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=10
#SBATCH --mem=10G
#SBATCH --time=1-00:15:00     # 1 day and 15 minutes
#SBATCH --mail-user=useremail@address.com
#SBATCH --mail-type=ALL
#SBATCH --job-name=&quot;just_a_test&quot;
#SBATCH -p epyc # You can use any of the following; epyc, intel, batch, highmem, gpu

# Print current date
date

# Load samtools
module load samtools

# Concatenate BAMs
samtools cat -h header.sam -o out.bam in1.bam in2.bam

# Print name of node
hostname
</code></pre><p>The above job will request 1 node, 10 cores (parallel threads), 10GB of memory, for 1 day and 15 minutes. An email will be sent to the user when the status of the job changes (Start, Failed, Completed).
For more information regarding parallel/multi core jobs refer to <a href=#parallelization>Parallelization</a>.</p><h4 id=interactive-submission>Interactive Submission</h4><p>Interactive jobs are submitted using <code>srun</code>. An example is as follows:</p><pre><code class=language-bash>srun --pty bash -l
</code></pre><p>If you do not specify a partition then the &ldquo;hpcc_default&rdquo; partition is used by default.</p><p>Here is a more complete example:</p><pre><code class=language-bash>srun --mem=1gb --cpus-per-task 1 --ntasks 1 --time 10:00:00 --x11 --pty bash -l
</code></pre><p>The above example enables X11 forwarding and requests 1GB of memory and 1 core for 10 hours within an interactive session.</p><h3 id=feature-constraints>Feature Constraints</h3><p>Using the <code>--constraint</code> (or <code>-C</code> flag) allows you to fine-tune what type of machine your job can run on, mainly useful on the &ldquo;short&rdquo; partitions. Below is a list of nodes as well as what features exist for each node type.</p><table><thead><tr><th>Nodes</th><th>Features</th></tr></thead><tbody><tr><td>c[01-47]</td><td>amd, abu_dhabi</td></tr><tr><td>h[01-06]</td><td></td></tr><tr><td>gpu[01-04]</td><td>gpu_legacy</td></tr><tr><td>gpu05</td><td>gpu_prev</td></tr><tr><td>gpu[06-08]</td><td>gpu_latest, gpu_highmem</td></tr><tr><td>gpu09</td><td>gpu_latest</td></tr><tr><td>i[01-62]</td><td>intel, broadwell</td></tr><tr><td>r[01-06]</td><td>amd, rome</td></tr><tr><td>r[07-38]</td><td>amd, milan</td></tr><tr><td>r[41-43]</td><td>amd, milan</td></tr><tr><td>r[44-51]</td><td>amd, genoa</td></tr><tr><td>x[01-06]</td><td>intel, cascade</td></tr><tr><td>h[01-06]</td><td>intel, ivy</td></tr><tr><td>h07</td><td>intel, sapphire</td></tr></tbody></table><h4 id=constraint-examples>Constraint Examples</h4><p>Since jobs on the &ldquo;short&rdquo; partition can run on any node, jobs can be narrowed down using constraints.</p><p>If you require an Intel node of any generation:</p><pre><code>srun -p short -t 2:00:00 -c 8 --mem 8GB --constraint intel --pty bash -l
</code></pre><p>If you require an AMD node, but want it to be Rome or Milan generation (ie. <strong>not</strong> Abu Dhabi):</p><pre><code>srun -p short -t 2:00:00 -c 8 --mem 8GB --constraint &quot;amd&amp;(rome|milan)&quot; --pty bash -l
</code></pre><p>If you want to run on a modern GPU machine:</p><pre><code>srun -p short_gpu -t 2:00:00 -c 8 --mem 8GB --gpu:1 --constraint &quot;gpu_latest&quot; --pty bash -l
</code></pre><blockquote><p>When using constraints with GPUs, make sure to request a generic GPU</p></blockquote><h3 id=monitoring-jobs>Monitoring Jobs</h3><p>To check on your jobs states, run the following:</p><pre><code class=language-bash>squeue -u $USER --start
</code></pre><p>To list all the details of a specific job (the JOBID can be found using <code>squeue</code>), run the following:</p><pre><code class=language-bash>scontrol show job JOBID
</code></pre><p>To view past jobs and their details, run the following:</p><pre><code class=language-bash>sacct -u $USER -l
</code></pre><p>You can also adjust the start <code>-S</code> time and/or end <code>-E</code> time to view, using the YYYY-MM-DD format.
For example, the following command uses start and end times:</p><pre><code class=language-bash>sacct -u $USER -S 2018-01-01 -E 2018-08-30 -l | less -S # Type 'q' to quit
</code></pre><p>Custom command for summarizing activity of all users on cluster</p><pre><code class=language-bash>jobMonitor # or qstatMonitor
</code></pre><h3 id=canceling-jobs>Canceling Jobs</h3><p>In cancel/stop your job run the following:</p><pre><code class=language-bash>scancel JOBID
</code></pre><p>You can also cancel multiple jobs:</p><pre><code class=language-bash>scancel JOBID1 JOBID2 JOBID3
</code></pre><p>If you want to cancel/stop/kill ALL your jobs it is possible with the following:</p><pre><code class=language-bash># Be very careful when running this, it will kill all your jobs.
squeue --user $USER --noheader --format '%i' | xargs scancel
</code></pre><p>For more information please refer to <a href=https://slurm.schedmd.com/scancel.html title="Slurm scancel doc">Slurm scancel documentation</a>.</p><h3 id=optimizing-jobs>Optimizing Jobs</h3><p>After a job has been completed, you can use <code>seff ##</code> ("##" being your Slurm Job ID) to check how many resources your job consumed during it&rsquo;s run. <code>seff</code> is only useful <strong>after</strong> a job has completed, and will not give useful information on currently-running jobs.</p><p>For example:</p><pre><code>$ seff 123123

Job ID: 123123
Cluster: hpcc
User/Group: your_username/yourlab
State: COMPLETED (exit code 0)
Nodes: 1
Cores per node: 20
CPU Utilized: 03:26:14
CPU Efficiency: 95.04% of 03:37:00 core-walltime
Job Wall-clock time: 00:10:51
Memory Utilized: 81.20 GB
Memory Efficiency: 81.20% of 100.00 GB
</code></pre><p>In the above example, we can see good utilization of the CPU cores (95%) as well as good utilization of memory usage (81%).</p><p>If CPU Efficiency is low, make sure that the program(s) you are running makes use of multi-threading correctly. Requesting more cores for a job will not make your program run faster if it does not properly take advantage of them.</p><p>If Memory Efficiency is low, then you can try reducing the requested memory for a job. <strong>Note:</strong> Just because you see your job uses 81.20GB of memory <strong>does not</strong> mean that next time you should request exactly 81.20GB of memory. Variations in input data <strong>will</strong> cause different memory usage characteristics. You should try to aim to request ~20% higher memory then will actually be used to account for any spikes in memory usage. Slurm might miss some quick spikes of memory usage, but the Operating System will not. In this regard it&rsquo;s better to overestimate on initial runs, and scale back once you find a good limit.</p><h3 id=slurm-job-reasonerror-codes>Slurm Job Reason/Error Codes</h3><p>If a job is stuck in the queue or fails to start, there are typically Slurm error codes assigned that explain the reason. Typically these are a bit hard to parse, so below is a table of common error codes and how to work around them.</p><table><thead><tr><th>Error Code</th><th>Reason</th><th>Fix</th></tr></thead><tbody><tr><td>Resources</td><td>This isn&rsquo;t an error, but rather why your job can&rsquo;t start immediately.</td><td>Once requested resources are available, then your job will start.</td></tr><tr><td>Priority</td><td>This isn&rsquo;t an error, but rather why your job can&rsquo;t start immediately.</td><td>You have likely submitted many jobs in a short period of time and Slurm&rsquo;s Fair-Share algorithm is allowing other higher priority jobs to run first.</td></tr><tr><td>QOSMaxWallDurationPerJobLimit</td><td>The time limit requested on the selected partition goes over the limits. For example, requesting 3 days on the &ldquo;short&rdquo; partition.</td><td>Make sure that you are within the partition&rsquo;s time limit. Please refer to the <a href=https://hpcc.ucr.edu/manuals/hpc_cluster/queue/#partition-quotas>Queue Policies</a> page for the per-partition time limits.</td></tr><tr><td>AssocGrpCpuLimit</td><td>You are exceeding the Per-User CPU limit on a specific partition.</td><td>You must wait until jobs finish within a partition to free up resources to allow additional jobs to run.</td></tr><tr><td>AssocGrpMemLimit</td><td>You are exceeding the Per-User Memory limit on a specific partition.</td><td>You must wait until jobs finish within a partition to free up resources to allow additional jobs to run.</td></tr><tr><td>AssocGrpGRES</td><td>You are exceeding the Per-User GRES (GPU) limit</td><td>You must wait until your GPU jobs finish to free up resources to allow additional jobs to run.</td></tr><tr><td>MaxSubmitJobLimit</td><td>You are trying to submit more than 5000 jobs. There is a 5000 job limit per-user for queued and running jobs.</td><td>Wait until some of your jobs finish, then you can continue submitting jobs.</td></tr><tr><td>ReqNodeNotAvail, Reserved for maintenance</td><td>The time limit of your job would cause it to overlap with an upcoming maintenance.</td><td>You can either reduce your job&rsquo;s runtime or wait for the maintenance to complete.</td></tr><tr><td>PartitionConfig</td><td>The job has been queued to the wrong partition under the wrong account.</td><td>Some partitions require that you queue under a specific account. eg. preempt jobs need to use the preempt account (<code>-A preempt</code>)</td></tr></tbody></table><p>This is only a small number of the most common reasons. For a full list please see Slurm&rsquo;s <a href=https://slurm.schedmd.com/job_reason_codes.html>Job Reason Codes</a> page. If you are confused as to why you&rsquo;re getting a specific reason, please reach out to support.</p><h3 id=advanced-jobs>Advanced Jobs</h3><p>There is a third way of submitting jobs by using steps.
Single Step submission:</p><pre><code class=language-bash>srun &lt;command&gt;
</code></pre><p>Under a single step job your command will hang until appropriate resources are found and when the step command is finished the results will be sent back on STDOUT. This may take some time depending on the job load of the cluster.
Multi Step submission:</p><pre><code class=language-bash>salloc -N 4 bash -l
srun &lt;command&gt;
...
srun &lt;command&gt;
exit
</code></pre><p>Under a multi step job the salloc command will request resources and then your parent shell will be running on the head node. This means that all commands will be executed on the head node unless preceeded by the srun command. You will also need to exit this shell in order to terminate your job.</p><h4 id=array-jobs>Array Jobs</h4><p>If a large batch of fairly similar jobs need to be submitted, an Array Job might be a good option. For an array job, include the <code>--array</code> parameter in your sbatch script, similar to the following:</p><pre><code class=language-bash>#!/bin/bash
#SBATCH --nodes=1
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=2     # This will be the number of CPUs per individual array job
#SBATCH --mem=1G     # This will be the memory per individual array job
#SBATCH --time=0-00:15:00     # 15 minutes
#SBATCH --array=1-2500
#SBATCH --job-name=&quot;just_a_test&quot;

echo &quot;I have array ID ${SLURM_ARRAY_TASK_ID}&quot;

</code></pre><p>Within each job, the <code>SLURM_ARRAY_TASK_ID</code> environment variable is set and can be used to slightly change how each job is run.</p><p>Note that there is a 2500 job limit for array jobs.</p><p>More information can be found on the <a href=https://slurm.schedmd.com/job_array.html>Slurm Documentation</a> including other Environment Variables that are set per-job.</p><h3 id=highmem-jobs>Highmem Jobs</h3><p>The highmem partition does not have a default amount of memory set, however it does has a minimum limit of 100GB per job. This means that you need to explicity request at least 100GB or more of memory.</p><p>Non-Interactive:</p><pre><code class=language-bash>sbatch -p highmem --mem=100g --time=24:00:00 SBATCH_SCRIPT.sh
</code></pre><p>Interactive</p><pre><code class=language-bash>srun -p highmem --mem=100g --time=24:00:00 --pty bash -l
</code></pre><p>Of course you should adjust the time argument according to your job requirements.</p><h3 id=gpu-jobs>GPU Jobs</h3><p>GPU nodes have multiple GPUs, and vary in type (K80, P100, or A100). This means you need to request how many GPUs and of what type that you would like to use.</p><p>To request a gpu of any type, only indicate how many GPUs you would like to use.</p><p>Non-Interactive:</p><pre><code class=language-bash>sbatch -p gpu --gres=gpu:1 --mem=100g --time=1:00:00 SBATCH_SCRIPT.sh
</code></pre><p>Interactive</p><pre><code class=language-bash>srun -p gpu --gres=gpu:4 --mem=100g --time=1:00:00 --pty bash -l
</code></pre><p>Since the HPCC Cluster has three types of GPUs installed (K80s, P100s, and A100s), GPUs can be requested explicitly by type. More info on what GPUs are available can be found in the <a href=https://hpcc.ucr.edu/about/hardware/details/#worker-nodes>Worker Node</a> section of our <a href=https://hpcc.ucr.edu/about/hardware/details/>Hardware Details</a> page.</p><p>Non-Interactive:</p><pre><code class=language-bash>sbatch -p gpu --gres=gpu:k80:1 --mem=100g --time=1:00:00 SBATCH_SCRIPT.sh
sbatch -p gpu --gres=gpu:p100:1 --mem=100g --time=1:00:00 SBATCH_SCRIPT.sh
sbatch -p gpu --gres=gpu:a100:1 --mem=100g --time=1:00:00 SBATCH_SCRIPT.sh
</code></pre><p>Interactive</p><pre><code class=language-bash>srun -p gpu --gres=gpu:k80:1 --mem=100g --time=1:00:00 --pty bash -l
srun -p gpu --gres=gpu:p100:1 --mem=100g --time=1:00:00 --pty bash -l
srun -p gpu --gres=gpu:a100:1 --mem=100g --time=1:00:00 --pty bash -l
</code></pre><p>Of course you should adjust the time argument according to your job requirements.</p><p>Once your job starts your code must reference the environment variable &ldquo;CUDA_VISIBLE_DEVICES&rdquo; which will indicate which GPUs have been assigned to your job. Most CUDA enabled software, like MegaHIT, will check this environment variable and automatically limit accordingly.</p><p>For example, after reserving 4 GPUs for a NAMD2 job:</p><pre><code class=language-bash>echo $CUDA_VISIBLE_DEVICES
0,1,2,3
namd2 +idlepoll +devices $CUDA_VISIBLE_DEVICES MD1.namd
</code></pre><p>Each group is limited to a maximum of 8 GPUs on the gpu partition. Please be respectful of others and keep in mind that the GPU nodes are a limited shared resource.
Since the CUDA libraries will only run with GPU hardware, development and compiling of code must be done within a job session on a GPU node.</p><p>Here are a few more examples of jobs that utilize more complex features (ie. array, dependency, MPI etc):
<a href=https://github.com/ucr-hpcc/hpcc_slurm_examples>Slurm Examples</a></p><h3 id=web-browser-access>Web Browser Access</h3><h4 id=ports>Ports</h4><p>Some jobs require web browser access in order to utilize the software effectively.
These kinds of jobs typically use (bind) ports in order to provide a graphical user interface (GUI) through a web browser.
Users are able to run jobs that use (bind) ports on a compute node.
Any port can be used on any compute node, as long as the port number is greater than 1000 and it is not already in use (bound).</p><h4 id=tunneling>Tunneling</h4><p>Once a job is running on a compute node and bound to a port, you may access this compute node via a web browser.
This is accomplished by using 2 chained SSH tunnels to route traffic through our firewall.
This acts much like 2 runners in a relay race, handing the baton to the next runner, to get past a security checkpoint.</p><p>Running the following command on your local machine will create a tunnel that goes though a headnode and connect to a
compute node on a particular port.</p><pre><code class=language-bash>ssh -NL 8888:NodeName:8888 username@cluster.hpcc.ucr.edu
</code></pre><p>Port 8888 (first) is the local port you will be using on your local machine.
NodeName is the compute node where where job is running, which can be found by using the <code>squeue -u $USER</code> command.
Port 8888 (second) is the remote port on the compute node.
Again, the NodeName and ports will be different depending on where your job runs and what port your job uses.</p><p>At this point you may need to provide a password to make the SSH tunnel.
Once this has succeeded, the command will hang (this is normal).
Leave this session connected, if you close it your tunnel will be closed.</p><p>Then open a browser on your local computer (PC/laptop) and point it to:</p><pre><code>http://localhost:8888
</code></pre><p>If your job uses TSL/SSL, so you may need to try https if the above does not work:</p><pre><code>https://localhost:8888
</code></pre><h4 id=examples>Examples</h4><ol><li><p>A perfect example of this method is used for Jupyter Lab/Notebook. For more details please refer to the <a href=https://hpcc.ucr.edu/manuals/linux_basics/text/#jupyter-server>JupyterLab Usage</a> page.</p></li><li><p>RStudio Server instances can also be started directly on a compute node and accessed via an SSH tunnel. For details see <a href=https://hpcc.ucr.edu/manuals/linux_basics/text/#2-compute-node-instance>here</a>.</p></li></ol><h3 id=desktop-environments>Desktop Environments</h3><h4 id=vnc-server-cluster>VNC Server (cluster)</h4><p><strong>Start VNC Server</strong></p><p>Log into the cluster:</p><pre><code class=language-bash>ssh username@cluster.hpcc.ucr.edu
</code></pre><p>The VNC programs are only available on Compute Nodes, and additionally the first time you run the vncserver it will need to be configured:</p><pre><code class=language-bash>srun -p epyc -c 2 --mem 4GB -t 10:00 --pty bash -l # Start compute session
vncserver -fg # Configure VNC
exit # Leave compute session
</code></pre><p>You should set a password for yourself, and the read-only password is optional.</p><p>After your vncserver is configured, submit a vncserver job to get it started:</p><pre><code class=language-bash>sbatch -p epyc --cpus-per-task=4 --mem=10g --time=2:00:00 --wrap='vncserver -fg' --output='vncserver-%j.out'
</code></pre><blockquote><p>Note: Appropriate job resources should be requested based on the processes you will be running from within the VNC session.</p></blockquote><p>Check the contents of your job log to determine the <code>NodeName</code> and <code>Port</code> you were assigned:</p><pre><code class=language-bash>cat vncserver-*.out
</code></pre><p>The contents of your slurm job log should be similar to the following:</p><pre><code class=language-bash>vncserver

New 'i54:1' desktop is i54:1

Creating default startup script /rhome/username/.vnc/xstartup
Starting applications specified in /rhome/username/.vnc/xstartup
Log file is /rhome/username/.vnc/i54:1.log
</code></pre><p>The VNC <code>Port</code> used should be 5900+N, N being the display number mentioned above in the format <code>NodeName</code>:<code>DisplayNumber</code> (ie. <code>i54:1</code>).
In this example (default), the port is <code>5901</code>, if this <code>Port</code> were already in use then the vncserver will automatically increment the DisplayNumber and you might find something like <code>i54:2</code> or <code>i54:3</code> and so on.</p><p><strong>Stop VNC Server</strong></p><p>To stop the vncserver, you can click on the logout option from the upper right hand menu from within your VNC desktop environment.
If you want to kill your vncserver manually, then you will need to do the following:</p><pre><code class=language-bash>ssh NodeName 'vncserver -kill :DisplayNumber'
</code></pre><p>You will need to replace <code>NodeName</code> with the node name of your where your job is running, and the <code>DisplayNumber</code> with the DisplayNumber from your slurm job log.</p><h4 id=vnc-client-desktoplaptop>VNC Client (Desktop/Laptop)</h4><p>After you know the <code>NodeName</code> and VNC <code>Port</code> you should be able to create an SSH tunnel to your vncserver, like so:</p><pre><code class=language-bash>ssh -N -L Port:NodeName:Port cluster.hpcc.ucr.edu
</code></pre><p>Now let us create an SSH tunnel on your local machine (desktop/laptop) using the <code>NodeName</code> and VNC <code>Port</code> from above:</p><pre><code class=language-bash>ssh -L 5901:i54:5901 cluster.hpcc.ucr.edu
</code></pre><p>After you have logged into the cluster with this shell, log into the node where your VNC server is running:</p><pre><code class=language-bash>ssh NodeName
</code></pre><p>After you have logged into the correct <code>NodeName</code>, just let this terminal sit here, do not close it.</p><p>Then launch vncviewer on your local system (laptop/workstation), like so:</p><pre><code class=language-bash>vncviewer localhost:5901
</code></pre><p>After launching the vncviewer, and providing your VNC password (not your cluster password), you should be able to see a Linux desktop environment.</p><p>For more information regarding tunnels and VNC in MS Windows, please refer <a href=https://docs.ycrc.yale.edu/clusters-at-yale/access/vnc/>More VNC Info</a>.</p><h2 id=parallelization>Parallelization</h2><p>There are 3 major ways to parallelize work on the cluster:</p><ol><li>Batch</li><li>Thread</li><li>MPI</li></ol><h3 id=parallel-methods>Parallel Methods</h3><p>For <strong>batch</strong> jobs, all that is required is that you have a way to split up the data and submit multiple jobs running with the different chunks.
Some data sets, for example a FASTA file is very easy to split up (ie. fasta-splitter). This can also be more easily achieved by submitting an array job. For more details please refer to <a href=#advanced-jobs>Advanced Jobs</a>.</p><p>For <strong>threaded</strong> jobs, your software must have an option referring to &ldquo;number of threads&rdquo; or &ldquo;number of processors&rdquo;. Once the thread/processor option is identified in the software, (ie. blastn flag <code>-num_threads 4</code>) you can use that as long as you also request the same number of CPU cores (ie. slurm flag <code>--cpus-per-task=4</code>).</p><p>For <strong>MPI</strong> jobs, your software must be MPI enabled. This generally means that it was compiled with MPI libraries. Please refer to the user manual of the software you wish to use as well as our documentation regarding <a href=#mpi>MPI</a>. It is important that the number of cores used is equal to the number requested.</p><p>In Slurm you will need 2 different flags to request cores, which may seem similar, however they have different purposes:</p><ul><li>The <code>--cpus-per-task=N</code> will provide N number of virtual cores with locality as a factor.
Closer virtual cores can be faster, assuming there is a need for rapid communication between threads.
Generally, this is good for threading, however not so good for independent subprocesses nor for MPI.</li><li>The <code>--ntasks=N</code> flag will provide N number of physical cores on a single or even multiple nodes.
These cores can be further away, since the need for physical CPUs and dedicated memory is more important.
Generally this is good for independent subprocesses, and MPI, however not so good for threading.</li></ul><p>Here is a table to better explain when to use these Slurm options:</p><table><thead><tr><th>Slurm Flag</th><th>Single Threaded</th><th>Multi Threaded (OpenMP)</th><th>MPI only</th><th>MPI + Multi Threaded (hybrid)</th></tr></thead><tbody><tr><td><code>--cpus-per-task</code></td><td></td><td>X</td><td></td><td>X</td></tr><tr><td><code>--ntasks</code></td><td></td><td></td><td>X</td><td>X</td></tr></tbody></table><p>As you can see:</p><ol><li>A single threaded job would use neither Slurm option, since Slurm already assumes at least a single core.</li><li>A multi threaded OpenMP job would use <code>--cpus-per-task</code>.</li><li>A MPI job would use <code>--ntasks</code>.</li><li>A Hybrid job would use both.</li></ol><p>For more details on how these Slurm options work please review <a href=https://slurm.schedmd.com/mc_support.html>Slurm Multi-core/Multi-thread Support</a>.</p><h4 id=mpi>MPI</h4><p>MPI stands for the Message Passing Interface. MPI is a standardized API typically used for parallel and/or distributed computing.
The HPCC cluster has a custom compiled versions of MPI that allows users to run MPI jobs across multiple nodes.
These types of jobs have the ability to take advantage of hundreds of CPU cores symultaniously, thus improving compute time.</p><p>Many implementations of MPI exists, however we only support the following:</p><ul><li><a href=http://www.open-mpi.org/>Open MPI</a></li><li><a href=http://www.mpich.org/>MPICH</a></li><li><a href=https://software.intel.com/en-us/mpi-developer-guide-linux>IMPI</a></li></ul><p>For general information on MPI under Slurm look <a href=https://slurm.schedmd.com/mpi_guide.html>here</a>.
If you need to compile an MPI application then please email <a href=mailto:support@hpcc.ucr.edu>support@hpcc.ucr.edu</a> for assistance.</p><p>When submitting MPI jobs it is best to ensure that the nodes are identical, since MPI is sensitive to differences in CPU and/or memory speeds.
The <code>batch</code> and <code>intel</code> partitions are designed to be homogeneous, however, the <code>short</code> partition is a mixed set of nodes.
When using the <code>short</code> partition for MPI append the constraint flag for Slurm.</p><p><strong>Short Example</strong></p><p>Here is an example that shows how to ensure that your job will only run on <code>intel</code> nodes from the <code>short</code> partition:</p><pre><code class=language-bash>sbatch -p short --constraint=intel myJobScript.sh
</code></pre><p><strong>NAMD Example</strong></p><p>To run a NAMD2 process as an OpenMPI job on the cluster:</p><ol><li><p>Log-in to the cluster</p></li><li><p>Create SBATCH script</p><pre><code class=language-bash>#!/bin/bash -l

#SBATCH -J c3d_cr2_md
#SBATCH -p epyc
#SBATCH --ntasks=32
#SBATCH --mem=16gb
#SBATCH --time=01:00:00

# Load needed modules
# You could also load frequently used modules from within your ~/.bashrc
module load slurm # Should already be loaded
module load openmpi # Should already be loaded
module load namd

# Run job utilizing all requested processors
# Please visit the namd site for usage details: http://www.ks.uiuc.edu/Research/namd/
mpirun --mca btl ^tcp namd2 run.conf &amp;&gt; run_namd.log
</code></pre></li><li><p>Submit SBATCH script to Slurm queuing system</p><pre><code class=language-bash>sbatch run_namd.sh
</code></pre></li></ol><p><strong>Maker Example</strong></p><p>OpenMPI does not function properly with Maker, you must use MPICH.
Our version of MPICH does not use the mpirun/mpiexec wrappers, instead use srun:</p><pre><code class=language-bash>#!/bin/bash -l

#SBATCH -p epyc
#SBATCH --ntasks=32
#SBATCH --mem=16gb
#SBATCH --time=01:00:00

# Load maker
module load maker/2.31.11

mpirun maker # Provide appropriate maker options here

</code></pre><h2 id=more-examples>More examples</h2><p>The range of differing jobs and how to submit them is endless:</p><pre><code>1. Singularity containers
2. Database services
3. Graphical user interfaces
4. Etc ...
</code></pre><p>For a growing list of examples please visit <a href=https://github.com/ucr-hpcc/hpcc_slurm_examples>HPCC Slurm Examples</a>.</p><section class=footnotes role=doc-endnotes><hr><ol><li id=fn:1 role=doc-endnote><p>These only list the most common CPU Extensions for each platform. A full list of supported extensions can be found using the <code>lscpu</code> command on the respective node type.&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></section><div class="text-muted mt-5 pt-3 border-top">Last modified June 17, 2025: <a href=https://github.com/ucr-hpcc/ucr-hpcc.github.io/commit/07453f7b99fce25cbb732d9515acea56e5dde457>Update jobs.md (07453f7b9)</a></div></div></main></div></div><footer class="bg-dark py-5 row d-print-none"><div class="container-fluid mx-sm-5"><div class=row><div class="col-6 col-sm-4 text-xs-center order-sm-2"><ul class="list-inline mb-0"><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title="Email Support" aria-label="Email Support"><a class=text-white target=_blank rel="noopener noreferrer" href=mailto:support@hpcc.ucr.edu><i class="fa fa-envelope"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Twitter aria-label=Twitter><a class=text-white target=_blank rel="noopener noreferrer" href=https://twitter.com/UCR_HPCC><i class="fab fa-twitter"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title="Stack Overflow" aria-label="Stack Overflow"><a class=text-white target=_blank rel="noopener noreferrer" href=https://ucr-hpcc.slack.com/><i class="fab fa-slack"></i></a></li></ul></div><div class="col-6 col-sm-4 text-right text-xs-center order-sm-3"><ul class="list-inline mb-0"><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title aria-label><a class=text-white target=_blank rel="noopener noreferrer" href><i></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title aria-label><a class=text-white target=_blank rel="noopener noreferrer" href><i></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title aria-label><a class=text-white target=_blank rel="noopener noreferrer" href><i></i></a></li></ul></div><div class="col-12 col-sm-4 text-center py-2 order-sm-2"><small class=text-white>&copy; 2025 The Docsy Authors All Rights Reserved</small>
<small class=ml-1><a href=https://policies.google.com/privacy target=_blank>Privacy Policy</a></small></div></div></div></footer></div><script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.3/umd/popper.min.js integrity=sha384-ZMP7rVo3mIykV+2+9J3UJ46jBk0WLaUAdn689aCwoqbBJiSnjAK/l8WvCWPIPm49 crossorigin=anonymous></script><script src=https://stackpath.bootstrapcdn.com/bootstrap/4.1.3/js/bootstrap.min.js integrity=sha384-ChfqqxuZUCnJSK3+MXmPNIyE6ZbWh2IMqE241rYiqJxyMiZ6OW/JmZQ5stwEULTy crossorigin=anonymous></script><script>(function(){var c,a,b,d=document.getElementsByTagName('code');for(c=0;c<d.length;){if(b=d[c],b.parentNode.tagName!=='PRE'&&b.childElementCount===0)if(a=b.textContent,/^\$[^$]/.test(a)&&/[^$]\$$/.test(a)&&(a=a.replace(/^\$/,'\\(').replace(/\$$/,'\\)'),b.textContent=a),/^\\\((.|\s)+\\\)$/.test(a)||/^\\\[(.|\s)+\\\]$/.test(a)||/^\$(.|\s)+\$$/.test(a)||/^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(a)){b.outerHTML=b.innerHTML;continue}c++}})()</script><script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?version=4.8.0&features=es6"></script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><script type=text/x-mathjax-config>
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
        processEscapes: true,
        processEnvironments: true
    },
    // Center justify equations in code and markdown cells. Elsewhere
    // we use CSS to left justify single line equations in code cells.
    displayAlign: 'center',
    "HTML-CSS": {
        styles: {'.MathJax_Display': {"margin": 0}},
        linebreaks: { automatic: true }
    }
});
</script><script src=../../../js/main.min.63db3e552bd0543be17ce4a79a18b744e9cb72256bec42b540e3ab0cd43722d0.js integrity="sha256-Y9s+VSvQVDvhfOSnmhi3ROnLciVr7EK1QOOrDNQ3ItA=" crossorigin=anonymous></script><script src=../../../js/prism.js></script></body></html>