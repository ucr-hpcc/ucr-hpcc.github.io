<!doctype html><html lang=en class=no-js><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=generator content="Hugo 0.83.1"><meta name=ROBOTS content="INDEX, FOLLOW"><link rel="shortcut icon" href=../../../favicons/favicon.ico><link rel=apple-touch-icon href=../../../favicons/apple-touch-icon-180x180.png sizes=180x180><link rel=icon type=image/png href=../../../favicons/favicon-16x16.png sizes=16x16><link rel=icon type=image/png href=../../../favicons/favicon-32x32.png sizes=32x32><link rel=icon type=image/png href=../../../favicons/android-36x36.png sizes=36x36><link rel=icon type=image/png href=../../../favicons/android-48x48.png sizes=48x48><link rel=icon type=image/png href=../../../favicons/android-72x72.png sizes=72x72><link rel=icon type=image/png href=../../../favicons/android-96x96.png sizes=96x96><link rel=icon type=image/png href=../../../favicons/android-144x144.png sizes=144x144><link rel=icon type=image/png href=../../../favicons/android-192x192.png sizes=192x192><title>Queue Policies | HPCC</title><meta property="og:title" content="Queue Policies"><meta property="og:description" content="Start Times Start times are a great way to track your jobs:
squeue -u $USER --start  Start times are rough estimates based on the current state of the queue.
Partition Quotas Each partition has a specific usecase. Below outlines each partition, it&rsquo;s usecase, as well as any job/user/group limits that are in place. Empty boxes imply no limit, but is still limited by the next higher limit. Job limits are capped by user limits, and user limits are capped by group limits."><meta property="og:type" content="article"><meta property="og:url" content="https://hpcc.ucr.edu/manuals/hpc_cluster/queue/"><meta property="article:section" content="Manuals"><meta property="article:modified_time" content="2025-06-26T09:18:22-07:00"><meta property="og:site_name" content="HPCC"><meta itemprop=name content="Queue Policies"><meta itemprop=description content="Start Times Start times are a great way to track your jobs:
squeue -u $USER --start  Start times are rough estimates based on the current state of the queue.
Partition Quotas Each partition has a specific usecase. Below outlines each partition, it&rsquo;s usecase, as well as any job/user/group limits that are in place. Empty boxes imply no limit, but is still limited by the next higher limit. Job limits are capped by user limits, and user limits are capped by group limits."><meta itemprop=dateModified content="2025-06-26T09:18:22-07:00"><meta itemprop=wordCount content="2342"><meta itemprop=keywords content><meta name=twitter:card content="summary"><meta name=twitter:title content="Queue Policies"><meta name=twitter:description content="Start Times Start times are a great way to track your jobs:
squeue -u $USER --start  Start times are rough estimates based on the current state of the queue.
Partition Quotas Each partition has a specific usecase. Below outlines each partition, it&rsquo;s usecase, as well as any job/user/group limits that are in place. Empty boxes imply no limit, but is still limited by the next higher limit. Job limits are capped by user limits, and user limits are capped by group limits."><link rel=preload href=../../../scss/main.min.398442d9d5d4e5f14be4a4e8e9b450b7e55d46418388cfab16ab414422fad5fa.css as=style><link href=../../../scss/main.min.398442d9d5d4e5f14be4a4e8e9b450b7e55d46418388cfab16ab414422fad5fa.css rel=stylesheet integrity><script src=https://code.jquery.com/jquery-3.5.1.min.js integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0=" crossorigin=anonymous></script><script src=https://unpkg.com/lunr@2.3.8/lunr.min.js integrity=sha384-vRQ9bDyE0Wnu+lMfm57BlYLO0/XauFuKpVsZPs7KEDwYKktWi5+Kz3MP8++DFlRY crossorigin=anonymous></script><link rel=stylesheet href=../../../css/prism.css><script src=../../../js/driver0.9.8.min.js></script><link rel=stylesheet href=../../../css/driver0.9.8.min.css><link rel=stylesheet href=../../../css/site.css><script src=../../../js/site.js></script></head><body class="td-page line-numbers"><header><nav class="js-navbar-scroll navbar navbar-expand navbar-dark flex-column flex-md-row td-navbar"><a class=navbar-brand href=../../../><span class=navbar-logo><svg height="16" id="Layer_1" style="enable-background:new 0 0 16 16" viewBox="0 0 16 16" width="16" sodipodi:docname="126572_home_house_icon.svg" inkscape:export-filename="/home/jhayes/126572_home_house_icon.png" inkscape:export-xdpi="96" inkscape:export-ydpi="96" inkscape:version="1.1 (c4e8f9ed74, 2021-05-24)" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape" xmlns:sodipodi="http://sodipodi.sourceforge.net/DTD/sodipodi-0.dtd" xmlns="http://www.w3.org/2000/svg" xmlns:svg="http://www.w3.org/2000/svg"><defs id="defs7"/><sodipodi:namedview id="namedview5" pagecolor="#ffffff" bordercolor="#666666" borderopacity="1" inkscape:pageshadow="2" inkscape:pageopacity="0" inkscape:pagecheckerboard="0" showgrid="false" inkscape:zoom="49.0625" inkscape:cx="3.5464968" inkscape:cy="7.9694268" inkscape:window-width="3440" inkscape:window-height="1391" inkscape:window-x="0" inkscape:window-y="24" inkscape:window-maximized="1" inkscape:current-layer="Layer_1"/><path style="display:inline;fill:#fff;fill-opacity:1;stroke-width:.0203822" d="m2.8659775 15.977961c-.3936273-.051-.7525483-.389889-.8358103-.789155-.015862-.07606-.022524-1.009027-.022524-3.15414V8.988535H1.4412249C.82810729 8.988535.73920188 8.977312.53578892 8.8742347.39201211 8.8013774.21162731 8.6243834.13540635 8.4813782.01718088 8.2595644-.01861304 7.986667.04066723 7.7590746c.03869392-.148556.08395752-.2367427.20614877-.4016385C.36550378 7.1972684 1.9544118 5.5981695 5.0547771 2.5186377L7.3783439.21068447 7.5602468.12062886 7.7421497.03057325h.2563137c.246189.0.2617641.0025817.3942926.06535805.075888.03594693.1905382.10669563.254777.15721933.064239.0505237.6212568.59095827 1.2378173 1.20096567l1.1210187 1.1091045.01139-.4000817c.01-.3510397.01694-.413483.05667-.5094082.08521-.2057341.220395-.3661097.415434-.4928535.209421-.1360904.280363-.1452781 1.065556-.1380008l.692994.00642.159642.076227c.292895.1398524.506525.4124209.562217.7173255.01479.080962.02209.7172189.02209 1.9247442v1.8038031l.83131.8370395c.970032.9767169 1.061962 1.0876674 1.135666 1.3706362.134824.5176265-.189416 1.0615314-.71432 1.1982515-.09368.024401-.243383.031209-.686238.031209h-.566418v3.0331994c0 2.090488-.0069 3.070788-.02209 3.15414-.07531.412343-.432865.748345-.854561.803061-.08611.01117-.794528.02008-1.62526.02044L10.017834 16 10.017431 13.498089C10.01718 11.939735 10.009334 10.942371 9.9966286 10.853503 9.9447492 10.490634 9.6889196 10.179327 9.3299104 10.042205 9.2140078 9.9979369 9.1996048 9.9974522 8 9.9974522s-1.2140078 4851e-7-1.3299104.044753c-.3590092.137122-.6148388.448429-.6667182.811298-.012705.08887-.02055 1.086232-.020802 2.644586L5.9821656 16 4.489172 15.997416c-.8211465-.0014-1.551584-.01018-1.6231945-.01945z" id="path58637" sodipodi:insensitive="true"/></svg></span><span class=font-weight-bold>HPCC</span></a>
<row class=header-switch><div class="custom-control custom-switch"><input type=checkbox class=custom-control-input id=sidebarSwitch checked>
<label class=custom-control-label for=sidebarSwitch>sidebar</label></div><div class="custom-control custom-switch"><input type=checkbox class=custom-control-input id=tocSwitch checked>
<label class=custom-control-label for=tocSwitch>toc</label></div></row><div class="td-navbar-nav-scroll ml-md-auto" id=main_navbar><ul class="navbar-nav mt-2 mt-lg-0"><li class="nav-item mr-4 mb-2 mb-lg-0" style=position:relative><i class="fas fa-info-circle btn-primary"></i><a class="nav-link dropdown-toggle" href=../../../about/ id=navbarDropdown role=button>
About</a><div class=dropdown-menu aria-labelledby=navbarDropdownMenuLink><a class=dropdown-item href=../../../about/overview/ style=padding-left:1rem;font-weight:calc(3/3*500)>Overview</a>
<a class=dropdown-item href=../../../about/overview/introduction/ style=padding-left:2rem;font-weight:calc(3/4*500)>Introduction</a>
<a class=dropdown-item href=../../../about/overview/access/ style=padding-left:2rem;font-weight:calc(3/4*500)>Access</a>
<a class=dropdown-item href=../../../about/overview/activity/ style=padding-left:2rem;font-weight:calc(3/4*500)>Activity Report</a>
<a class=dropdown-item href=../../../about/overview/rates/ style=padding-left:2rem;font-weight:calc(3/4*500)>Rates</a>
<a class=dropdown-item href=../../../about/overview/contact/ style=padding-left:2rem;font-weight:calc(3/4*500)>Contact</a>
<a class=dropdown-item href=../../../about/overview/acknowledgement/ style=padding-left:2rem;font-weight:calc(3/4*500)>Acknowledgement</a>
<a class=dropdown-item href=../../../about/hardware/ style=padding-left:1rem;font-weight:calc(3/3*500)>Hardware</a>
<a class=dropdown-item href=../../../about/hardware/overview style=padding-left:2rem;font-weight:calc(3/4*500)>Overview</a>
<a class=dropdown-item href=../../../about/hardware/details style=padding-left:2rem;font-weight:calc(3/4*500)>Details</a>
<a class=dropdown-item href=../../../about/software/ style=padding-left:1rem;font-weight:calc(3/3*500)>Software</a>
<a class=dropdown-item href=../../../about/software/conda_packages/ style=padding-left:2rem;font-weight:calc(3/4*500)>Conda Packages</a>
<a class=dropdown-item href=../../../about/software/installs/ style=padding-left:2rem;font-weight:calc(3/4*500)>Installs</a>
<a class=dropdown-item href=../../../about/software/modules/ style=padding-left:2rem;font-weight:calc(3/4*500)>Modules</a>
<a class=dropdown-item href=../../../about/software/system/ style=padding-left:2rem;font-weight:calc(3/4*500)>System</a>
<a class=dropdown-item href=../../../about/software/commercial style=padding-left:2rem;font-weight:calc(3/4*500)>Commercial</a></div></li><li class="nav-item mr-4 mb-2 mb-lg-0" style=position:relative><i class="fas fa-newspaper btn-primary"></i><a class="nav-link dropdown-toggle" href=../../../news id=navbarDropdown role=button>
News</a><div class=dropdown-menu aria-labelledby=navbarDropdownMenuLink><a class=dropdown-item href=../../../news/announce/ style=padding-left:1rem;font-weight:calc(3/3*500)>Announcements</a>
<a class=dropdown-item href=../../../news/alerts/ style=padding-left:1rem;font-weight:calc(3/3*500)>Alerts</a>
<a class=dropdown-item href=../../../changes/ style=padding-left:1rem;font-weight:calc(3/3*500)>Upgrades</a></div></li><li class="nav-item mr-4 mb-2 mb-lg-0" style=position:relative><i class="fas fa-users btn-primary"></i><a class="nav-link dropdown-toggle" href=../../../events id=navbarDropdown role=button>
Events</a><div class=dropdown-menu aria-labelledby=navbarDropdownMenuLink><a class=dropdown-item href=../../../events/small/ style=padding-left:1rem;font-weight:calc(3/3*500)>Workshops</a>
<a class=dropdown-item href=../../../events/related/ style=padding-left:1rem;font-weight:calc(3/3*500)>Related Events</a></div></li><li class="nav-item mr-4 mb-2 mb-lg-0" style=position:relative><i class="fas fa-book btn-primary"></i><a class="nav-link dropdown-toggle" href=../../../manuals id=navbarDropdown role=button>
Manuals</a><div class=dropdown-menu aria-labelledby=navbarDropdownMenuLink><a class=dropdown-item href=../../../manuals/access style=padding-left:1rem;font-weight:calc(3/3*500)>Access</a>
<a class=dropdown-item href=../../../about/facility/access/ style=padding-left:2rem;font-weight:calc(3/4*500)>How to Get Account?</a>
<a class=dropdown-item href=../../../manuals/access/login style=padding-left:2rem;font-weight:calc(3/4*500)>Login</a>
<a class=dropdown-item href=../../../manuals/linux_basics/ style=padding-left:1rem;font-weight:calc(3/3*500)>Linux Basics</a>
<a class=dropdown-item href=../../../manuals/linux_basics/cmdline_basics style=padding-left:2rem;font-weight:calc(3/4*500)>Command Line Basics</a>
<a class=dropdown-item href=../../../manuals/linux_basics/filesystems style=padding-left:2rem;font-weight:calc(3/4*500)>File Systems and Transfers</a>
<a class=dropdown-item href=../../../manuals/linux_basics/permissions style=padding-left:2rem;font-weight:calc(3/4*500)>Permissions and Ownership</a>
<a class=dropdown-item href=../../../manuals/linux_basics/finding_things style=padding-left:2rem;font-weight:calc(3/4*500)>Finding Things</a>
<a class=dropdown-item href=../../../manuals/linux_basics/text style=padding-left:2rem;font-weight:calc(3/4*500)>Text Editors</a>
<a class=dropdown-item href=../../../manuals/linux_basics/streams style=padding-left:2rem;font-weight:calc(3/4*500)>Streams</a>
<a class=dropdown-item href=../../../manuals/linux_basics/pipes style=padding-left:2rem;font-weight:calc(3/4*500)>Piping</a>
<a class=dropdown-item href=../../../manuals/linux_basics/variables style=padding-left:2rem;font-weight:calc(3/4*500)>Variables</a>
<a class=dropdown-item href=../../../manuals/linux_basics/scripting style=padding-left:2rem;font-weight:calc(3/4*500)>Scripting</a>
<a class=dropdown-item href=../../../manuals/linux_basics/processes style=padding-left:2rem;font-weight:calc(3/4*500)>Processe Management</a>
<a class=dropdown-item href=../../../manuals/linux_basics/shell style=padding-left:2rem;font-weight:calc(3/4*500)>Shell Bootcamp</a>
<a class=dropdown-item href=../../../manuals/hpc_cluster/ style=padding-left:1rem;font-weight:calc(3/3*500)>HPC Cluster</a>
<a class=dropdown-item href=../../../manuals/hpc_cluster/intro style=padding-left:2rem;font-weight:calc(3/4*500)>Introduction</a>
<a class=dropdown-item href=../../../manuals/hpc_cluster/start style=padding-left:2rem;font-weight:calc(3/4*500)>Getting Started</a>
<a class=dropdown-item href=../../../manuals/hpc_cluster/jobs style=padding-left:2rem;font-weight:calc(3/4*500)>Managing Jobs</a>
<a class=dropdown-item href=../../../manuals/hpc_cluster/queue style=padding-left:2rem;font-weight:calc(3/4*500)>Queue Policies</a>
<a class=dropdown-item href=../../../manuals/hpc_cluster/package_manage style=padding-left:2rem;font-weight:calc(3/4*500)>Package Management</a>
<a class=dropdown-item href=../../../manuals/hpc_cluster/selected_software style=padding-left:2rem;font-weight:calc(3/4*500)>Selected Software</a>
<a class=dropdown-item href=../../../manuals/hpc_cluster/storage style=padding-left:2rem;font-weight:calc(3/4*500)>Data Storage</a>
<a class=dropdown-item href=../../../manuals/hpc_cluster/sharing style=padding-left:2rem;font-weight:calc(3/4*500)>Sharing Data</a>
<a class=dropdown-item href=../../../manuals/hpc_cluster/security style=padding-left:2rem;font-weight:calc(3/4*500)>Data Security</a>
<a class=dropdown-item href=../../../manuals/hpc_cluster/users style=padding-left:2rem;font-weight:calc(3/4*500)>Communicating</a>
<a class=dropdown-item href=../../../manuals/hpc_cluster/terminalide style=padding-left:2rem;font-weight:calc(3/4*500)>Terminal IDEs</a>
<a class=dropdown-item href=../../../manuals/hpc_cluster/parallelr style=padding-left:2rem;font-weight:calc(3/4*500)>Parallel Evaluations in R</a>
<a class=dropdown-item href=../../../manuals/hpc_cluster/sshkeys style=padding-left:2rem;font-weight:calc(3/4*500)>SSH Keys</a>
<a class=dropdown-item href=../../../manuals/ext_cloud/ style=padding-left:1rem;font-weight:calc(3/3*500)>Cloud/External</a>
<a class=dropdown-item href=../../../manuals/hpc_cluster/visual style=padding-left:2rem;font-weight:calc(3/4*500)>Visualization</a>
<a class=dropdown-item href=../../../manuals/ext_cloud/aws/ style=padding-left:2rem;font-weight:calc(3/4*500)>AWS</a></div></li><li class="nav-item mr-4 mb-2 mb-lg-0" style=position:relative><i class="fas fa-share-alt btn-primary"></i><a class="nav-link dropdown-toggle" href=../../../links/ id=navbarDropdown role=button>
Links</a><div class=dropdown-menu aria-labelledby=navbarDropdownMenuLink><a class=dropdown-item href=https://research.ucr.edu style=padding-left:1rem;font-weight:calc(3/3*500)>RED-UCR</a>
<a class=dropdown-item href=http://datascience.ucr.edu style=padding-left:1rem;font-weight:calc(3/3*500)>Data Science Center</a>
<a class=dropdown-item href=http://bigdata.ucr.edu style=padding-left:1rem;font-weight:calc(3/3*500)>FIELDS Program</a>
<a class=dropdown-item href=http://genomics.ucr.edu style=padding-left:1rem;font-weight:calc(3/3*500)>Institute of Integrative Genome Biology</a></div></li></ul></div><div class="navbar-nav d-none d-lg-block"><input type=search class="form-control td-search-input" placeholder="&#xf002; Search this site…" aria-label="Search this site…" autocomplete=off data-offline-search-index-json-src=../../../offline-search-index.0db6e931b1e7a6a0aed085cc679f3a59.json data-offline-search-base-href=../../../ data-offline-search-max-results=10></div></nav></header><div class="container-fluid td-outer"><div class=td-main><div class="row flex-xl-nowrap"><div class="col-12 col-md-3 col-xl-2 td-sidebar d-print-none"><div id=td-sidebar-menu class=td-sidebar__inner><form class="td-sidebar__search d-flex align-items-center d-lg-none"><input type=search class="form-control td-search-input" placeholder="&#xf002; Search this site…" aria-label="Search this site…" autocomplete=off data-offline-search-index-json-src=../../../offline-search-index.0db6e931b1e7a6a0aed085cc679f3a59.json data-offline-search-base-href=../../../ data-offline-search-max-results=10>
<button class="btn btn-link td-sidebar__toggle d-md-none p-0 ml-3 fas fa-bars" type=button data-toggle=collapse data-target=#td-section-nav aria-controls=td-docs-nav aria-expanded=false aria-label="Toggle section navigation"></button></form><nav class="collapse td-sidebar-nav" id=td-section-nav><ul class="td-sidebar-nav__section pr-md-3"><li class=td-sidebar-nav__section-title><a href=../../../manuals/ class="align-left pl-0 pr-2 td-sidebar-link td-sidebar-link__section">Manuals</a></li><ul><li class="collapse show" id=manuals><ul class="td-sidebar-nav__section pr-md-3"><li class=td-sidebar-nav__section-title><a href=../../../manuals/access/ class="align-left pl-0 pr-2 collapsed td-sidebar-link td-sidebar-link__section">Access</a></li><ul><li class=collapse id=manualsaccess><a class="td-sidebar-link td-sidebar-link__page" id=m-manualsaccesslogin href=../../../manuals/access/login/>Login</a></li></ul></ul><ul class="td-sidebar-nav__section pr-md-3"><li class=td-sidebar-nav__section-title><a href=../../../manuals/linux_basics/ class="align-left pl-0 pr-2 collapsed td-sidebar-link td-sidebar-link__section">Linux Basics</a></li><ul><li class=collapse id=manualslinux_basics><a class="td-sidebar-link td-sidebar-link__page" id=m-manualslinux_basicscmdline_basics href=../../../manuals/linux_basics/cmdline_basics/>Command Line Basics</a>
<a class="td-sidebar-link td-sidebar-link__page" id=m-manualslinux_basicsfilesystems href=../../../manuals/linux_basics/filesystems/>File Systems and Transfers</a>
<a class="td-sidebar-link td-sidebar-link__page" id=m-manualslinux_basicspermissions href=../../../manuals/linux_basics/permissions/>Permissions and Ownership</a>
<a class="td-sidebar-link td-sidebar-link__page" id=m-manualslinux_basicsfinding_things href=../../../manuals/linux_basics/finding_things/>Finding Things</a>
<a class="td-sidebar-link td-sidebar-link__page" id=m-manualslinux_basicstext href=../../../manuals/linux_basics/text/>Text Editors</a>
<a class="td-sidebar-link td-sidebar-link__page" id=m-manualslinux_basicsstreams href=../../../manuals/linux_basics/streams/>Streams</a>
<a class="td-sidebar-link td-sidebar-link__page" id=m-manualslinux_basicspipes href=../../../manuals/linux_basics/pipes/>Piping</a>
<a class="td-sidebar-link td-sidebar-link__page" id=m-manualslinux_basicsvariables href=../../../manuals/linux_basics/variables/>Variables</a>
<a class="td-sidebar-link td-sidebar-link__page" id=m-manualslinux_basicsscripting href=../../../manuals/linux_basics/scripting/>Scripting</a>
<a class="td-sidebar-link td-sidebar-link__page" id=m-manualslinux_basicsprocesses href=../../../manuals/linux_basics/processes/>Process Management</a>
<a class="td-sidebar-link td-sidebar-link__page" id=m-manualslinux_basicsshell href=../../../manuals/linux_basics/shell/>Shell Bootcamp</a></li></ul></ul><ul class="td-sidebar-nav__section pr-md-3"><li class=td-sidebar-nav__section-title><a href=../../../manuals/hpc_cluster/ class="align-left pl-0 pr-2 active td-sidebar-link td-sidebar-link__section">HPC Cluster</a></li><ul><li class="collapse show" id=manualshpc_cluster><a class="td-sidebar-link td-sidebar-link__page" id=m-manualshpc_clusterintro href=../../../manuals/hpc_cluster/intro/>Introduction</a>
<a class="td-sidebar-link td-sidebar-link__page" id=m-manualshpc_clusterstart href=../../../manuals/hpc_cluster/start/>Getting Started</a>
<a class="td-sidebar-link td-sidebar-link__page" id=m-manualshpc_clusterjobs href=../../../manuals/hpc_cluster/jobs/>Managing Jobs</a>
<a class="td-sidebar-link td-sidebar-link__page active" id=m-manualshpc_clusterqueue href=../../../manuals/hpc_cluster/queue/>Queue Policies</a>
<a class="td-sidebar-link td-sidebar-link__page" id=m-manualshpc_clusterpackage_manage href=../../../manuals/hpc_cluster/package_manage/>Package Management</a><ul class="td-sidebar-nav__section pr-md-3"><li class=td-sidebar-nav__section-title><a href=../../../manuals/hpc_cluster/selected_software/ class="align-left pl-0 pr-2 collapsed td-sidebar-link td-sidebar-link__section">Selected Software</a></li><ul><li class=collapse id=manualshpc_clusterselected_software><a class="td-sidebar-link td-sidebar-link__page" id=m-manualshpc_clusterselected_softwarealphafold href=../../../manuals/hpc_cluster/selected_software/alphafold/>AlphaFold</a>
<a class="td-sidebar-link td-sidebar-link__page" id=m-manualshpc_clusterselected_softwareondemand href=../../../manuals/hpc_cluster/selected_software/ondemand/>Open OnDemand</a>
<a class="td-sidebar-link td-sidebar-link__page" id=m-manualshpc_clusterselected_softwarepytorch_in_jupyter href=../../../manuals/hpc_cluster/selected_software/pytorch_in_jupyter/>PyTorch In Jupyter</a>
<a class="td-sidebar-link td-sidebar-link__page" id=m-manualshpc_clusterselected_softwarevscode href=../../../manuals/hpc_cluster/selected_software/vscode/>VSCode</a></li></ul></ul><a class="td-sidebar-link td-sidebar-link__page" id=m-manualshpc_clusterstorage href=../../../manuals/hpc_cluster/storage/>Data Storage</a>
<a class="td-sidebar-link td-sidebar-link__page" id=m-manualshpc_clustersharing href=../../../manuals/hpc_cluster/sharing/>Sharing Data</a>
<a class="td-sidebar-link td-sidebar-link__page" id=m-manualshpc_clustersecurity href=../../../manuals/hpc_cluster/security/>Data Security</a>
<a class="td-sidebar-link td-sidebar-link__page" id=m-manualshpc_clusterusers href=../../../manuals/hpc_cluster/users/>Communicating</a>
<a class="td-sidebar-link td-sidebar-link__page" id=m-manualshpc_clusterterminalide href=../../../manuals/hpc_cluster/terminalide/>Terminal IDEs</a>
<a class="td-sidebar-link td-sidebar-link__page" id=m-manualshpc_clusterparallelr href=../../../manuals/hpc_cluster/parallelr/>Parallel Evaluations in R</a><ul class="td-sidebar-nav__section pr-md-3"><li class=td-sidebar-nav__section-title><a href=../../../manuals/hpc_cluster/sshkeys/ class="align-left pl-0 pr-2 collapsed td-sidebar-link td-sidebar-link__section">SSH Keys</a></li><ul><li class=collapse id=manualshpc_clustersshkeys><a class="td-sidebar-link td-sidebar-link__page" id=m-manualshpc_clustersshkeyssshkeys_simple href=../../../manuals/hpc_cluster/sshkeys/sshkeys_simple/>SSH Keys Summary</a>
<a class="td-sidebar-link td-sidebar-link__page" id=m-manualshpc_clustersshkeyssshkeys_macos href=../../../manuals/hpc_cluster/sshkeys/sshkeys_macos/>SSH Keys Apple macOS</a>
<a class="td-sidebar-link td-sidebar-link__page" id=m-manualshpc_clustersshkeyssshkeys_winos href=../../../manuals/hpc_cluster/sshkeys/sshkeys_winos/>SSH Keys Microsoft Windows</a>
<a class="td-sidebar-link td-sidebar-link__page" id=m-manualshpc_clustersshkeyssshkeys_linux href=../../../manuals/hpc_cluster/sshkeys/sshkeys_linux/>SSH Keys Linux</a></li></ul></ul><a class="td-sidebar-link td-sidebar-link__page" id=m-manualshpc_clustervisual href=../../../manuals/hpc_cluster/visual/>Visualization</a>
<a class="td-sidebar-link td-sidebar-link__page" id=m-manualshpc_clustersingularity href=../../../manuals/hpc_cluster/singularity/>Singularity Jobs</a><ul class="td-sidebar-nav__section pr-md-3"><li class=td-sidebar-nav__section-title><a href=../../../manuals/hpc_cluster/data/ class="align-left pl-0 pr-2 collapsed td-sidebar-link td-sidebar-link__section">Data Transfer</a></li><ul><li class=collapse id=manualshpc_clusterdata><a class="td-sidebar-link td-sidebar-link__page" id=m-manualshpc_clusterdataglobus href=../../../manuals/hpc_cluster/data/globus/>Globus Connect Personal</a></li></ul></ul></li></ul></ul><ul class="td-sidebar-nav__section pr-md-3"><li class=td-sidebar-nav__section-title><a href=../../../manuals/ext_cloud/ class="align-left pl-0 pr-2 collapsed td-sidebar-link td-sidebar-link__section">Cloud/External</a></li><ul><li class=collapse id=manualsext_cloud><ul class="td-sidebar-nav__section pr-md-3"><li class=td-sidebar-nav__section-title><a href=../../../manuals/ext_cloud/aws/ class="align-left pl-0 pr-2 collapsed td-sidebar-link td-sidebar-link__section">AWS</a></li><ul><li class=collapse id=manualsext_cloudaws><a class="td-sidebar-link td-sidebar-link__page" id=m-manualsext_cloudawsegress href=../../../manuals/ext_cloud/aws/egress/>Account Egress Waiver</a>
<a class="td-sidebar-link td-sidebar-link__page" id=m-manualsext_cloudawsbilling href=../../../manuals/ext_cloud/aws/billing/>Cost Control and Billing</a></li></ul></ul></li></ul></ul></li></ul></ul></nav></div></div><div class="d-none d-xl-block col-xl-2 td-toc d-print-none"><div class="td-page-meta ml-2 pb-1 pt-2 mb-0"><a href=https://raw.githubusercontent.com/ucr-hpcc/ucr-hpcc.github.io/master/content/en/Manuals/hpc_cluster/queue.md target=_blank class=source-link><i class="fa fa-code fa-fw"></i> View source code</a>
<a href=https://github.com/ucr-hpcc/ucr-hpcc.github.io/edit/master/content/en/Manuals/hpc_cluster/queue.md target=_blank><i class="fa fa-edit fa-fw"></i> Edit this page</a>
<a href="https://github.com/ucr-hpcc/ucr-hpcc.github.io/new/master/content/en/Manuals/hpc_cluster/queue.md?filename=change-me.md&value=---%0Atitle%3A+%22Long+Page+Title%22%0AlinkTitle%3A+%22Short+Nav+Title%22%0Aweight%3A+100%0Adescription%3A+%3E-%0A+++++Page+description+for+heading+and+indexes.%0A---%0A%0A%23%23+Heading%0A%0AEdit+this+template+to+create+your+new+page.%0A%0A%2A+Give+it+a+good+name%2C+ending+in+%60.md%60+-+e.g.+%60getting-started.md%60%0A%2A+Edit+the+%22front+matter%22+section+at+the+top+of+the+page+%28weight+controls+how+its+ordered+amongst+other+pages+in+the+same+directory%3B+lowest+number+first%29.%0A%2A+Add+a+good+commit+message+at+the+bottom+of+the+page+%28%3C80+characters%3B+use+the+extended+description+field+for+more+detail%29.%0A%2A+Create+a+new+branch+so+you+can+preview+your+new+file+and+request+a+review+via+Pull+Request.%0A" target=_blank><i class="fa fa-edit fa-fw"></i> Create child page</a>
<a href="https://github.com/ucr-hpcc/ucr-hpcc.github.io/issues/new?title=Queue%20Policies" target=_blank><i class="fab fa-github fa-fw"></i> Create documentation issue</a></div><nav id=TableOfContents><ul><li><a href=#start-times>Start Times</a></li><li><a href=#partition-quotas>Partition Quotas</a><ul><li><a href=#external-labs>External Labs</a></li><li><a href=#private-node-ownership>Private Node Ownership</a></li><li><a href=#additional-resource-request>Additional Resource Request</a></li><li><a href=#example-scenarios>Example Scenarios</a><ul><li><a href=#per-job-limit>Per-Job Limit</a></li><li><a href=#per-user-limit>Per-User Limit</a></li><li><a href=#per-lab-limit>Per-Lab Limit</a></li></ul></li></ul></li><li><a href=#changing-partitions>Changing Partitions</a></li><li><a href=#fair-share>Fair-Share</a></li><li><a href=#priority>Priority</a></li><li><a href=#backfill>Backfill</a></li><li><a href=#priority-partition>Priority Partition</a></li><li><a href=#using-the-preempt-partitions-tenative>Using the Preempt Partitions (TENATIVE)</a><ul><li><a href=#job-limitations>Job Limitations</a><ul><li><a href=#time>Time</a></li><li><a href=#resources>Resources</a></li></ul></li><li><a href=#starting-a-job>Starting a job</a><ul><li><a href=#interactive-example>Interactive Example</a></li><li><a href=#non-interactive-batch-example>Non-interactive (batch) Example</a></li><li><a href=#selecting-resources>Selecting Resources</a></li></ul></li></ul></li></ul></nav></div><main class="col-12 col-md-9 col-xl-8 pl-md-5" role=main><nav aria-label=breadcrumb class="d-none d-md-block d-print-none"><ol class="breadcrumb spb-1"><li class=breadcrumb-item><a href=https://hpcc.ucr.edu/manuals/>Manuals</a></li><li class=breadcrumb-item><a href=https://hpcc.ucr.edu/manuals/hpc_cluster/>HPC Cluster</a></li><li class="breadcrumb-item active" aria-current=page><a href=https://hpcc.ucr.edu/manuals/hpc_cluster/queue/>Queue Policies</a></li></ol></nav><div class=td-content><h1>Queue Policies</h1><p class=reading-time><i class="fa fa-clock" aria-hidden=true></i> 11 minute read</p><h2 id=start-times>Start Times</h2><p>Start times are a great way to track your jobs:</p><pre><code class=language-bash>squeue -u $USER --start
</code></pre><p>Start times are rough estimates based on the current state of the queue.</p><h2 id=partition-quotas>Partition Quotas</h2><p>Each partition has a specific usecase. Below outlines each partition, it&rsquo;s usecase, as well as any job/user/group limits that are in place.
Empty boxes imply no limit, but is still limited by the next higher limit. Job limits are capped by user limits, and user limits are capped by group limits.</p><table><thead><tr><th>Partition Name</th><th>Usecase</th><th>Per-User Limit</th><th>Per-Job Limit</th><th>Max Job Time</th></tr></thead><tbody><tr><td>epyc (2021 CPU)</td><td>CPU Intensive Workloads, Multithreaded, MPI, OpenMP</td><td>384 Cores, 1TB memory <sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup></td><td>64GB memory per Core <sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup>,<sup id=fnref:3><a href=#fn:3 class=footnote-ref role=doc-noteref>3</a></sup></td><td>30 Days</td></tr><tr><td>intel (2016 CPU)</td><td>CPU Intensive Workloads, Multithreaded, MPI, OpenMP</td><td>384 Cores, 1TB memory <sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup></td><td>64GB memory per Core <sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup>,<sup id=fnref:4><a href=#fn:4 class=footnote-ref role=doc-noteref>4</a></sup></td><td>30 Days</td></tr><tr><td>batch (2012 CPU)</td><td>CPU Intensive Workloads, Multithreaded, MPI, OpenMP</td><td>384 Cores, 1TB memory <sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup></td><td>64GB memory per Core <sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup>,<sup id=fnref:5><a href=#fn:5 class=footnote-ref role=doc-noteref>5</a></sup></td><td>30 Days</td></tr><tr><td>short</td><td>Short CPU Intensive Workloads, Multithreaded, MPI, OpenMP</td><td>384 Cores, 1TB memory</td><td>64GB memory per Core, 2-hour time limit</td><td>2 Hours</td></tr><tr><td>highmem</td><td>Memory Intensive Workloads</td><td>32 Cores, 2TB memory</td><td></td><td>30 Days</td></tr><tr><td>highclock</td><td>Low Parallelism Workloads</td><td>32 Cores, 256GB memory</td><td></td><td>30 Days</td></tr><tr><td>gpu</td><td>GPU-Enabled Workloads</td><td>4 GPUs<sup id=fnref:6><a href=#fn:6 class=footnote-ref role=doc-noteref>6</a></sup>,48 Cores, 512GB memory</td><td>16 Cores, 256GB memory <sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup>,<sup id=fnref:7><a href=#fn:7 class=footnote-ref role=doc-noteref>7</a></sup></td><td>7 Days</td></tr></tbody></table><p>In addition to the above limits, there is:</p><ul><li>A 768 core group limit that spans across all users in a group across all partitions.</li><li>A 8 GPU group limit that spans across all users in a group across all GPU-enabled partitions.</li></ul><p>Attempting to allocate more member than a node can support, eg 500GB on an Intel node, will cause the job to immediately fail. Limits are for actively running jobs,
and any newly queued job that exceeds a limit will be queued until resources become available. If you require additional
resourced beyond the listed limits, please see the &ldquo;<a href=#additional-resource-request>Additional Resource Request</a>&rdquo; section below.</p><p>Partition quotas can also be viewed on the cluster using the <code>slurm_limits</code> command.</p><p>Additionally, users can have up to 5000 jobs in queue/running at the same time. Attempting to queue more than 5000 jobs will cause jobs submissions to fail with the reason &ldquo;MaxSubmitJobLimit&rdquo;.</p><h3 id=external-labs>External Labs</h3><p>Labs external to UCR will have reduced resource limits as follows:</p><ul><li>Labs will have a CPU quota of 256 cores across all lab users across all partitions</li><li>Per user CPU quotas on epyc, intel, batch, and short will be 128 cores</li><li>Per user CPU quotas on highmem will be 16</li><li>GPU quotas on the gpu partition will be 4 per-lab, and 2 per-user</li><li>CPU quotas on the gpu partition will be 24 per-user and 8 per-job</li></ul><h3 id=private-node-ownership>Private Node Ownership</h3><p>Labs have the ability to purchase nodes and connect them to the cluster for increased quotas. More information can be found in the <a href=https://hpcc.ucr.edu/about/overview/access/#ownership-models>Ownership Model</a> section of our Access page.</p><h3 id=additional-resource-request>Additional Resource Request</h3><p>Sometimes, whether it be due to deadlines or technical limitations, more resources might be needed than are supplied by default. If you require
a temporary increase in quotas, please reach out to <a href=mailto:support@hpcc.ucr.edu>support@hpcc.ucr.edu</a> with a complete &ldquo;<a href=https://docs.google.com/document/d/1Ate2yOdmaYrwzcjNp8S4-8VeAuH2WYwAuyVxAv0FDfo/>Justification for Quota Exception</a>&rdquo; form.
The following are typical circumstances that could justify increased quotas:</p><ul><li><strong>Urgent Deadlines</strong>: ie. Grant submissions, conference presentations, paper deadlines</li><li><strong>Special Technical Needs</strong>: The limits do not meet the technical requirements for the program(s) that are trying to be ran.</li></ul><p>The amount of additional resources, the length of time that the resources are needed, and the frequency of the requests are all factors that determine whether your request
will be accepted. It also must be within the capacity of the HPCC&rsquo;s infrastructure while also ensuring minimal disruption to other users. The final decision of approving
exception requests, and how many extra resources to provide, will be decided by the HPCC Staff, the Director, and in exceptional cases the HPCC Oversight Committee.</p><p>Requests limited by unoptimized code/datasets or strictly for the sake of convenience will be denied.</p><p>Additionally at this time we are unable to grant additional resource requests for external labs due to how our cluster is partially subsidized by our campus. We appologize for this,
and suggest looking into national computing facilities or cloud offerings to fill the gap in compute.</p><h3 id=example-scenarios>Example Scenarios</h3><h4 id=per-job-limit>Per-Job Limit</h4><p>A job is submitted on the gpu partition. The job requests 32 cores.</p><blockquote><p>This job will not be able to be submitted, as 32 cores is above the partition&rsquo;s 16 core per-job limit.</p></blockquote><h4 id=per-user-limit>Per-User Limit</h4><p>You submit a job the highmem partition, requesting 32 cores.</p><blockquote><p>This job will start successfully, as it is within the partition&rsquo;s core limit.</p></blockquote><p>You submit a second job while the first job is still running. The new job is requesting 32 cores.</p><blockquote><p>Because you are at your per-user core limit on the highmem partition, the second job will be queued until the first job finishes.</p></blockquote><h4 id=per-lab-limit>Per-Lab Limit</h4><p>User A submits a job requesting 384 cores. User B submits a job requesting 384 cores.</p><blockquote><p>Because each user is within their per-user limits and the lab is within their limit, the jobs will run in parallel.</p></blockquote><p>User C submits a job, requesting 16 cores.</p><blockquote><p>Because User A and User B are using all 768 cores within the lab, User C&rsquo;s job will be queued until either User A&rsquo;s or User B&rsquo;s jobs finishes.</p></blockquote><h2 id=changing-partitions>Changing Partitions</h2><p>In <code>srun</code> commands and <code>sbatch</code> scripts, the <code>-p</code> or <code>--partition</code> flag controls which partition/queue a job will run on. For example,
using <code>-p epyc</code> will have your job queued and ran on the <code>epyc</code> partition. For more examples and information on running jobs,
see the <a href=https://hpcc.ucr.edu/manuals/hpc_cluster/jobs/>Managing Jobs</a> page of our documentation.</p><h2 id=fair-share>Fair-Share</h2><p>Users that have not submitted any jobs in a long time usually have a higher priority over others that have ran jobs recently.
Thus the estimated start times can be extended to allow everyone their fair share of the system.
This prevents a few large groups from dominating the queuing system for long periods of time.</p><p>You can see with the <code>sqmore</code> command what priority your job has (list is sorted from lowest to highest priority).
You can also check to see how your group&rsquo;s priority is compared to other groups on the cluster with the &ldquo;sshare&rdquo; command.</p><p>For example:</p><pre><code class=language-bash>sshare
</code></pre><p>It may also be useful to see your entire group&rsquo;s fairshare score and who has used the most shares:</p><pre><code class=language-bash>sshare -A $GROUP --all
</code></pre><p>Lastley, if you only want to see your own fairshare score:</p><pre><code class=language-bash>sshare -A $GROUP -u $USER
</code></pre><p>The fairshare score is a number between 0 and 1. The best score being 1, and the worst being 0.
The fairshare score approches zero the more resource you (or your group) consume.
Your individual consumption of resources (usage) does affect your entire group&rsquo;s fiarshare score.
The affects of your running/completed jobs on your fairshare score are halved each day (half-life).
Thus, after waiting several days without running any jobs, you should see an improvment in your fairshare score.</p><p>Here is a very good <a href=https://www.rc.fas.harvard.edu/fairshare/>explaination of fairshare</a>.</p><h2 id=priority>Priority</h2><p>The fairshare score and jobs queue wait time is used to calculate your job&rsquo;s priority.
You can use the <code>sprio</code> command to check the priority of your jobs:</p><pre><code>sprio -u $USER
</code></pre><p>Even if your group has a lower fairshare score, your job may still have a very high priority.
This would be likely due to the job&rsquo;s queue wait time, and it should start as soon as possible regardless of fairshare score.
You can use the <code>sqmore</code> command to see a list of all jobs sorted by priority.</p><h2 id=backfill>Backfill</h2><p>Some small jobs may start before yours, only if they can complete before yours starts and thus not negatively affecting your start time.</p><h2 id=priority-partition>Priority Partition</h2><p>Some groups on our system have purchased additional hardware. These nodes will not be affected by the fairshare score.
This is because jobs submitted to the group&rsquo;s partition will be evaluated first before any other jobs that have been submitted to those nodes from a different partition.</p><h2 id=using-the-preempt-partitions-tenative>Using the Preempt Partitions (TENATIVE)</h2><p><strong>NOTE</strong> The full release of the preempt partition is planned for future release and <strong>is not</strong> yet available!</p><p>This guide assumes that you know how to run Interactive and Batch jobs through Slurm. If you do not, then please see the <a href=https://hpcc.ucr.edu/manuals/hpc_cluster/jobs/>Managing Jobs</a> page of our documentation.</p><p>There are two partitions that will have preemption enabled: &ldquo;preempt&rdquo; for CPU jobs, and &ldquo;preempt_gpu&rdquo; for GPU jobs.</p><p>To fully take advantage of preemption, your jobs must be be able to tolerate being cancelled at a random time and restarted at some later point in the future. When your job is preempted, it will be cancelled and requeued. When the job is elegible to start again, it will start from the beginning of the sbatch script as if it were newly run.</p><p>Your job is only guaranteed 5 minutes of runtime when it starts before it is elegible to be preempted.</p><h3 id=job-limitations>Job Limitations</h3><h4 id=time>Time</h4><p>As mentioned above, jobs can be killed at any time after the 5 minute grace period. Jobs should be set up such that any initialization steps that cannot tolerate being randomly killed happen within those first 5 minutes. The max walltime of a job is currently set to 1 day (24 hours).</p><h4 id=resources>Resources</h4><p>Currently, users are allowed to use an equal number of CPU cores as their current per-partition CPU limit. If you&rsquo;re currently allowed to use 384 cores on the epyc partition, then you can use 384 cores on the preempt partition. The same applies to memory. For the GPU partition, users are currently allowed to use 1 GPU on the &ldquo;preempt_gpu&rdquo; partition.</p><h3 id=starting-a-job>Starting a job</h3><p>Similar to other partitions, you must specifically queue jobs to the <code>preempt</code> partition. One special thing that is required is to also specify the <code>preempt</code> account using <code>-A preempt</code>. Jobs started on the preempt partition <strong>do not</strong> count against your lab&rsquo;s CPU quota.</p><h4 id=interactive-example>Interactive Example</h4><p>To start a CPU preemptable interactive job, you can build off of the following command:</p><pre><code class=language-bash>srun -A preempt -p preempt -c 8 --mem 8GB --pty bash -l
</code></pre><p>This will start a job with 8 cores and 8GB of memory on the <code>preempt</code> partition under the <code>preempt</code> account. Jobs that do not explicitly state <code>-A preempt</code> will fail to start. Note that because this is a preemptable job, your session can be terminated at any moment without notice after the 5 minute grace period.</p><p>To start a GPU preemptable interactive job, you can build off of the following command:</p><pre><code class=language-bash>srun -A preempt -p preempt_gpu --gres=gpu:1 -c 8 --mem 8GB --pty bash -l
</code></pre><h4 id=non-interactive-batch-example>Non-interactive (batch) Example</h4><p>As with all preemptable jobs, batch jobs can be cancelled at any time without notice and the programs <em>must</em> be able to tolerate this. Jobs that have been preempted will automatically be requeued to resume running at a later time when resources become available. The <code>$SLURM_RESTART_COUNT</code> environment variable can be used to check if the job has been preempted and restarted to allow you to recover and resume running.</p><p>To start a batch job, you can build off of the following sbatch file:</p><pre><code>#!/bin/bash -l

#SBATCH -A preempt
#SBATCH -p preempt
#SBATCH -c 8
#SBATCH --mem 8GB
#SBATCH --time 1-00:00:00

# Check if this is the first run or a resumed job
if [ &quot;$SLURM_RESTART_COUNT&quot; -eq 0 ]; then
    echo &quot;This is the first time running the job&quot;
    # Put the code for the first run here
    # Example: initializing data or setting up environment
    # Remember that a job only has 5 minutes of guaranteed runtime. Keep
    # any initialization/recovery short otherwise it might be interrupted
else
    echo &quot;The job is being resumed after a preemption&quot;
    # Put the code for a resumed job here
    # Example: resuming from a checkpoint or continuing work
    # Remember that a job only has 5 minutes of guaranteed runtime. Keep
    # any initialization/recovery short otherwise it might be interrupted
fi

# Common job code that runs regardless of first run or resume
echo &quot;Running main job tasks...&quot;
# Put your main job code here
</code></pre><p>Jobs that do not explicitly state <code>#SBATCH -A preempt</code> will fail to start. Note that because this is a preemptable job, your job can be cancelled at any moment without notice.</p><h4 id=selecting-resources>Selecting Resources</h4><p>Similar to the Short partition, the Preempt partition is a union of all public and private machines, excluding specialty machines like highmem and GPU. This means that if you do not specify any restrictions, your job can run on nodes in the batch, intel, or epyc partition. If a certain architecture is required for your job, then you can use the <code>--constraint</code> flag.</p><p>For example, if you want your job to run on an Intel machine, you can include <code>#SBATCH --constraint=intel</code> in your sbatch script, or <code>--constraint=intel</code> in your srun command. If you want either an Intel or Epyc Rome machine, then you could use <code>#SBATCH --constraint=intel|rome</code> in your sbatch script, or <code>constraint=intel|rome</code> in your srun command. More information on constraints is available in the <a href=https://slurm.schedmd.com/sbatch.html#OPT_constraint>Slurm Documentation</a>.</p><p>To view which nodes contain which features, see the Feature Constraints listed on the <a href=https://hpcc.ucr.edu/manuals/hpc_cluster/jobs/#feature-constraints>Feature Constraints</a> page</p><section class=footnotes role=doc-endnotes><hr><ol><li id=fn:1 role=doc-endnote><p>The 384 core/1TB limit is per-user across all CPU compute partitions (epyc, intel, and batch). Attempting to run more then 384 cores, even if across multiple CPU compute partitions, will be queued until resources become available. Specialized partitions (eg. short, highmem, gpu) will not count against the CPU compute partition&rsquo;s quotas.&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:2 role=doc-endnote><p>A 64GB-per-core limit is placed to prevent over allocating memory compared to CPUs. If more than a 64GB-per-core ratio is requested, the core count will be increased to match.&#160;<a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:3 role=doc-endnote><p>Allocatable memory per-node in the <strong>epyc</strong> partition is limited to <strong>~950GB</strong> to allow for system overhead.&#160;<a href=#fnref:3 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:4 role=doc-endnote><p>Allocatable memory per-node in the <strong>intel</strong> partition is limited to <strong>~450GB</strong> to allow for system overhead.&#160;<a href=#fnref:4 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:5 role=doc-endnote><p>Allocatable memory per-node in the <strong>batch</strong> partition is limited to <strong>~500GB</strong> to allow for system overhead.&#160;<a href=#fnref:5 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:6 role=doc-endnote><p>If a user needs more than 4 GPUs, please contact <a href=mailto:support@hpcc.ucr.edu>support@hpcc.ucr.edu</a> with a short justification for a temporary increase.&#160;<a href=#fnref:6 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:7 role=doc-endnote><p>Allocatable memory per-node in the <strong>gpu</strong> partition is dependent on the node. 115GB for gpu[01-02], 500GB for gpu[03-04], 200GB for gpu05, 922GB for gpu06, 950GB for gpu[07-08]&#160;<a href=#fnref:7 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></section><div class="text-muted mt-5 pt-3 border-top">Last modified June 26, 2025: <a href=https://github.com/ucr-hpcc/ucr-hpcc.github.io/commit/7bcb7b7d1708085eed14dc07c9762caa6c6fef73>Update queue.md (7bcb7b7d1)</a></div></div></main></div></div><footer class="bg-dark py-5 row d-print-none"><div class="container-fluid mx-sm-5"><div class=row><div class="col-6 col-sm-4 text-xs-center order-sm-2"><ul class="list-inline mb-0"><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title="Email Support" aria-label="Email Support"><a class=text-white target=_blank rel="noopener noreferrer" href=mailto:support@hpcc.ucr.edu><i class="fa fa-envelope"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Twitter aria-label=Twitter><a class=text-white target=_blank rel="noopener noreferrer" href=https://twitter.com/UCR_HPCC><i class="fab fa-twitter"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title="Stack Overflow" aria-label="Stack Overflow"><a class=text-white target=_blank rel="noopener noreferrer" href=https://ucr-hpcc.slack.com/><i class="fab fa-slack"></i></a></li></ul></div><div class="col-6 col-sm-4 text-right text-xs-center order-sm-3"><ul class="list-inline mb-0"><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title aria-label><a class=text-white target=_blank rel="noopener noreferrer" href><i></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title aria-label><a class=text-white target=_blank rel="noopener noreferrer" href><i></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title aria-label><a class=text-white target=_blank rel="noopener noreferrer" href><i></i></a></li></ul></div><div class="col-12 col-sm-4 text-center py-2 order-sm-2"><small class=text-white>&copy; 2025 The Docsy Authors All Rights Reserved</small>
<small class=ml-1><a href=https://policies.google.com/privacy target=_blank>Privacy Policy</a></small></div></div></div></footer></div><script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.3/umd/popper.min.js integrity=sha384-ZMP7rVo3mIykV+2+9J3UJ46jBk0WLaUAdn689aCwoqbBJiSnjAK/l8WvCWPIPm49 crossorigin=anonymous></script><script src=https://stackpath.bootstrapcdn.com/bootstrap/4.1.3/js/bootstrap.min.js integrity=sha384-ChfqqxuZUCnJSK3+MXmPNIyE6ZbWh2IMqE241rYiqJxyMiZ6OW/JmZQ5stwEULTy crossorigin=anonymous></script><script>(function(){var c,a,b,d=document.getElementsByTagName('code');for(c=0;c<d.length;){if(b=d[c],b.parentNode.tagName!=='PRE'&&b.childElementCount===0)if(a=b.textContent,/^\$[^$]/.test(a)&&/[^$]\$$/.test(a)&&(a=a.replace(/^\$/,'\\(').replace(/\$$/,'\\)'),b.textContent=a),/^\\\((.|\s)+\\\)$/.test(a)||/^\\\[(.|\s)+\\\]$/.test(a)||/^\$(.|\s)+\$$/.test(a)||/^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(a)){b.outerHTML=b.innerHTML;continue}c++}})()</script><script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?version=4.8.0&features=es6"></script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><script type=text/x-mathjax-config>
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
        processEscapes: true,
        processEnvironments: true
    },
    // Center justify equations in code and markdown cells. Elsewhere
    // we use CSS to left justify single line equations in code cells.
    displayAlign: 'center',
    "HTML-CSS": {
        styles: {'.MathJax_Display': {"margin": 0}},
        linebreaks: { automatic: true }
    }
});
</script><script src=../../../js/main.min.63db3e552bd0543be17ce4a79a18b744e9cb72256bec42b540e3ab0cd43722d0.js integrity="sha256-Y9s+VSvQVDvhfOSnmhi3ROnLciVr7EK1QOOrDNQ3ItA=" crossorigin=anonymous></script><script src=../../../js/prism.js></script></body></html>