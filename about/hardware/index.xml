<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>HPCC â€“ Hardware</title><link>https://hpcc.ucr.edu/about/hardware/</link><description>Recent content in Hardware on HPCC</description><generator>Hugo -- gohugo.io</generator><atom:link href="https://hpcc.ucr.edu/about/hardware/index.xml" rel="self" type="application/rss+xml"/><item><title>About: Hardware Overview</title><link>https://hpcc.ucr.edu/about/hardware/overview/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://hpcc.ucr.edu/about/hardware/overview/</guid><description>
&lt;h2 id="computer-cluster">Computer cluster&lt;/h2>
&lt;ul>
&lt;li>Over 16,000 CPU cores (60% AMD, 40% Intel)&lt;/li>
&lt;li>512-1024GB RAM per node&lt;/li>
&lt;li>GPU: 24x K80, 2x P100, 24x A100&lt;/li>
&lt;li>IB network @ 56Gbs - 200Gbps (FDR, HDR and NDR)&lt;/li>
&lt;li>Queueing system Slurm&lt;/li>
&lt;/ul>
&lt;h2 id="parallel-data-storage-system">Parallel data storage system&lt;/h2>
&lt;ul>
&lt;li>5PB GPFS storage (scales to &amp;gt;50PB)&lt;/li>
&lt;li>Home directories on dedicated system&lt;/li>
&lt;/ul>
&lt;h2 id="backup-system">Backup system&lt;/h2>
&lt;ul>
&lt;li>5PB GPFS storage&lt;/li>
&lt;li>Geographically separated server room&lt;/li>
&lt;/ul>
&lt;h2 id="server-room">Server room&lt;/h2>
&lt;ul>
&lt;li>Genomics Building, Rm 1120A&lt;/li>
&lt;li>Size 600 sqft&lt;/li>
&lt;li>Raised floor cooling with redundant AC units&lt;/li>
&lt;li>Backup power: UPS plus generator&lt;/li>
&lt;/ul></description></item><item><title>About: Hardware Details</title><link>https://hpcc.ucr.edu/about/hardware/details/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://hpcc.ucr.edu/about/hardware/details/</guid><description>
&lt;h2 id="storage">Storage&lt;/h2>
&lt;ul>
&lt;li>Four enterprise class HPC storage systems&lt;/li>
&lt;li>Approximately 3 PB production and 3 PB backup storage (total 6 PB or 6,144 TB)&lt;/li>
&lt;li>GPFS (NFS and SAMBA via GPFS)&lt;/li>
&lt;li>Automatic snapshots and monthly backups&lt;/li>
&lt;/ul>
&lt;h2 id="network">Network&lt;/h2>
&lt;ul>
&lt;li>Ethernet
&lt;ul>
&lt;li>2 x 40 Gb/s switch for campus high performance research network&lt;/li>
&lt;li>1 x 10 Gb/s connection to public internet&lt;/li>
&lt;li>Redundant, load balanced, robust mesh topology&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Interconnect
&lt;ul>
&lt;li>56/200/400 Gb/s InfiniBand (FDR/HDR/NDR)&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="head-nodes">Head Nodes&lt;/h2>
&lt;p>All users should access the cluster via SSH through &lt;code>cluster.hpcc.ucr.edu&lt;/code>. This address will automatically balance traffic to one of the available head nodes.&lt;/p>
&lt;ul>
&lt;li>bluejay, skylark
&lt;ul>
&lt;li>Resources: 256 cores, 512 GB memory&lt;/li>
&lt;li>Primary function: submitting jobs to the queuing system&lt;/li>
&lt;li>Secondary function: development; code editing and running small (under 50 % CPU and under 0.5 GB RAM) sample jobs&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="worker-nodes">Worker Nodes&lt;/h2>
&lt;ul>
&lt;li>Batch
&lt;ul>
&lt;li>c01-c48: each with 64x AMD cores (AMD Opteron 6376) and 512 GB memory&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Intel
&lt;ul>
&lt;li>i[01-02,17-40]: each with 32 Intel cores (Intel Xeon E5-2683 v4) and 512 GB memory&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Epyc
&lt;ul>
&lt;li>r21-r38: each with 256 AMD cores (AMD EPYC 7713) and 1 TB memory&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Highmem
&lt;ul>
&lt;li>h01-h06: each with 32x Intel cores (Intel Xeon E5-4620 v2) and 1024 GB memory&lt;/li>
&lt;li>h07: 192x Intel Cores (Xeon Max 9468) and 3TB of memory&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Highclock
&lt;ul>
&lt;li>hz01-hz04: each with 64x Intel cores (Xeon Gold 6558Q) and 1024 GB memory&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>GPU
&lt;ul>
&lt;li>gpu01-gpu02: each with dual 8c/16t Intel E5-2630 v3 Haswell CPUs and 2 x NVIDIA Tesla K80 GPUs (12GB and 2496 CUDA cores per GPU) and 128 GB memory&lt;/li>
&lt;li>gpu03-gpu04: each with dual 12c/24t Intel E5-2650 v4 Intel Broadwell CPUs and 4 x NVIDIA Tesla K80 GPUs (12GB and 2496 CUDA cores per GPU) and 512 GB memory&lt;/li>
&lt;li>gpu05: with dual 16c/32t Intel E5-2683 v4 Broadwell CPUs and 2 x NVIDIA Tesla P100 GPUs (16GB and 3584 CUDA cores per GPU) and 256 GB memory&lt;/li>
&lt;li>gpu06-gpu08: with dual 32c/32t AMD EPYC 7543 CPUs and 8 x NVIDIA A100 GPUs (80GB and 6912 CUDA cores per GPU) and 1,024 GB memory&lt;/li>
&lt;li>gpu11: with dual 64c/64t AMD EPYC 9554 CPUs and 2 x NVIDIA H100 GPUs (94GB and 16896 CUDA cores per GPU) and 1,536 GB memory&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>For more info about queueing jobs see the &lt;a href="https://hpcc.ucr.edu/manuals/hpc_cluster/jobs/">Managing Jobs&lt;/a> page. For queueing jobs to nodes with specific features, see the &lt;a href="https://hpcc.ucr.edu/manuals/hpc_cluster/jobs/#feature-constraints">Feature Constraints&lt;/a> section.&lt;/p></description></item></channel></rss>