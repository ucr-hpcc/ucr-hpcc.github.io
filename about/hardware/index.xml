<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>HPCC â€“ Hardware</title><link>https://hpcc.ucr.edu/about/hardware/</link><description>Recent content in Hardware on HPCC</description><generator>Hugo -- gohugo.io</generator><atom:link href="https://hpcc.ucr.edu/about/hardware/index.xml" rel="self" type="application/rss+xml"/><item><title>About: Hardware Overview</title><link>https://hpcc.ucr.edu/about/hardware/overview/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://hpcc.ucr.edu/about/hardware/overview/</guid><description>
&lt;h2 id="computer-cluster">Computer cluster&lt;/h2>
&lt;ul>
&lt;li>Over 16,000 CPU cores (60% AMD, 40% Intel)&lt;/li>
&lt;li>512-1024GB RAM per node&lt;/li>
&lt;li>GPU: 24x K80, 2x P100, 24x A100&lt;/li>
&lt;li>IB network @ 56Gbs - 200Gbps (FDR, HDR and NDR)&lt;/li>
&lt;li>Queueing system Slurm&lt;/li>
&lt;/ul>
&lt;h2 id="parallel-data-storage-system">Parallel data storage system&lt;/h2>
&lt;ul>
&lt;li>5PB GPFS storage (scales to &amp;gt;50PB)&lt;/li>
&lt;li>Home directories on dedicated system&lt;/li>
&lt;/ul>
&lt;h2 id="backup-system">Backup system&lt;/h2>
&lt;ul>
&lt;li>5PB GPFS storage&lt;/li>
&lt;li>Geographically separated server room&lt;/li>
&lt;/ul>
&lt;h2 id="server-room">Server room&lt;/h2>
&lt;ul>
&lt;li>Genomics Building, Rm 1120A&lt;/li>
&lt;li>Size 600 sqft&lt;/li>
&lt;li>Raised floor cooling with redundant AC units&lt;/li>
&lt;li>Backup power: UPS plus generator&lt;/li>
&lt;/ul></description></item><item><title>About: Hardware Details</title><link>https://hpcc.ucr.edu/about/hardware/details/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://hpcc.ucr.edu/about/hardware/details/</guid><description>
&lt;h2 id="storage">Storage&lt;/h2>
&lt;ul>
&lt;li>Four enterprise class HPC storage systems&lt;/li>
&lt;li>Approximately 3 PB production and 3 PB backup storage (total 6 PB or 6,144 TB)&lt;/li>
&lt;li>GPFS (NFS and SAMBA via GPFS)&lt;/li>
&lt;li>Automatic snapshots and monthly backups&lt;/li>
&lt;/ul>
&lt;h2 id="network">Network&lt;/h2>
&lt;ul>
&lt;li>Ethernet
&lt;ul>
&lt;li>2 x 40 Gb/s switch for campus high performance research network&lt;/li>
&lt;li>1 x 10 Gb/s connection to public internet&lt;/li>
&lt;li>Redundant, load balanced, robust mesh topology&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Interconnect
&lt;ul>
&lt;li>56/200/400 Gb/s InfiniBand (FDR/HDR/NDR)&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="head-nodes">Head Nodes&lt;/h2>
&lt;p>All users should access the cluster via SSH through &lt;code>cluster.hpcc.ucr.edu&lt;/code>. This address will automatically balance traffic to one of the available head nodes.&lt;/p>
&lt;ul>
&lt;li>bluejay, skylark
&lt;ul>
&lt;li>Resources: 256 cores, 512 GB memory&lt;/li>
&lt;li>Primary function: submitting jobs to the queuing system&lt;/li>
&lt;li>Secondary function: development; code editing and running small (under 50 % CPU and under 0.5 GB RAM) sample jobs&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="worker-nodes">Worker Nodes&lt;/h2>
&lt;p>For a full list of nodes, please see our &lt;a href="https://docs.google.com/spreadsheets/d/1SVH1-c1i075vjt-B0wNPiK87wmLkPltWJlIPgLkmoqU/">Node List&lt;/a> sheet. This contains info on all nodes (public and private) that we have. This includes CPU, memory, and GPU specs, as well as any features that the nodes have.&lt;/p>
&lt;p>For more info about queueing jobs see the &lt;a href="https://hpcc.ucr.edu/manuals/hpc_cluster/jobs/">Managing Jobs&lt;/a> page. For queueing jobs to nodes with specific features, see the &lt;a href="https://hpcc.ucr.edu/manuals/hpc_cluster/jobs/#feature-constraints">Feature Constraints&lt;/a> section.&lt;/p></description></item></channel></rss>